[
  {
    "objectID": "posts/working-with-geospatial-vector-data/working-with-geospatial-vector-data.html",
    "href": "posts/working-with-geospatial-vector-data/working-with-geospatial-vector-data.html",
    "title": "Working with Geospatial Vector Data",
    "section": "",
    "text": "Geospatial vector data is a way of representing geographic features in a digital format using points, lines, and polygons. Unlike raster data, which represents geographic data as a grid of cells or pixels, vector data represents features more precisely with distinct shapes and boundaries. Each vector feature can have associated attributes, such as names, types, or other descriptive information.\n\nTypes of Geospatial Vector Data\n\nPoints: Represent discrete locations such as cities, landmarks, or individual trees. Each point has a specific location defined by coordinates (e.g., latitude and longitude).\nLines (or polylines): Represent linear features such as roads, rivers, or boundaries. Lines are composed of a series of connected points.\nPolygons (or multipolygons): Represent areas or shapes such as lakes, parks, or country borders. Polygons are defined by a series of points that create a closed shape.\n\nShapefiles are one of the most common formats for vector data. They store points, lines, and polygons along with attribute information. The US Census Bureau makes a number of shapefiles available here. In this notebook, we’ll walkthrough how to load shapefiles into GeoPandas, plotting the boundaries and create a choropleth map based on a second dataset (choropleth maps are those where the color of each shape is based on the value of an associated variable).\nTo start, download US state shapefile cb_2018_us_state_500k.zip from the United States Census Bureau boundary files page. Under the State subheader, you will see three files:\n\ncb_2018_us_state_500k.zip\ncb_2018_us_state_5m.zip\ncb_2018_us_state_20m.zip\n\nThe 500k files are the most detailed, but also the largest. The 20m files are the smallest, but at the cost of some dramatic simplification. The 5m files fall somewhere between the other two. We will work with the 500k files.\nOnce downloaded, the shapefile can be loaded into a GeoPandas DataFrame as follows:\n\n\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\n\nshp_path = \"cb_2018_us_state_500k.zip\"\n\ndfshp = gpd.read_file(shp_path)\n\ndfshp.head(5)\n\n\n\n\n\n\n\n\nSTATEFP\nSTATENS\nAFFGEOID\nGEOID\nSTUSPS\nNAME\nLSAD\nALAND\nAWATER\ngeometry\n\n\n\n\n0\n28\n01779790\n0400000US28\n28\nMS\nMississippi\n00\n121533519481\n3926919758\nMULTIPOLYGON (((-88.50297 30.21523, -88.49176 ...\n\n\n1\n37\n01027616\n0400000US37\n37\nNC\nNorth Carolina\n00\n125923656064\n13466071395\nMULTIPOLYGON (((-75.72681 35.93584, -75.71827 ...\n\n\n2\n40\n01102857\n0400000US40\n40\nOK\nOklahoma\n00\n177662925723\n3374587997\nPOLYGON ((-103.00257 36.52659, -103.00219 36.6...\n\n\n3\n51\n01779803\n0400000US51\n51\nVA\nVirginia\n00\n102257717110\n8528531774\nMULTIPOLYGON (((-75.74241 37.80835, -75.74151 ...\n\n\n4\n54\n01779805\n0400000US54\n54\nWV\nWest Virginia\n00\n62266474513\n489028543\nPOLYGON ((-82.64320 38.16909, -82.64300 38.169...\n\n\n\n\n\n\n\n\nThe geometry column is a special column in a GeoDataFrame that stores the geometric shapes associated with each row (in this case, the shapes in latitude-longitude pairs that define the boundary of each state). This column contains the vector data that defines the spatial features in the dataset. Some states have boundaries defined by a MULTIPOLYGON, such as Hawaii, whose boundary consists of multiple closed POLYGONS. If it isn’t already present, the geometry column needs to be defined.\nWe can plot the data present in the present in the shapefile by calling the GeoDataFrame’s plot method:\n\n\ndfshp.plot()\n\n\n\n\n\n\n\n\n\nLet’s zoom in and focus on a map of the lower 48 states only:\n\n\nexclude = [\"American Samoa\", \"Alaska\", \"Hawaii\", \"Guam\", \"United States Virgin Islands\",\n           \"Commonwealth of the Northern Mariana Islands\", \"Puerto Rico\"]\n\ndfshp48 = dfshp[~dfshp.NAME.isin(exclude)].reset_index(drop=True)\n\ndfshp48.plot()\n\n\n\n\n\n\n\n\n\nWe can get a better view of the boundaries of each state by calling boundary.plot:\n\n\ndfshp48.boundary.plot()\n\n\n\n\n\n\n\n\nBy default, the plots rendered via GeoPandas are smaller than we might like. We can increase the size of the rendered map, suppress ticklabels, change the boundary color and add a title as follows:\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 8), tight_layout=True)\n\nax.set_title(\"U.S. Boundaries - Lower 48 States\")\ndfshp48.boundary.plot(ax=ax, edgecolor=\"red\", linewidth=.50)\n\nax.axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nTo overlay the state name at the center of each state, use:\n\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 8), tight_layout=True)\nax.set_title(\"U.S. Boundaries - Lower 48 States\")\ndfshp48.boundary.plot(ax=ax, edgecolor=\"red\", linewidth=.50)\ndfshp48.apply(lambda x: ax.annotate(x.NAME, xy=x.geometry.centroid.coords[0], ha='center', fontsize=6), axis=1)\nax.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\nIn the shapefile, ALAND and AWATER represent the land and water area of each state in square meters. To create a choropleth map based on the natural log of AWATER, include the column argument to the plot method:\n\n\n# Compute natural log of AWATER to get better separation by state.\ndfshp48[\"log_AWATER\"] = np.log(dfshp48[\"AWATER\"])\ndfshp48.plot(column=\"log_AWATER\", cmap=\"plasma\")\n\n\n\n\n\n\n\n\n\nWe can reformat the map as before, while also adding a legend to give context the difference in colors by state. Options for colormaps are available here:\n\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 8), tight_layout=True)\nax.set_title(\"Ln(AWATER) - Lower 48 States\")\ndfshp48.plot(\n    ax=ax, column=\"log_AWATER\", edgecolor=\"gray\", linewidth=.50, \n    cmap=\"gist_rainbow\", alpha=.750, legend=True,\n    legend_kwds={\"label\": \"Ln(AWATER)\", \"orientation\": \"vertical\", \"shrink\": .35}\n    )\nax.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCongressional District Shapefiles\nFor variety, let’s download the Congressional District shapefile and plot the boundaries. It is available at the same link as above, and is identified as cb_2018_us_cd116_500k.zip. Reading the file into GeoPandas and displaying the first 5 rows yields:\n\n\ndfc = gpd.read_file(\"cb_2018_us_cd116_500k.zip\")\n\nprint(f\"dfc.shape: {dfc.shape}\")\n\ndfc.head(5)\n\ndfc.shape: (441, 9)\n\n\n\n\n\n\n\n\n\nSTATEFP\nCD116FP\nAFFGEOID\nGEOID\nLSAD\nCDSESSN\nALAND\nAWATER\ngeometry\n\n\n\n\n0\n17\n10\n5001600US1710\n1710\nC2\n116\n777404163\n31605644\nPOLYGON ((-88.19882 42.41557, -88.19860 42.415...\n\n\n1\n47\n06\n5001600US4706\n4706\nC2\n116\n16770155959\n324676580\nPOLYGON ((-87.15023 36.56770, -87.14962 36.568...\n\n\n2\n48\n06\n5001600US4806\n4806\nC2\n116\n5564805243\n255530191\nPOLYGON ((-97.38860 32.61731, -97.38856 32.618...\n\n\n3\n48\n07\n5001600US4807\n4807\nC2\n116\n419784487\n3069802\nPOLYGON ((-95.77383 29.87515, -95.76962 29.875...\n\n\n4\n48\n26\n5001600US4826\n4826\nC2\n116\n2349987793\n191353567\nPOLYGON ((-97.39826 32.99996, -97.39792 33.013...\n\n\n\n\n\n\n\nWe again display the boundaries:\n\n\ndfc.boundary.plot()\n\n\n\n\n\n\n\n\nWe’d like to focus on the lower 48 states again, but this time the shapefile doesn’t have a NAME column. How should we proceed?\nOne approach is to define a bounding box that encloses the lower 48 states, then filter the shapefile to retain only those congressional districts whose geometry intersects the bounding box. GeoPandas provides coordinate based indexing with the cx indexer, which slices using a bounding box. Geometries in the GeoSeries or GeoDataFrame that intersect the bounding box will be returned.\nFor the lower 48 states bounding box, we’ll use (-125, 24.6), (-65, 50), southwest to northeast. We also include a circle marker at the center of each congressional district:\n\n\nxmin, ymin, xmax, ymax = -125, 24.6, -65, 50\ndfc48 = dfc.cx[xmin:xmax, ymin:ymax]\n\n\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 8), tight_layout=True)\nax.set_title(\"US Congressional Districts, 116th Congress - Lower 48 States\", fontsize=11)\ndfc48.boundary.plot(ax=ax, edgecolor=\"black\", linewidth=.50)\ndfc48.geometry.centroid.plot(ax=ax, markersize=6, color=\"red\")\nax.axis(\"off\")\nplt.show()\n\nC:\\Users\\jtriv\\AppData\\Local\\Temp\\ipykernel_8996\\3296541533.py:9: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  dfc48.geometry.centroid.plot(ax=ax, markersize=6, color=\"red\")\n\n\n\n\n\n\n\n\n\n\n\n\nGeoJSON\nWorking with GeoJSON is much the same as working with shapefiles, one difference being that with GeoJSON, vector data is contained within a single file as opposed to an archive of multiple file types. See here for an example.\nBut once read into GeoPandas, we work with it the same way. We can load US state boundary files as GeoJSON from GitHub via:\n\n\ndfstate = gpd.read_file(\"https://raw.githubusercontent.com/PublicaMundi/MappingAPI/master/data/geojson/us-states.json\")\n\ndfstate.head()\n\n\n\n\n\n\n\n\nid\nname\ndensity\ngeometry\n\n\n\n\n0\n01\nAlabama\n94.650\nPOLYGON ((-87.35930 35.00118, -85.60667 34.984...\n\n\n1\n02\nAlaska\n1.264\nMULTIPOLYGON (((-131.60202 55.11798, -131.5691...\n\n\n2\n04\nArizona\n57.050\nPOLYGON ((-109.04250 37.00026, -109.04798 31.3...\n\n\n3\n05\nArkansas\n56.430\nPOLYGON ((-94.47384 36.50186, -90.15254 36.496...\n\n\n4\n06\nCalifornia\n241.700\nPOLYGON ((-123.23326 42.00619, -122.37885 42.0...\n\n\n\n\n\n\n\n\n\ndfstate.plot()"
  },
  {
    "objectID": "posts/tweedie-dispersion/tweedie-dispersion.html",
    "href": "posts/tweedie-dispersion/tweedie-dispersion.html",
    "title": "The Buried Assumption in Constant Dispersion Tweedie Models",
    "section": "",
    "text": "Instead of building separate models for claim frequency and claim severity, the Tweedie distribution can be used to model pure premium directly. The Tweedie family of distributions are Exponential Dispersion Models with variance function \\(V(\\mu) = \\mu^{p}\\) where \\(1 \\lt p \\lt 2\\).\nHowever, the Tweedie distribution has an important limitation: Buried in the model parameterization is the assumption of positive correlation between claim frequency and severity. Put another way, as commonly implemented within various GLM frameworks, the Tweedie distribution assumes that each covariate impacts frequency and severity in the same direction.\nTweedie is a compund Poisson-gamma process, where claim frequency is assumed to follow Poisson distribution and claim severity assumed to follow a gamma distribution (conditional on the occurrence of a claim). In the original Smyth and Jorgensen paper, the Tweedie distribution is parameterized as follows:\n\\[\n\\mu = \\lambda \\cdot \\alpha \\cdot \\theta; \\hspace{.75em} p = \\frac{\\alpha + 2}{\\alpha + 1}; \\hspace{.75em} \\phi = \\frac{\\lambda^{1 - p} \\cdot (\\alpha \\cdot \\theta)^{2 - p}}{2 - p}\n\\]\nWhere:\n\n\\(\\lambda\\) is the mean of the underlying Poisson distribution.\n\\(\\alpha\\) is the gamma distribution shape parameter.\n\\(\\theta\\) is the gamma distribution scale parameter.\n\\(p\\) is constant across the dataset, specified by the user. Since \\(\\alpha \\gt 0\\), \\(1 \\lt p \\lt 2\\).\n\nBecause \\(p\\) is constant, we have:\n\\[\n\\phi \\sim \\frac{(\\alpha \\cdot \\theta)^{2 - p}}{\\lambda^{p-1}}\n\\]\n\\(\\phi\\) is proportional to the mean severity raised to some positive power divided by the mean frequency raised to some positive power. If \\(\\phi\\) is assumed to be constant across the dataset (as is the default assumption in many popular libraries such as scikit-learn and statsmodels), any factor that increases mean severity must also increase mean frequency. To ensure that \\(\\phi\\) remains constant, mean frequency and mean severity must move in the same direction. However, the assumption that frequency and severity move in the same direction is often unrealistic. Think of how auto claims are usually distributed: A large number of low severity claims, along with fewer of high severity.\nThe Tweedie distribution can still be used in situations in which frequency and severity move in opposite directions, as long as \\(\\phi\\) and \\(\\mu\\) are modeled together. This is referred to as a double GLM, and is outlined in the Smyth and Jorgensen paper. If the dispersion parameter is allowed to vary across the dataset, the assumption of positive association between frequency and severity no longer applies.\nFitting a Tweedie double GLM can be accomplished as follows:\n\nUsing pure premium as the target, fit a Tweedie GLM for the mean. The weight is exposures divided by \\(\\phi\\), where initially \\(\\phi = 1\\) for all inputs.\nCalculate the weighted deviance for each record. The sum of these unit deviances equals the total deviance for the initial model for the mean.\nFit a gamma GLM for dispersion. The target variable is the unit deviance with each record given equal weight. The predictions from this model represent the new per observation dispersion.\nRe-fit the GLM for the mean from step 1., this time using the dispersion parameters obtained in step 3. The weight for this GLM is exposure divided by the new dispersion parameters.\n\nThis is straightforward to implement, and is left as an exercise for the reader. For R users, the dglm package is available.\nI recall a few years back it wasn’t possible to set p to a variable when using the dglm package; p had to be hard-coded with a scalar value, which was inconvenient if you wanted to evaluate a number of Tweedie models with different ps in an iterative fashion. A work-around was proposed here, but the package creator may have since incorporated the update into the codebase."
  },
  {
    "objectID": "posts/sqlserver-from-python/sqlserver-from-python.html",
    "href": "posts/sqlserver-from-python/sqlserver-from-python.html",
    "title": "Querying SQL Server from Pandas",
    "section": "",
    "text": "If you research how to connect to a database from Python, many examples use the pyodbc library, which, aptly named, creates a connection to any ODBC-compatible database. However, connections with pyodbc itself are uni-directional: Data can be retrieved, but it cannot be uploaded into the database. To allow for simple, bi-directional database transactions, we use pyodbc along with sqlalchemy, a Python SQL toolkit and Object Relational Mapper that gives application developers the full power and flexibility of SQL. With pyodbc and sqlalchemy together, it becomes possible to retrieve and upload data from Pandas DataFrames with relative ease. Let’s assume we’re interested in connecting to a SQL Server database on some server. A connection using sqlalchemy is created as follows:\n\"\"\"\nCreating a database connection with sqlalchemy. \n\"\"\"\nimport pandas as pd\nimport sqlalchemy\n\nDRIVER = \"SQL Server\"\nSERVER = \"SERVER\"\nDATABASE = \"DATABASE\"\n\n# Create connection uri.\nconn_uri = f\"mssql+pyodbc://{SERVER}/{DATABASE}?driver={DRIVER}\".replace(\" \", \"+\")\n\n# Initialize connection.\nconn =  sqlalchemy.create_engine(conn_uri)\nA few points to highlight:\n\nconn_uri is a string that contains information needed to connect to our database. The prefix mssql+pyodbc:// indicates that we’re targeting a SQL Server database via the pyodbc connector. Also, if we weren’t using Windows authentication, or were working with a different RDBMS, it would be necessary to change conn_uri. For example, an Oracle connection uri would be specified as oracle://[USERNAME]:[PASSWORD]@[DATABASE].\nAlso in conn_uri, within the format substitution, whitespace in DRIVER is replaced with +. This is consistent with how whitespace is encoded for web addresses.\n\nNext, to query the French Motor Third-Party Liability Claims sample dataset in the table SAMPLE_FREMTPL, use the read_sql function. I’ve included the connection initialization logic for convenience:\n\"\"\"\nReading database data into Pandas DataFrame.\n\"\"\"\nimport pandas as pd\nimport sqlalchemy\n\nDRIVER = \"SQL Server\"\nSERVER = \"SERVER\"\nDATABASE = \"DATABASE\"\n\n# Create connection uri.\nconn_uri = f\"mssql+pyodbc://{SERVER}/{DATABASE}?driver={DRIVER}\".replace(\" \", \"+\")\n\n# Initialize connection.\nconn =  sqlalchemy.create_engine(conn_uri)\n\n# Create query. \nSQL = \"SELECT * FROM SAMPLE_TABLE\"\n\ndf = pd.read_sql(SQL, con=conn)\nInstead of passing a query to pd.read_sql, the tablename could have been provided. pd.read_sql is convenience wrapper around read_sql_table and read_sql_query which will delegate to the specific function depending on the input (dispatches read_sql_table if input is a tablename, read_sql_query if input is a query). Refer to the documentation for more information.\nLet’s assume SAMPLE_TABLE represents the French Motor Third-Party Liability Claims dataset available here. Inspecting the first 10 records of the dataset yields:\n  IDPOL CLAIMNB  EXPOSURE AREA  VEHPOWER VEHAGE  DRIVAGE  BONUSMALUS VEHBRAND     VEHGAS  DENSITY REGION\n0  1290       1   0.66000  'B'         7      0       28          60    'B12'  'Regular'       52  'R72'\n1  1292       1   0.12000  'B'         7      0       28          60    'B12'  'Regular'       52  'R72'\n2  1295       1   0.08000  'E'         5      0       36          50    'B12'  'Regular'     3274  'R11'\n3  1296       1   0.50000  'E'         5      0       36          50    'B12'  'Regular'     3274  'R11'\n4  1297       1   0.20000  'E'         5      0       36          50    'B12'  'Regular'     3274  'R11'\n5  1299       1   0.74000  'D'         6      0       76          50    'B12'  'Regular'      543  'R91'\n6  1301       1   0.05000  'D'         6      0       76          50    'B12'  'Regular'      543  'R91'\n7  1303       1   0.03000  'B'        11      0       39          50    'B12'   'Diesel'       55  'R52'\n8  1304       1   0.76000  'B'        11      0       39          50    'B12'   'Diesel'       55  'R52'\n9  1306       1   0.49000  'E'        10      0       38          50    'B12'  'Regular'     2715  'R93'\n\nIterative Data Retrieval\nWhen working with large datasets, it may be inefficient to retrieve the entire dataset in a single pass. Pandas provides functionality to retrieve data in chunksize-record blocks, which can result in significant speedups. In the following example, the same French Motor Third-Party Liability Claims sample dataset is retrieved in 20,000-record blocks. The only change in the call to read_sql is the inclusion of chunksize, which specifies the maximum number of records to retrieve for a given iteration. We assume conn has already been initialized:\n\"\"\"\nUsing `read_sql`'s *chunksize* parameter for iterative retrieval.\n\"\"\"\nCHUNKSIZE = 20000\nSQL = \"SELECT * FROM SAMPLE_TABLE\"\ndfiter = pd.read_sql(SQL, con=conn, chunksize=CHUNKSIZE)\ndf = pd.concat([dd for dd in dfiter])\n\nCHUNKSIZE specifies the maximum number of records to retrieve at each iteration.\ndfiter is a reference to the data targeted in our query. dfiter is not a DataFrame, rather it is a generator, a Python object which makes it easy to create iterators. Generators yield values lazily, so they are particularly memory efficient.\ndf = pd.concat([dd for dd in dfiter]) can be decomposed into two parts: First, [dd for dd in dfiter] is a list comprehension, a very powerful tool that works similar to a flattened for loop. If we bound [dd for dd in dfiter] to a variable directly, the result would be a list of 34 DataFrames, each having no more than 20,000 records. Second, pd.concat takes the list of DataFrames, and performs a row-wise concatenation of each DataFrame, resulting in a single DataFrame with 678,013 records. pd.concat is akin to the SQL UNION operator. The final result, df, is a DataFrame having 678,013 rows and 12 columns.\n\n\n\nExporting Results to File\nInstead of reading the data into memory, it may be necessary to retrieve the dataset, then write the results to file for later analysis. This can be accomplished in an iterative fashion so that no more than CHUNKSIZE records are in-memory at any point in time. Results will be saved to .csv in a file named \"FREMTPL.csv\" in 100,000 record blocks:\n\"\"\"\nWriting queried results to file. \n\"\"\"\nimport time\n\nCHUNKSIZE = 100000\nCSV_PATH  = \"FREMTPL.csv\"\nSQL = \"SELECT * FROM SAMPLE_TABLE\"\n\ndfiter = pd.read_sql(SQL, conn, chunksize=CHUNKSIZE)\n\nt_i = time.time()\ntrkr, nbrrecs = 0, 0\n\nwith open(CSV_PATH, \"w\", encoding=\"utf-8\", newline=\"\") as fcsv:\n    for df in dfiter:\n        fcsv.write(df.to_csv(header=nbrrecs==0, index=False, mode=\"a\"))\n        nbrrecs+=df.shape[0]\n        print(\"Retrieved records {}-{}\".format((trkr * CHUNKSIZE) + 1, nbrrecs))\n        trkr+=1\n\nt_tot = time.time() - t_i\nretrieval_rate = nbrrecs / t_tot\n\nprint(\n    f\"Retrieved {nbrrecs} records in {t_tot:.0f} seconds ({retrieval_rate:.0f} recs/sec.).\"\n    )\nExecuting the code above produces the following output:\nRetrieved records 1-100000\nRetrieved records 100001-200000\nRetrieved records 200001-300000\nRetrieved records 300001-400000\nRetrieved records 400001-500000\nRetrieved records 500001-600000\nRetrieved records 600001-678013\nRetrieved 678013 records in 20 seconds (33370 recs/sec.).\n\n\nExporting Data\nIn order to export a DataFrame into a database, we leverage the DataFrame’s to_sql method. We provide the name of the table we wish to upload data into, along with a connection object, and what action to take if the table already exists. if_exists can be one of:\n\n“fail”: Raise a ValueError.\n“replace”: Drop the table before inserting new values.\n“append”: Insert new values to the existing table.\n\nAs a simple transformation, we determine aggregate EXPOSURE by AREA, append a timestamp, then export the result as “SAMPLE_AREA_SUMM”. If the table exists, we want the query to fail:\n\"\"\"\nSummary of aggregate EXPOSURE by AREA based on the French Motor Third-Party \nLiability Claims sample dataset.\n\"\"\"\nimport datetime\n\n# Compute aggregate EXPOSURE by AREA.\ndfsumm = df.groupby(\"AREA\", as_index=False)[\"EXPOSURE\"].sum()\n\n# Append timestamp.\ntimestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\ndfsumm[\"TIMESTAMP\"] = timestamp\n\n# Export results.\ndfsumm.to_sql(\"SAMPLE_AREA_SUMM\", con=conn, if_exists=\"fail\")\nIf the table already exists, an error like the following will be generated:\nValueError: Table 'SAMPLE_AREA_SUMM' already exists.\nOtherwise, no output will be generated.\n\n\nWriting Queried Data to Compressed Format\nNext we demonstrate how data can be queried iteratively and written directly to a compressed file format. This is especially useful when working with very large datasets, or when the data exceeds available system resources. Another reason to save datasets in compressed format is that Pandas can read compressed files just as easily as csvs. Once read into memory, the dataset will expand to the full uncompressed size, but by writing data to compressed format we reduce our overall storage footprint. Here’s the code to do it:\nimport gzip\nimport time\nimport pandas as pd\nimport sqlalchemy\n\nDRIVER = \"SQL Server\"\nSERVER = \"SERVER\"\nDATABASE = \"DATABASE\"\nCHUNKSIZE = 100000\nDATA_PATH = \"COMPRESSED-SAMPLE-TABLE.csv.gz\"\n\n# Create connection uri.\nconn_uri = f\"mssql+pyodbc://{SERVER}/{DATABASE}?driver={DRIVER}\".replace(\" \", \"+\")\n\n# Initialize connection.\nconn =  sqlalchemy.create_engine(conn_uri)\nSQL = \"SELECT * FROM SAMPLE_TABLE\"\ndfiter = pd.read_sql(SQL, con=conn, chunksize=CHUNKSIZE)\n\nt_i = time.time()\ntrkr, nbrrecs = 0, 0\nwith gzip.open(DATA_PATH, \"wb\") as fgz:\n    for df in dfiter:\n        fgz.write(df.to_csv(header=nbrrecs==0, index=False, mode=\"a\").encode(\"utf-8\"))\n        nbrrecs+=df.shape[0]\n        print(\"Retrieved records {}-{}\".format((trkr * CHUNKSIZE) + 1, nbrrecs))\n        trkr+=1\n\nt_tot = time.time() - t_i\nretrieval_rate = nbrrecs / t_tot\n\nprint(\n    \"Retrieved {} records in {:.0f} seconds ({:.0f} recs/sec.).\".format(\n        nbrrecs, t_tot, retrieval_rate\n        )\n    ) \nThe only expression requiring explanation is within df.to_csv, where header=nbrrecs==0 is specified. This ensures that headers are written for the first batch of records only, and ignored for subsequent batches (100,000 record chunks are read in at each iteration).\nTo read the compressed file back into Pandas, use the pd.read_csv function specifying the compression type (in this example we used “gzip” - other options are “zip”, “bz2” or “xz”):\nIn [1]: df = pd.read_csv(DATA_PATH, compression=\"gzip\")\nIn [2]: df.shape\nOut[2]: (678013, 12)"
  },
  {
    "objectID": "posts/sql-server-pandas/sql-server-pandas.html",
    "href": "posts/sql-server-pandas/sql-server-pandas.html",
    "title": "Querying SQL Server with Pandas",
    "section": "",
    "text": "Pandas is a powerful, flexible and easy to use open source data analysis and manipulation tool built on top of the Python programming language. It has become the data manipulation library of choice for Machine Learning and Data Science practitioners. Pandas exposes two powerful data structures: Series objects (roughly akin R vectors) represent indexed, homogeneously-typed data. Series objects have a close affinity with numpy ndarray objects, allowing for straightforward conversion from Series to ndarray objects. DataFrame objects are two-dimensional, size-mutable, potentially heterogeneous tabular datasets. DataFrame’s are comprised of one or more Series objects, similar to how a data.frame in R is comprised of one or more vectors.\nIf you research how to connect to a database from Python, many examples use the pyodbc library, which creates a connection to any ODBC-compatible database. However, connections with pyodbc itself are uni-directional: Data can be retrieved, but it cannot be uploaded into the database. To allow for simple, bi-directional database transactions, we use pyodbc along with sqlalchemy, a Python SQL toolkit and Object Relational Mapper that gives application developers the full power and flexibility of SQL. With pyodbc and sqlalchemy together, it becomes possible to retrieve and upload data from Pandas DataFrames with relative ease. Let’s assume we’re interested in connecting to a database running on some known server. A connection using sqlalchemy is created as follows (assuming a SQL Server database):\n\"\"\"\nCreating a database connection with sqlalchemy. \n\"\"\"\nimport pandas as pd\nimport sqlalchemy\n\nDRIVER = \"SQL Server\"\nSERVER = \"SERVER\"\nDATABASE = \"DB\"\n\n# Create connection uri.\nconn_uri = \"mssql+pyodbc://{SERVER}/{DATABASE}?driver={DRIVER}\".replace(\" \", \"+\")\n\n# Initialize connection.\nconn =  sqlalchemy.create_engine(conn_uri)\nA few points to highlight:\n\nconn_uri is a string that contains information needed to connect to our database. The prefix mssql+pyodbc:// indicates that we’re targeting a SQL Server database via the pyodbc connector. Also, if we weren’t using Windows authentication, or were working with a different RDBMS, it would be necessary to change conn_uri. For example, an Oracle connection uri would be specified as oracle://[USERNAME]:[PASSWORD]@[DATABASE].\nAlso in conn_uri, within the format substitution, whitespace in DRIVER is replaced with +. This is consistent with how whitespace is encoded for web addresses.\n\nNext, to query the French Motor Third-Party Liability Claims sample dataset in the table SAMPLE_FREMTPL, use the read_sql function. I’ve included the connection initialization logic for convenience:\n\"\"\"\nReading database data into Pandas DataFrame.\n\"\"\"\nimport pandas as pd\nimport sqlalchemy\n\nDRIVER = \"SQL Server\"\nSERVER = \"SERVER\"\nDATABASE = \"DB\"\n\n# Create connection uri.\nconn_uri = \"mssql+pyodbc://{SERVER}/{DATABASE}?driver={DRIVER}\".replace(\" \", \"+\")\n\n# Initialize connection.\nconn =  sqlalchemy.create_engine(conn_uri)\n\n# Create query. \nSQL = \"SELECT * FROM SAMPLE_FREMTPL\"\n\ndf = pd.read_sql(SQL, con=conn)\nInstead of passing a query to pd.read_sql, the tablename could have been provided. pd.read_sql is convenience wrapper around read_sql_table and read_sql_query which will delegate to the specific function depending on the input (dispatches read_sql_table if input is a tablename, read_sql_query if input is a query). Refer to the documentation for more information.\nInspecting the first 10 records of the dataset yields:\n  IDPOL CLAIMNB  EXPOSURE AREA  VEHPOWER VEHAGE  DRIVAGE  BONUSMALUS VEHBRAND     VEHGAS  DENSITY REGION\n0  1290       1   0.66000  'B'         7      0       28          60    'B12'  'Regular'       52  'R72'\n1  1292       1   0.12000  'B'         7      0       28          60    'B12'  'Regular'       52  'R72'\n2  1295       1   0.08000  'E'         5      0       36          50    'B12'  'Regular'     3274  'R11'\n3  1296       1   0.50000  'E'         5      0       36          50    'B12'  'Regular'     3274  'R11'\n4  1297       1   0.20000  'E'         5      0       36          50    'B12'  'Regular'     3274  'R11'\n5  1299       1   0.74000  'D'         6      0       76          50    'B12'  'Regular'      543  'R91'\n6  1301       1   0.05000  'D'         6      0       76          50    'B12'  'Regular'      543  'R91'\n7  1303       1   0.03000  'B'        11      0       39          50    'B12'   'Diesel'       55  'R52'\n8  1304       1   0.76000  'B'        11      0       39          50    'B12'   'Diesel'       55  'R52'\n9  1306       1   0.49000  'E'        10      0       38          50    'B12'  'Regular'     2715  'R93'\nIn Python, to determine the type of an object, use the type function:\nIn [1]: type(df)\nOut[1]: pandas.core.frame.DataFrame\nTo determine the dimensionality of a DataFrame, access the shape attribute (returned as number of rows by number of columns):\nIn [2]: df.shape\nOut[2]: (678013, 12)\nA single column can be extracted from a DataFrame using in one of two ways: The first approach entails specifying the fieldname as a string within brackets. For example, to reference the AREA column, run:\nIn [3]: area = df[\"AREA\"]\nIn [4]: type(area)\nOut[4]: pandas.core.series.Series\nWe see that area is a Series object. The second method that can be used to extract a column from a DataFrame entails specifying the column name after the DataFrame variable separated by ., similar to how class methods and attributes are accessed in Python. We again reference the AREA column using this approach:\nIn [5]: area2 = df.AREA\nIn [6]: type(area)\nOut[6]: pandas.core.series.Series\nWe see that area2 is also a Series type. We can show that area and area2 are the same by running:\nIn [7]: area.equals(area2)\nAs mentioned in the introduction, converting between Series objects and numpy ndarrays is simple: Call the Series object’s values attribute:\nIn [8]: type(area)\nOut[8]: pandas.core.series.Series\nIn [9]: arr = area.values\nIn[10]: type(arr)\nOut[10]: numpy.ndarray\nOne last point regarding Series objects: To produce a frequency distribution for a particular column, leverage the value_counts method:\nIn[11]: df[\"AREA\"].value_counts()\nOut[11]: \n'A'    103957\n'B'     75459\n'C'    191880\n'D'    151596\n'E'    137167\n'F'     17954\nTo sort results by index, append .sort_index():\nIn[12]: df[\"AREA\"].value_counts().sort_index()\nOut[12]:\n'A'    103957\n'B'     75459\n'C'    191880\n'D'    151596\n'E'    137167\n'F'     17954\nName: AREA, dtype: int64\nNaNs will not be counted in the summary returned by value_counts. If there were NaNs in df[\"AREA\"], the sum of the counts returned by value_counts would be less than the number of rows in df, so it’s a good idea to check if records are being dropped because of NaNs. We can accomplish this as follows:\nIn[13]: df[\"AREA\"].value_counts().sum() == df.shape[0] \nOut[13]: True\n\nIterative Data Retrieval\nWhen working with large datasets, it may be inefficient to retrieve the entire dataset in a single pass. Pandas provides functionality to retrieve data in chunksize blocks, which can result in significant speedups. In the following example, the same French Motor Third-Party Liability Claims sample dataset is retrieved in 20,000-record blocks. The only change in the call to read_sql is the inclusion of chunksize, which specifies the maximum number of records to retrieve for a given iteration. We assume conn has already been initialized:\n\"\"\"\nUsing `read_sql`'s *chunksize* parameter for iterative retrieval.\n\"\"\"\nCHUNKSIZE = 20000\nSQL = \"SELECT * FROM SAMPLE_FREMTPL\"\ndfiter = pd.read_sql(SQL, con=conn, chunksize=CHUNKSIZE)\ndf = pd.concat([dd for dd in dfiter])\n\nCHUNKSIZE specifies the maximum number of records to retrieve at each iteration.\ndfiter is a reference to the data targeted in our query. dfiter is not a DataFrame, rather it is a generator, a Python object which makes it easy to create iterators. Generators yield values lazily, so they are particularly memory efficient.\ndf = pd.concat([dd for dd in dfiter]) can be decomposed into two parts: First, [dd for dd in dfiter] is a list comprehension, a very powerful tool that works similar to a flattened for loop. If we bound [dd for dd in dfiter] to a variable directly, the result would be a list of 34 DataFrames, each having no more than 20,000 records. Second, pd.concat takes the list of DataFrames, and performs a row-wise concatenation of each DataFrame, resulting in a single DataFrame with 678,013 records. pd.concat is akin to the SQL UNION operator. The final result, df, is a DataFrame having 678,013 rows and 12 columns.\n\n\n\nExporting Results to File\nInstead of reading the data into memory, it may be necessary to retrieve the dataset, then write the results to file for later analysis. This can be accomplished in an iterative fashion so that no more than CHUNKSIZE records are in-memory at any point in time. Results will be saved to .csv in a file named \"FREMTPL.csv\" in 100,000 record blocks:\n\"\"\"\nWriting queried results to file. \n\"\"\"\nimport time\n\nCHUNKSIZE = 100000\nCSV_PATH = \"FREMTPL.csv\"\nSQL = \"SELECT * FROM SAMPLE_FREMTPL\"\n\ndfiter = pd.read_sql(SQL, conn, chunksize=CHUNKSIZE)\n\nt_i = time.time()\ntrkr, nbrrecs = 0, 0\nwith open(CSV_PATH, \"w\", encoding=\"utf-8\", newline=\"\") as fcsv:\n\n    for df in dfiter:\n        fcsv.write(df.to_csv(header=nbrrecs==0, index=False, mode=\"a\"))\n        nbrrecs+=df.shape[0]\n        print(\"Retrieved records {}-{}\".format((trkr * CHUNKSIZE) + 1, nbrrecs))\n        trkr+=1\n\nt_tot = time.time() - t_i\nretrieval_rate = nbrrecs / t_tot\n\nprint(\n    \"Retrieved {} records in {:.0f} seconds ({:.0f} recs/sec.).\".format(\n        nbrrecs, t_tot, retrieval_rate\n        )\n    )\nExecuting the code above produces the following output:\nRetrieved records 1-100000\nRetrieved records 100001-200000\nRetrieved records 200001-300000\nRetrieved records 300001-400000\nRetrieved records 400001-500000\nRetrieved records 500001-600000\nRetrieved records 600001-678013\nRetrieved 678013 records in 20 seconds (33370 recs/sec.).\nResults will be available in the file referenced by CSV_PATH.\n\n\nExporting Data\nIn order to export a DataFrame into a database, we leverage the DataFrame’s to_sql method. We provide the name of the table we wish to upload data in, along with a connection object, and what action to take if the table already exists. if_exists can be one of:\n\n“fail”: Raise a ValueError.\n“replace”: Drop the table before inserting new values.\n“append”: Insert new values to the existing table.\n\nAs a simple transformation, we determine aggregate EXPOSURE by AREA, append a timestamp, then export the result as “SAMPLE_AREA_SUMM”. If the table exists, we want the query to fail:\n\"\"\"\nSummary of aggregate EXPOSURE by AREA based on the French Motor Third-Party \nLiability Claims sample dataset.\n\"\"\"\nimport datetime\n\n# Compute aggregate EXPOSURE by AREA.\ndfsumm = df.groupby(\"AREA\", as_index=False)[\"EXPOSURE\"].sum()\n\n# Append timestamp.\ntimestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\ndfsumm[\"TIMESTAMP\"] = timestamp\n\n# Export results.\ndfsumm.to_sql(\"SAMPLE_AREA_SUMM\", con=conn, if_exists=\"fail\")\nIf the table already exists, an error like the following will be generated:\nValueError: Table 'SAMPLE_AREA_SUMM' already exists.\nOtherwise, no output will be generated."
  },
  {
    "objectID": "posts/sklearn-pipelines/sklearn-pipelines.html",
    "href": "posts/sklearn-pipelines/sklearn-pipelines.html",
    "title": "Creating scikit-learn Pipelines",
    "section": "",
    "text": "Within scikit-learn, pipelines allow for the consolidation of all data preprocessing steps along with a final estimator using a single interface. The pipeline object can then be passed into a grid search routine to identify optimal hyperparameters. According to the documentation, the purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. In this post, we’ll demonstrate how to utilize pipelines to preprocess the adult income data set and fit two classifiers to determine whether a given observation has an income in excess of $50,000 given the set of associated features. We first read in the data and inspect the first few records:\n\n\nfrom itertools import zip_longest\nimport os\nimport sys\nimport time\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nimport numpy as np\nimport pandas as pd\n\nimport sklearn\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, RobustScaler\nfrom sklearn.metrics import (\n    accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, \n    confusion_matrix, precision_recall_curve, roc_curve\n    )\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\n\nnp.set_printoptions(suppress=True, precision=8)\npd.options.mode.chained_assignment = None\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 10000)\n\ntrain_path = \"https://gist.githubusercontent.com/jtrive84/13d05ace37948cac9583a9ab1f2def31/raw/3dc5bc9e0b573c1039abc20f816321e570aae69c/adult.csv\"\ndftrain = pd.read_csv(train_path)\n\nprint(dftrain.head())\n\n   age  workclass  fnlwgt     education  educational-num      marital-status         occupation relationship   race  gender  capital-gain  capital-loss  hours-per-week native-country income\n0   25    Private  226802          11th                7       Never-married  Machine-op-inspct    Own-child  Black    Male             0             0              40  United-States  &lt;=50K\n1   38    Private   89814       HS-grad                9  Married-civ-spouse    Farming-fishing      Husband  White    Male             0             0              50  United-States  &lt;=50K\n2   28  Local-gov  336951    Assoc-acdm               12  Married-civ-spouse    Protective-serv      Husband  White    Male             0             0              40  United-States   &gt;50K\n3   44    Private  160323  Some-college               10  Married-civ-spouse  Machine-op-inspct      Husband  Black    Male          7688             0              40  United-States   &gt;50K\n4   18          ?  103497  Some-college               10       Never-married                  ?    Own-child  White  Female             0             0              30  United-States  &lt;=50K\n\n\nAfter loading the dataset, the first task is to get an idea of the frequency of different groups within categorical features. In the next cell, a dictionary is created for each categorical feature which remaps groups to ensure a reasonable number of observations in each:\n\n\ndworkclass = {\n    \"Federal-gov\": \"gov\",\n    \"Local-gov\": \"gov\",          \n    \"Never-worked\": \"other\",    \n    \"Private\": \"private\",\n    \"Self-emp-inc\": \"other\",\n    \"Self-emp-not-inc\": \"other\", \n    \"State-gov\": \"gov\", \n    \"Without-pay\": \"other\",\n    \"missing\": \"missing\"\n    }\n\ndeducation = {\n    \"Preschool\": \"no-hs\",        \n    \"1st-4th\": \"no-hs\",     \n    \"5th-6th\": \"no-hs\",  \n    \"7th-8th\": \"no-hs\",  \n    \"9th\": \"hs\",         \n    \"10th\": \"hs\",               \n    \"11th\": \"hs\",            \n    \"12th\": \"hs\",                 \n    \"HS-grad\": \"hs-grad\",\n    \"Prof-school\": \"some-college\",\n    \"Some-college\": \"some-college\",\n    \"Assoc-acdm\": \"some-college\", \n    \"Assoc-voc\": \"some-college\",   \n    \"Bachelors\": \"bachelors\",    \n    \"Masters\": \"masters\",    \n    \"Doctorate\": \"phd\",   \n    \"missing\": \"missing\"\n    }\n\ndmarital = {\n    \"Divorced\": \"divorced\",  \n    \"Married-AF-spouse\": \"married\", \n    \"Married-civ-spouse\": \"married\", \n    \"Married-spouse-absent\": \"married\", \n    \"Never-married\": \"not-married\", \n    \"Separated\": \"divorced\", \n    \"Widowed\": \"widowed\", \n    \"missing\": \"missing\"\n    }\n\ndoccupation = {\n    \"Adm-clerical\":  \"clerical\", \n    \"Armed-Forces\": \"other\",          \n    \"Craft-repair\": \"repair\",         \n    \"Exec-managerial\": \"managerial\",   \n    \"Farming-fishing\": \"farming\",     \n    \"Handlers-cleaners\": \"cleaners\",  \n    \"Machine-op-inspct\": \"repair\",    \n    \"Other-service\": \"service\",  \n    \"Priv-house-serv\": \"other\",\n    \"Prof-specialty\": \"specialty\",\n    \"Protective-serv\": \"other\",\n    \"Sales\": \"sales\", \n    \"Tech-support\": \"tech\", \n    \"Transport-moving\": \"moving\",\n    \"missing\": \"missing\"\n    }\n\ndoccupation2 = {\n    \"Adm-clerical\":  \"white\", \n    \"Armed-Forces\": \"other\", \n    \"Craft-repair\": \"blue\",  \n    \"Exec-managerial\": \"white\",\n    \"Farming-fishing\": \"blue\", \n    \"Handlers-cleaners\": \"blue\", \n    \"Machine-op-inspct\": \"blue\", \n    \"Other-service\": \"blue\", \n    \"Priv-house-serv\": \"other\", \n    \"Prof-specialty\": \"other\", \n    \"Protective-serv\": \"blue\",\n    \"Sales\": \"white\",  \n    \"Tech-support\": \"white\",\n    \"Transport-moving\": \"blue\",  \n    \"missing\": \"missing\"\n    }\n\ndrelationship = {\n    \"Husband\": \"husband\",  \n    \"Not-in-family\": \"no-family\", \n    \"Other-relative\": \"other\",  \n    \"Own-child\": \"child\",      \n    \"Unmarried\": \"unmarried\",  \n    \"Wife\": \"wife\", \n    \"missing\": \"missing\"\n    }\n\ndrace = {\n    \"Amer-Indian-Eskimo\": \"eskimo\",\n    \"Asian-Pac-Islander\": \"asian\", \n    \"Black\": \"black\", \n    \"Other\": \"other\",\n    \"White\": \"white\",\n    \"missing\": \"missing\"\n    }\n\ndgender = {\n    \"Female\": \"F\",\n    \"Male\": \"M\",\n    \"missing\": \"missing\"\n    }\n\nNext we distinguish between categorical and continuous features. Categorical features are re-mapped to align with the groups defined above. For categorical features, we assign null values to a “missing” category instead of relying on an imputation rule. This allows us to check for possible patterns in the missing data later on. capital-gain and capital-loss are converted into binary indicators and native-country into US vs. non-US. Finally, we split the data into training and validation sets ensuring the same proportion of positive instances in each cut:\n\n\ncategorical = [\n    \"workclass\", \"marital-status\", \"occupation\", \"relationship\",\n    \"race\", \"gender\", \"capital-gain\", \"capital-loss\", \"native-country\"\n    ]\n\ncontinuous = [\n    \"fnlwgt\", \"hours-per-week\", \"age\", \"educational-num\"\n    ]\n\n# workclass.\ndftrain[\"workclass\"] = dftrain[\"workclass\"].fillna(\"missing\")\ndftrain[\"workclass\"] = dftrain[\"workclass\"].map(dworkclass)\n\n# marital-status.\ndftrain[\"marital-status\"] = dftrain[\"marital-status\"].fillna(\"missing\")\ndftrain[\"marital-status\"] = dftrain[\"marital-status\"].map(dmarital)\n\n# occupation.\ndftrain[\"occupation\"] = dftrain[\"occupation\"].fillna(\"missing\")\ndftrain[\"occupation\"] = dftrain[\"occupation\"].map(doccupation)\n\n# relationship.\ndftrain[\"relationship\"] = dftrain[\"relationship\"].fillna(\"missing\")\ndftrain[\"relationship\"] = dftrain[\"relationship\"].map(drelationship)\n\n# race.\ndftrain[\"race\"] = dftrain[\"race\"].fillna(\"missing\")\ndftrain[\"race\"] = dftrain[\"race\"].map(drace)\n\n# sex.\ndftrain[\"gender\"] = dftrain[\"gender\"].fillna(\"missing\")\ndftrain[\"gender\"] = dftrain[\"gender\"].map(dgender)\n\n# capital-gain: Convert to binary indicator. \ndftrain[\"capital-gain\"] = dftrain[\"capital-gain\"].map(lambda v: 1 if v &gt; 0 else 0)\n\n# capital-loss: Convert to binary indicator. \ndftrain[\"capital-loss\"] = dftrain[\"capital-loss\"].map(lambda v: 1 if v &gt; 0 else 0)\n\n# Encode native-country.\ndftrain[\"native-country\"] = dftrain[\"native-country\"].map(lambda v: \"US\" if v == \"United-States\" else \"other\")\n\n# Encode response.\ndftrain[\"income\"] = dftrain[\"income\"].map(lambda v: 1 if v == \"&gt;50K\" else 0)\n\n# Create train and validation sets. \ny = dftrain[\"income\"]\ndft, dfv, yt, yv = train_test_split(dftrain, y, test_size=.125, stratify=y)\n\nprint(f\"dft.shape: {dft.shape}\")\nprint(f\"dfv.shape: {dfv.shape}\")\nprint(f\"prop. yt : {yt.sum() / dft.shape[0]:.4f}\")\nprint(f\"prop. yv : {yv.sum() / dfv.shape[0]:.4f}\")\n\ndft.shape: (42736, 15)\ndfv.shape: (6106, 15)\nprop. yt : 0.2393\nprop. yv : 0.2393\n\n\nWith categorical features re-mapped, it is useful to look at the proportion of positive instances in each group per feature:\n\n\nindices = [\n    (0, 0), (0, 1), (0, 2), \n    (1, 0), (1, 1), (1, 2),\n    (2, 0), (2, 1), (2, 2), \n    ]\n\nfig, ax = plt.subplots(3, 3, figsize=(9, 7), tight_layout=True) \n\nfor (ii, jj), col in zip_longest(indices, categorical):\n\n    if col is None:\n        ax[ii, jj].remove()\n    else:\n        gg = dftrain.groupby(col, as_index=False).agg(\n            leq50k=(\"income\", lambda v: v[v==0].size),\n            gt50k=(\"income\", \"sum\")\n            ).sort_values(col, ascending=True)\n        \n        if col in [\"education-num\", \"capital-gain\", \"capital-loss\"]:\n            gg[col] = gg[col].astype(str)\n\n        if col == \"occupation\":\n            rot = 25\n        else:\n            rot = 0\n\n        gg.plot.bar(ax=ax[ii, jj])\n        ax[ii, jj].set_title(col, fontsize=8, weight=\"bold\")\n        ax[ii, jj].set_xticklabels(gg[col].values, rotation=rot)\n        ax[ii, jj].yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter(\"{x:,.0f}\"))\n        ax[ii, jj].tick_params(axis=\"x\", which=\"major\", direction='in', labelsize=6)\n        ax[ii, jj].tick_params(axis=\"x\", which=\"minor\", direction='in', labelsize=6)\n        ax[ii, jj].tick_params(axis=\"y\", which=\"major\", direction='in', labelsize=6)\n        ax[ii, jj].tick_params(axis=\"y\", which=\"minor\", direction='in', labelsize=6)\n        ax[ii, jj].xaxis.set_ticks_position(\"none\")\n        ax[ii, jj].yaxis.set_ticks_position(\"none\")\n        ax[ii, jj].legend(loc=\"best\", fancybox=True, framealpha=1, fontsize=\"x-small\")\n        ax[ii, jj].grid(True)   \n        ax[ii, jj].set_axisbelow(True) \n\nplt.show()\n\n\n\n\n\n\n\n\nFrom the generated plot, we take-away the following:\n\neducation-num: Higher percentage of “&gt;50k” for levels &gt;= 13.\nmaritial-status: Higher proportion of “&gt;50k” for married vs. all other groups.\nsex: Higher proportion of “&gt;50k” for Males vs. Females.\noccupation: Higher proportion of “&gt;50k” for managerial and specialty.\n\nA similar exhibit for continuous features gives us an idea of the distribution of values in each:\n\n\nindices = [0, 1, 2, 3]\n\nfig, ax = plt.subplots(1, 4, figsize=(10, 3), tight_layout=True) \n\nfor ii, col in zip_longest(indices, continuous):\n    ax[ii].set_title(col, fontsize=8, weight=\"bold\")\n    ax[ii].hist(\n        dft[col], 16, density=True, alpha=1, color=\"#E02C70\", \n        edgecolor=\"#FFFFFF\", linewidth=1.0\n        )\n    #ax[ii].yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter(\"{x:,.0f}\"))\n    ax[ii].tick_params(axis=\"x\", which=\"major\", direction='in', labelsize=6)\n    ax[ii].tick_params(axis=\"x\", which=\"minor\", direction='in', labelsize=6)\n    ax[ii].tick_params(axis=\"y\", which=\"major\", direction='in', labelsize=6)\n    ax[ii].tick_params(axis=\"y\", which=\"minor\", direction='in', labelsize=6)\n    ax[ii].xaxis.set_ticks_position(\"none\")\n    ax[ii].yaxis.set_ticks_position(\"none\")\n    # ax[ii].legend(loc=\"best\", fancybox=True, framealpha=1, fontsize=\"x-small\")\n    ax[ii].grid(True)   \n    ax[ii].set_axisbelow(True) \n\nplt.show()\n\n\n\n\n\n\n\n\nWe are now in a position to create our pipelines. The first pipeline is created to support a logistic regression classifier. We initialize a ColumnTransformer instance, which gives us the ability to define separate preprocessing steps for different groups of columns (in our case, categorical vs. continuous). As the logistic regression classifier doesn’t support categorical features, we one-hot encode them. In addition, since the logistic regression classifier relies on gradient descent to estimate coefficients, continuous features are scaled using RobustScaler to help with convergence and missing values imputed using IterativeImputer. For the classifier, we use the elasticnet penalty, which is a blend of lasso and ridge penalties. We’ll determine the optimal weighting using grid search.\n\nfrom sklearn.linear_model import LogisticRegression\n\n# Data pre-processing for LogisticRegression model.\nlr = LogisticRegression(\n    penalty=\"elasticnet\", solver=\"saga\", max_iter=5000\n    )\ncontinuous_transformer1 = Pipeline(steps=[\n    (\"imputer\", IterativeImputer()),\n    (\"scaler\" , RobustScaler())\n    ])\ncategorical_transformer1 = Pipeline(steps=[\n    (\"onehot\", OneHotEncoder(drop=\"first\", sparse_output=False, handle_unknown=\"error\"))\n    ])\npreprocessor1 = ColumnTransformer(transformers=[\n    (\"continuous\" , continuous_transformer1, continuous),  \n    (\"categorical\", categorical_transformer1, categorical)\n    ], remainder=\"drop\"\n    )\npipeline1 = Pipeline(steps=[\n    (\"preprocessor\", preprocessor1),\n    (\"classifier\", lr)\n    ]).set_output(transform=\"pandas\")\n\nNotice that set_output is affixed to pipeline1 by specifying transform=\"pandas\". This was added in scikit-learn version 1.2, and allows intermediate and final datasets to be represented as Pandas DataFrames instead of Numpy arrays. I’ve found this to be particularly convenient, especially when inspecting the results of a transformation.\nA different set of preprocessing steps is carried out for the HistGradientBoostingClassifier instance, which is functionally equivalent to lightgbm. Since HistGradientBoostingClassifier supports categorical features, it isn’t necessary to one-hot encode: We pass a list of columns that should be treated as nominal categorical features to the categorical_features parameter. Coming out of ColumnTransformer, categorical features are renamed with a leading categorical__, so it is easy to identify which columns to pass. As before, IterativeImputer is used to impute missing continuous values. Within categorical_transformer2, we pass OrdinalEncoder to convert non-numeric categories to integers, which can then be processed by HistGradientBoostingClassifier. Since HistGradientBoostingClassifier doesn’t rely on gradient descent, it isn’t necessary to include RobustScalerin continuous_transformer2.\n\n\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\n# Data pre-processing for HistGradientBoostingClassifier model. Uses OrdinalEncoder\n# instead of OneHotEncoder since categorical features are supported.     \ngb = HistGradientBoostingClassifier(\n    categorical_features=[f\"categorical__{ii}\" for ii in categorical]\n    )\ncontinuous_transformer2 = Pipeline(steps=[\n    (\"imputer\", IterativeImputer())\n    ])\ncategorical_transformer2 = Pipeline(steps=[\n    (\"encoder\", OrdinalEncoder())\n    ])\npreprocessor2 = ColumnTransformer(transformers=[\n    (\"continuous\" , continuous_transformer2, continuous), \n    (\"categorical\", categorical_transformer2, categorical),\n    ], remainder=\"drop\"\n    )\npipeline2 = Pipeline(steps=[\n    (\"preprocessor\", preprocessor2), \n    (\"classifier\", gb)\n    ]).set_output(transform=\"pandas\")\n\nInstead og using GridSearchCV, we leverage RandomizedSearchCV. GridSearchCV evaluates a multi-dimensional array of hyperparameters, whereas RandomizedSearchCV samples from a pre-specified distribution a defined number of samples. For our logistic regression classifier, we sample uniformly from [0, 1] for l1_ratio and [0, 10] for the regularization parameter C.\n\n\nfrom scipy.stats import uniform\n\nRANDOM_STATE = 516\nverbosity = 3\nn_iter = 3\nscoring = \"accuracy\"\ncv = 5\n\nparam_grid1 = {\n    \"classifier__l1_ratio\": uniform(loc=0, scale=1), \n    \"classifier__C\": uniform(loc=0, scale=10)\n    }\n\nmdl1 = RandomizedSearchCV(\n    pipeline1, param_grid1, scoring=scoring, cv=cv, verbose=verbosity, \n    random_state=RANDOM_STATE, n_iter=n_iter\n    )\n\nmdl1.fit(dft.drop(\"income\", axis=1), yt)\n\nprint(f\"\\nbest parameters: {mdl1.best_params_}\")\n\n# Get holdout scores for each fold to compare against other model.\nbest_rank1 = np.argmin(mdl1.cv_results_[\"rank_test_score\"])\nbest_mdl_cv_scores1 = [\n    mdl1.cv_results_[f\"split{ii}_test_score\"][best_rank1] for ii in range(cv)\n    ]\n\n\ndfv[\"ypred1\"] = mdl1.predict_proba(dfv.drop(\"income\", axis=1))[:, 1]\ndfv[\"yhat1\"] = dfv[\"ypred1\"].map(lambda v: 1 if v &gt;= .50 else 0)\n\nmdl1_acc = accuracy_score(dfv[\"income\"], dfv[\"yhat1\"])\nmdl1_precision = precision_score(dfv[\"income\"], dfv[\"yhat1\"])\nmdl1_recall = recall_score(dfv[\"income\"], dfv[\"yhat1\"])\n\nprint(f\"\\nmdl1_acc      : {mdl1_acc}\")\nprint(f\"mdl1_precision: {mdl1_precision}\")\nprint(f\"mdl1_recall   : {mdl1_recall}\")\n\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV 1/5] END classifier__C=8.115660497752215, classifier__l1_ratio=0.7084090612742915;, score=0.841 total time=   2.3s\n[CV 2/5] END classifier__C=8.115660497752215, classifier__l1_ratio=0.7084090612742915;, score=0.840 total time=   1.7s\n[CV 3/5] END classifier__C=8.115660497752215, classifier__l1_ratio=0.7084090612742915;, score=0.843 total time=   2.6s\n[CV 4/5] END classifier__C=8.115660497752215, classifier__l1_ratio=0.7084090612742915;, score=0.843 total time=   1.8s\n[CV 5/5] END classifier__C=8.115660497752215, classifier__l1_ratio=0.7084090612742915;, score=0.850 total time=   1.7s\n[CV 1/5] END classifier__C=1.115284252761577, classifier__l1_ratio=0.5667878644753359;, score=0.841 total time=   1.6s\n[CV 2/5] END classifier__C=1.115284252761577, classifier__l1_ratio=0.5667878644753359;, score=0.840 total time=   1.5s\n[CV 3/5] END classifier__C=1.115284252761577, classifier__l1_ratio=0.5667878644753359;, score=0.843 total time=   3.4s\n[CV 4/5] END classifier__C=1.115284252761577, classifier__l1_ratio=0.5667878644753359;, score=0.843 total time=   1.6s\n[CV 5/5] END classifier__C=1.115284252761577, classifier__l1_ratio=0.5667878644753359;, score=0.850 total time=   1.3s\n[CV 1/5] END classifier__C=7.927782545875722, classifier__l1_ratio=0.8376069301429002;, score=0.841 total time=   1.7s\n[CV 2/5] END classifier__C=7.927782545875722, classifier__l1_ratio=0.8376069301429002;, score=0.840 total time=   1.7s\n[CV 3/5] END classifier__C=7.927782545875722, classifier__l1_ratio=0.8376069301429002;, score=0.843 total time=   2.3s\n[CV 4/5] END classifier__C=7.927782545875722, classifier__l1_ratio=0.8376069301429002;, score=0.843 total time=   1.9s\n[CV 5/5] END classifier__C=7.927782545875722, classifier__l1_ratio=0.8376069301429002;, score=0.850 total time=   1.7s\n\nbest parameters: {'classifier__C': 1.115284252761577, 'classifier__l1_ratio': 0.5667878644753359}\n\nmdl1_acc      : 0.8435964624959057\nmdl1_precision: 0.7184801381692574\nmdl1_recall   : 0.5694729637234771\n\n\nWe proceed analogously for HistGradientBoostingClassifier, but sample from different hyperparameters.\n\n\nRANDOM_STATE = 516\nscoring = \"accuracy\"\nverbosity = 3\nn_iter = 3\ncv = 5\n\n\nparam_grid2 = {\n    \"classifier__max_iter\": [100, 250, 500],\n    \"classifier__min_samples_leaf\": [10, 20, 50, 100],\n    \"classifier__l2_regularization\": uniform(loc=0, scale=1000),\n    \"classifier__learning_rate\": [.01, .05, .1, .25, .5],\n    \"classifier__max_leaf_nodes\": [None, 20, 31, 40, 50]\n    }\n\nmdl2 = RandomizedSearchCV(\n    pipeline2, param_grid2, scoring=scoring, cv=cv, verbose=verbosity, \n    random_state=RANDOM_STATE, n_iter=n_iter\n    )\n\nmdl2.fit(dft.drop(\"income\", axis=1), yt)\n\nprint(f\"\\nbest parameters: {mdl2.best_params_}\")\n\n# Get holdout scores for each fold to compare against other model.\nbest_rank2 = np.argmin(mdl2.cv_results_[\"rank_test_score\"])\nbest_mdl_cv_scores2 = [\n    mdl2.cv_results_[f\"split{ii}_test_score\"][best_rank2] for ii in range(cv)\n    ]\n\ndfv[\"ypred2\"] = mdl2.predict_proba(dfv.drop(\"income\", axis=1))[:, 1]\ndfv[\"yhat2\"] = dfv[\"ypred2\"].map(lambda v: 1 if v &gt;= .50 else 0)\n\n\nmdl2_acc = accuracy_score(dfv[\"income\"], dfv[\"yhat2\"])\nmdl2_precision = precision_score(dfv[\"income\"], dfv[\"yhat2\"])\nmdl2_recall = recall_score(dfv[\"income\"], dfv[\"yhat2\"])\n\nprint(f\"\\nmdl2_acc      : {mdl2_acc}\")\nprint(f\"mdl2_precision: {mdl2_precision}\")\nprint(f\"mdl2_recall   : {mdl2_recall}\")\n\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV 1/5] END classifier__l2_regularization=811.5660497752214, classifier__learning_rate=0.25, classifier__max_iter=500, classifier__max_leaf_nodes=None, classifier__min_samples_leaf=50;, score=0.846 total time=   1.1s\n[CV 2/5] END classifier__l2_regularization=811.5660497752214, classifier__learning_rate=0.25, classifier__max_iter=500, classifier__max_leaf_nodes=None, classifier__min_samples_leaf=50;, score=0.848 total time=   1.5s\n[CV 3/5] END classifier__l2_regularization=811.5660497752214, classifier__learning_rate=0.25, classifier__max_iter=500, classifier__max_leaf_nodes=None, classifier__min_samples_leaf=50;, score=0.849 total time=   1.1s\n[CV 4/5] END classifier__l2_regularization=811.5660497752214, classifier__learning_rate=0.25, classifier__max_iter=500, classifier__max_leaf_nodes=None, classifier__min_samples_leaf=50;, score=0.849 total time=   0.9s\n[CV 5/5] END classifier__l2_regularization=811.5660497752214, classifier__learning_rate=0.25, classifier__max_iter=500, classifier__max_leaf_nodes=None, classifier__min_samples_leaf=50;, score=0.852 total time=   1.0s\n[CV 1/5] END classifier__l2_regularization=138.5495352566758, classifier__learning_rate=0.1, classifier__max_iter=100, classifier__max_leaf_nodes=50, classifier__min_samples_leaf=10;, score=0.845 total time=   0.6s\n[CV 2/5] END classifier__l2_regularization=138.5495352566758, classifier__learning_rate=0.1, classifier__max_iter=100, classifier__max_leaf_nodes=50, classifier__min_samples_leaf=10;, score=0.846 total time=   0.6s\n[CV 3/5] END classifier__l2_regularization=138.5495352566758, classifier__learning_rate=0.1, classifier__max_iter=100, classifier__max_leaf_nodes=50, classifier__min_samples_leaf=10;, score=0.849 total time=   0.6s\n[CV 4/5] END classifier__l2_regularization=138.5495352566758, classifier__learning_rate=0.1, classifier__max_iter=100, classifier__max_leaf_nodes=50, classifier__min_samples_leaf=10;, score=0.849 total time=   0.6s\n[CV 5/5] END classifier__l2_regularization=138.5495352566758, classifier__learning_rate=0.1, classifier__max_iter=100, classifier__max_leaf_nodes=50, classifier__min_samples_leaf=10;, score=0.854 total time=   0.6s\n[CV 1/5] END classifier__l2_regularization=189.1538419557398, classifier__learning_rate=0.1, classifier__max_iter=100, classifier__max_leaf_nodes=20, classifier__min_samples_leaf=20;, score=0.846 total time=   0.4s\n[CV 2/5] END classifier__l2_regularization=189.1538419557398, classifier__learning_rate=0.1, classifier__max_iter=100, classifier__max_leaf_nodes=20, classifier__min_samples_leaf=20;, score=0.848 total time=   0.5s\n[CV 3/5] END classifier__l2_regularization=189.1538419557398, classifier__learning_rate=0.1, classifier__max_iter=100, classifier__max_leaf_nodes=20, classifier__min_samples_leaf=20;, score=0.852 total time=   0.4s\n[CV 4/5] END classifier__l2_regularization=189.1538419557398, classifier__learning_rate=0.1, classifier__max_iter=100, classifier__max_leaf_nodes=20, classifier__min_samples_leaf=20;, score=0.850 total time=   0.4s\n[CV 5/5] END classifier__l2_regularization=189.1538419557398, classifier__learning_rate=0.1, classifier__max_iter=100, classifier__max_leaf_nodes=20, classifier__min_samples_leaf=20;, score=0.855 total time=   0.6s\n\nbest parameters: {'classifier__l2_regularization': 189.1538419557398, 'classifier__learning_rate': 0.1, 'classifier__max_iter': 100, 'classifier__max_leaf_nodes': 20, 'classifier__min_samples_leaf': 20}\n\nmdl2_acc      : 0.8524402227317392\nmdl2_precision: 0.7348993288590604\nmdl2_recall   : 0.5995893223819302\n\n\nNotice that mdl1 and mdl2 expose predict/predict_proba methods, so we can generate predictions using the resulting RandomizedSearchCV object directly, and it will dispatch a call to the estimator associated with the hyperparameters that maximize accuracy.\nPrecision, recall and accuracy are close for each model. We can check if the difference between models is significant using the approach outlined here:\n\n\nfrom scipy.stats import t\n\ndef corrected_std(differences, n_train, n_test):\n    \"\"\"\n    Corrects standard deviation using Nadeau and Bengio's approach.\n    \"\"\"\n    kr = len(differences)\n    corrected_var = np.var(differences, ddof=1) * (1 / kr + n_test / n_train)\n    corrected_std = np.sqrt(corrected_var)\n    return(corrected_std)\n\n\ndef compute_corrected_ttest(differences, df, n_train, n_test):\n    \"\"\"\n    Computes right-tailed paired t-test with corrected variance.\n    \"\"\"\n    mean = np.mean(differences)\n    std = corrected_std(differences, n_train, n_test)\n    t_stat = mean / std\n    p_val = t.sf(np.abs(t_stat), df)  # right-tailed t-test\n    return(t_stat, p_val)\n\n\ndifferences = np.asarray(best_mdl_cv_scores2) - np.asarray(best_mdl_cv_scores1)\nn = len(differences)\ndf = n - 1\nn_train = 4 * (dft.shape[0] // 5) \nn_test = dft.shape[0] // 5\n\nt_stat, p_val = compute_corrected_ttest(differences, df, n_train, n_test)\n\nprint(f\"t-value: {t_stat:.3f}\")\nprint(f\"p-value: {p_val:.3f}\")\n\nt-value: 5.231\np-value: 0.003\n\n\nAt a significance alpha level at p=0.05, the test concludes that HistGradientBoostingClassifier is significantly better than the LogisticRegression model.\nFinally, we can overlay the histograms of model predictions by true class:\n\n\ncolor0 = \"#E02C70\"\ncolor1 = \"#6EA1D5\"\nalpha = .65\nn_bins = 12\n\nfig, ax = plt.subplots(1, 2, figsize=(9, 4), tight_layout=True) \n\n# LogisticRegression.\nyy0 = dfv[dfv.income==0][\"ypred1\"].values\nyy1 = dfv[dfv.income==1][\"ypred1\"].values\nax[0].set_title(\n    f\"LogisticRegression  (acc={mdl1_acc:.3f})\", \n    fontsize=9, weight=\"normal\"\n    )\nax[0].hist(\n    yy0, n_bins, density=True, alpha=alpha, color=color0, \n    edgecolor=\"#000000\", linewidth=1.0, label=\"&lt;=50K\"\n    )\nax[0].hist(\n    yy1, n_bins, density=True, alpha=alpha, color=color1,\n    edgecolor=\"#000000\", linewidth=1.0, label=\"&gt;50K\"\n    )\nax[0].tick_params(axis=\"x\", which=\"major\", direction='in', labelsize=6)\nax[0].tick_params(axis=\"x\", which=\"minor\", direction='in', labelsize=6)\nax[0].tick_params(axis=\"y\", which=\"major\", direction='in', labelsize=6)\nax[0].tick_params(axis=\"y\", which=\"minor\", direction='in', labelsize=6)\nax[0].xaxis.set_ticks_position(\"none\")\nax[0].yaxis.set_ticks_position(\"none\")\nax[0].set_yticklabels([])\nax[0].legend(loc=\"best\", fancybox=True, framealpha=1, fontsize=\"small\")\nax[0].grid(True)   \nax[0].set_axisbelow(True) \n\n# HistGradientBoostingClassifier.\nyy0 = dfv[dfv.income==0][\"ypred2\"].values\nyy1 = dfv[dfv.income==1][\"ypred2\"].values\nax[1].set_title(\n    f\"HistGradientBoostingClassifier (acc={mdl2_acc:.3f})\", \n    fontsize=9, weight=\"normal\"\n    )\nax[1].hist(\n    yy0, n_bins, density=True, alpha=alpha, color=color0, \n    edgecolor=\"#000000\", linewidth=1.0, label=\"&lt;=50K\"\n    )\nax[1].hist(\n    yy1, n_bins, density=True, alpha=alpha, color=color1,\n    edgecolor=\"#000000\", linewidth=1.0, label=\"&gt;50K\"\n    )\nax[1].tick_params(axis=\"x\", which=\"major\", direction='in', labelsize=6)\nax[1].tick_params(axis=\"x\", which=\"minor\", direction='in', labelsize=6)\nax[1].tick_params(axis=\"y\", which=\"major\", direction='in', labelsize=6)\nax[1].tick_params(axis=\"y\", which=\"minor\", direction='in', labelsize=6)\nax[1].xaxis.set_ticks_position(\"none\")\nax[1].yaxis.set_ticks_position(\"none\")\nax[1].set_yticklabels([])\nax[1].legend(loc=\"best\", fancybox=True, framealpha=1, fontsize=\"small\")\nax[1].grid(True)   \nax[1].set_axisbelow(True) \n\nplt.show()"
  },
  {
    "objectID": "posts/rolling-joins-r/rolling-joins-r.html",
    "href": "posts/rolling-joins-r/rolling-joins-r.html",
    "title": "Performing Rolling Joins with data.table",
    "section": "",
    "text": "A rolling join refers to a situation in which two or more tables need to be associated, but there isn’t a direct correspondence of values in each table’s key column(s). For example, assume we have a table that represents thresholds in which to group claims based on loss amount:\nlibrary(\"data.table\")\n\nDF1 = data.table(\n    group=c(\"A\", \"B\", \"C\", \"D\", \"E\"),\n    loss=c(0, 10000, 20000, 30000, 40000),\n    stringsAsFactors=FALSE\n    )\nReviewing the contents of DF1:\n   group      loss\n1:     A         0\n2:     B     10000\n3:     C     20000\n4:     D     30000\n5:     E     40000\nLet’s also assume we have a table of claims, DF2:\nDF2 = data.table(\n    claimno=paste0(\"000\", 10:20),\n    loss=c(8101, 15700, 64140, 20000, 11655, 31850, 23680, 41440, 16161, 77000, 4564),\n    stringsAsFactors=FALSE\n    )\nReviewing the contents of DF2:\n    claimno  loss\n 1:   00010  8101\n 2:   00011 15700\n 3:   00012 64140\n 4:   00013 20000\n 5:   00014 11655\n 6:   00015 31850\n 7:   00016 23680\n 8:   00017 41440\n 9:   00018 16161\n10:   00019 77009\n11:   00020  4564\nThe goal is for each claimno in DF2, assign the corresponding group from DF1 such that loss threshold (from DF1) is the maximum value less than or equal to loss from DF2. For example, a loss amount of 15700 should be assigned to group B, since 10000 is the maximum loss threshold less than or equal to 15700.\nThe way many people first attack this problem is to use a deeply nested sequence of ifelse statements. Something akin to:\nDF2[,\n    group:=\n        ifelse(loss&gt;=0 & loss&lt;10000, \"A\",\n            ifelse(loss&gt;=10000 & loss&lt;20000, \"B\",\n                ifelse(loss&gt;=20000 & loss&lt;30000, \"C\",\n                    ifelse(loss&gt;=30000 & loss&lt;40000, \"D\",\n                    \"E\"\n                    )\n                )\n            )\n        )   \n    ]\nWhich results in:\n    claimno  loss group\n 1:   00010  8101     A\n 2:   00011 15700     B\n 3:   00012 64140     E\n 4:   00013 20000     C\n 5:   00014 11655     B\n 6:   00015 31850     D\n 7:   00016 23680     C\n 8:   00017 41440     E\n 9:   00018 16161     B\n10:   00019 77000     E\n11:   00020  4564     A\nThis solution works, but is not optimal for a number of reasons. First, it’s overly verbose and brittle. If the number of groups changes from 5 to 10 or 15, it becomes necessary to extend the nesting of ifelses by the number of new groups. One should always try to avoid writing code that requires updates in proportion to the size of the input. Perhaps more importantly, this approach has poor runtime performance, which we demonstrate later on.\n\nRolling Joins\nPerforming a rolling join in data.table is straightforward. Simply add the roll modifier within the join expression, specifying either +Inf (or TRUE) or -Inf to specify the direction in which to roll. Sticking with the same DF1 and DF2 from before, we create a new table DF, which represents DF2 along with the target group associated with each claimno:\nDF = DF1[DF2, on=\"loss\", roll=+Inf]\nResulting in:\n    group  loss claimno\n 1:     A  8101   00010\n 2:     B 15700   00011\n 3:     E 64140   00012\n 4:     C 20000   00013\n 5:     B 11655   00014\n 6:     D 31850   00015\n 7:     C 23680   00016\n 8:     E 41440   00017\n 9:     B 16161   00018\n10:     E 77000   00019\n11:     A  4564   00020\nNote that in this example, the key column loss is the same in both tables. If this was not the case, say, threshold in DF1 vs. loss in DF2, one would specify on=c(\"threshold\"=\"loss\").\nFor completeness, let’s see what happens if switch to roll=-Inf (assume we changed loss to threshold in DF1):\nDF = DF1[DF2, on=c(\"threshold\"=\"loss\"), roll=-Inf]\nResulting in:\n   group threshold claimno\n 1:     B      8101   00010\n 2:     C     15700   00011\n 3:  &lt;NA&gt;     64140   00012\n 4:     C     20000   00013\n 5:     C     11655   00014\n 6:     E     31850   00015\n 7:     D     23680   00016\n 8:  &lt;NA&gt;     41440   00017\n 9:     C     16161   00018\n10:  &lt;NA&gt;     77000   00019\n11:     B      4564   00020\nAny value in excess of the largest threshold gets set to NA, and all other claims get set to the minimum threshold from DF1 greater than or equal to loss in DF1.\n\n\nPerformance Comparison\nTo demonstrate to difference in performance, we generate a new DF2 with one million random claim amounts. We’ll then compare the performance between the naive initial implementation and the rolling join implementation. To make it easier to use with the microbenchmark profiling tool, each implementation is encapsulated within separate functions:\nlibrary(\"data.table\")\nlibrary(\"microbenchmark\")\n\nDF1 = data.table(\n    group=c(\"A\", \"B\", \"C\", \"D\", \"E\"),\n    loss=c(0, 10000, 20000, 30000, 40000),\n    stringsAsFactors=FALSE\n    )\n\nDF2 = data.table(\n    claimno=formatC(1:1000000, format=\"d\", width=7, flag=0),\n    loss=rgamma(n=1000000, shape=1, scale=25000),\n    stringsAsFactors=FALSE\n    )\n\n# Create copies to operate on for each implementation. \nmethod1DF = data.table::copy(DF2)\nmethod2DF = data.table::copy(DF2)\n\n\nfmethod1 = function() {\n    # First method. \n    method1DF[,\n        group:=\n            ifelse(loss&gt;=0 & loss&lt;10000, \"A\",\n                ifelse(loss&gt;=10000 & loss&lt;20000, \"B\",\n                    ifelse(loss&gt;=20000 & loss&lt;30000, \"C\",\n                        ifelse(loss&gt;=30000 & loss&lt;40000, \"D\",\n                        \"E\"\n                        )\n                    )\n                )\n            )   \n        ]\n}\n\n\nfmethod2 = function() {\n    # Second method.\n    DF = DF1[method2DF, on=c(\"loss\"), roll=+Inf]\n}\n\n# Run comparison 10 times, compare max result from each. \nmicrobenchmark(\n    fmethod1(), \n    fmethod2(), \n    times=10\n    )\nThe results from microbenchmark are provided below:\nUnit: milliseconds\n       expr       min        lq      mean    median        uq       max neval\n fmethod1() 2116.3529 2212.5053 2518.0355 2558.7205 2779.6253 3061.8588    10\n fmethod2()  494.8094  536.9963  622.7095  586.1551  677.1586  825.6939    10\nIn the worst case, the rolling join approach is almost 4 times faster, and as the number of records increases, so does the relative performance improvement between the two methods."
  },
  {
    "objectID": "posts/r-packages/r-packages.html",
    "href": "posts/r-packages/r-packages.html",
    "title": "Creating R Packages with devtools and roxygen",
    "section": "",
    "text": "If you have a collection of user-defined functions written in R, it’s a good practice to compile the functionality into an R package which can then be loaded into any working session. This certainly beats copying and pasting the source from project to project, and makes it straightforward to share/distribute the functionality to other users and/or machines.\nIn this post I’ll walk through creating an R package using the devtools and roxygen2 libraries, which make the packaging process straightforward and intuitive. For our sample library, we’re going to compile a package from a collection of three functions: A recursive factorial function, a combination function and a permutation function, which will be identified as combinatronics. Here’s the contents of combinatronics.R:\n# combinatronics.R ====================================================&gt;\n#                                                                      |\n#    rfactorial(n) =&gt; Compute the factorial (recursively) of `n`       |\n#                     returns =&gt; int &gt; 0                               |\n#                                                                      |\n#    nCr(n, r)     =&gt; Compute the combination of `n C r`: n!/k!(n-k)!  |\n#                     returns =&gt; int &gt; 0                               |\n#                                                                      |\n#    nPr(n, r)     =&gt; Compute the permutation of `n P r`: n!/(n-k)!    |\n#                     returns =&gt; int &gt; 0                               |\n#                                                                      |\n# =====================================================================&gt;\n\n\nrfactorial = function(n) {\n    if (n&lt;=1) {\n        return(1)\n    } else {\n        return(n*rfactorial(n-1))\n    }\n}\n\n\nnCr = function(n, r) {\n\n    return(rfactorial(n)/(rfactorial(r)*(rfactorial(n-r))))\n\n}\n\n\nnPr = function(n, r) {\n\n    return(rfactorial(n)/(rfactorial(n-r)))\n\n}\nThe devtools library exposes the create function, which automates the setup of new source packages. Pass a directory location with the desired package name appended to create, and devtools will generate the required files and directories for a new source package (note that create requires that the directory doesn’t yet exist).\nFor example, to initialize the combinatronics library package in the U:/Dev folder, you’d run the following from the R interpreter:\n&gt; create(\"U:/Dev/combinatronics\") \nIn U:/Dev/combinatronics, the following directory tree is created:\n#[U:]\n#   \\\n#  [Dev]\n#      \\\n#    [combinatronics]\n#         \\\n#         R                     &lt;dir&gt;\n#         .gitignore           &lt;file&gt;\n#         .Rbuildignore        &lt;file&gt;\n#         combinatronics.Rproj &lt;file&gt;\n#         DESCRIPTION          &lt;file&gt;\n#         NAMESPACE            &lt;file&gt;\nPopulate as much information as you’d like in the DESCRIPTION file. At minimum, provide an email address so users can report bugs and/or provide feedback.\nCopy the source file combinatronics.R, into the R directory created under U:/Dev/combinatronics.\nNext we’ll annotate the three functions in combinatronics.R in a way that can be parsed by roxygen2. After running this step, documentation will be generated that conforms to the R style, which can then be accessed like all other builtin or third-party library help files. The format is best demonstrated by example. Here’s combinatronics.R with roxygen2-compatible function annotations:\n# combinatronics.R ====================================================&gt;\n#                                                                      |\n#    rfactorial(n) =&gt; Compute the factorial (recursively) of `n`       |\n#                     returns =&gt; int &gt; 0                               |\n#                                                                      |\n#    nCr(n, r)     =&gt; Compute the combination of `n C r`: n!/k!(n-k)!  |\n#                     returns =&gt; int &gt; 0                               |\n#                                                                      |\n#    nPr(n, r)     =&gt; Compute the permutation of `n P r`: n!/(n-k)!    |\n#                     returns =&gt; int &gt; 0                               |\n#                                                                      |\n# =====================================================================&gt;\n\n\n#' Recursive Implementation of the Factorial Function\n#'\n#' Determine the product of all positive integers less than or equal to n.\n#' @param n An integer to pass to the factorial function. Must be &gt;= 0\n#' @export\n#' @examples \n#' rfactorial(5)\n\nrfactorial &lt;- function(n) {\n    if (n&lt;=1) {\n        return(1)\n    } else {\n        return(n*rfactorial(n-1))\n    }\n}\n\n\n\n#' Combination Function - nCr = n!/k!(n-k)! \n#' \n#' Returns the combination of n things taken r at a time without repetition.\n#' @param n The n things\n#' @param r The subgroup of r items at a time\n#' @export\n#' @examples \n#' nCr(n=7, r=4)\n\nnCr &lt;- function(n, r) {\n\n    return(rfactorial(n)/(rfactorial(r)*(rfactorial(n-r))))\n\n}\n\n\n\n#' Permutation Function - nPr = n!/(n-k)! \n#' \n#' Permutation relates to the act of arranging r members of a set n into some order.\n#' @param n The superset\n#' @param r The r members of the set n to arrange in order\n#' @export\n#' @examples \n#' nPr(n=7, r=4)\n\nnPr &lt;- function(n, r) {\n\n    return(rfactorial(n)/(rfactorial(n-r)))\n\n}\nAfter saving the annotations, call the document function from roxygen2 to generate the documentation. We’ll need to provide the absolute path to our development directory, U:/Dev/combinatronics to document (typical output is listed below):\n&gt; document(\"U:/Dev/combinatronics\")\n\n# output #\n# Updating combinatronics documentation\n# Loading combinatronics\n# Updating roxygen version in U:\\Dev\\combinatronics/DESCRIPTION\n# Writing NAMESPACE\n# Writing rfactorial.Rd\n# Writing nCr.Rd\n# Writing nPr.Rd\ndocument creates an additional directory identified as man in the combinatronics folder, which contains the compiled annotations for each of the functions in combinatronics.R. For our package, man contains rfactorial.Rd, nPr.Rd and nCr.Rd.\nFinally, install the package. We need to set our working directory to the parent of our combinatronics folder. Once the working directory is set, simply call install along with the name of the package:\nsetwd(\"U:/Dev\")\ninstall(\"combinatronics\")\n\n# output =&gt;\n# Installing combinatronics\n# \"C:/PROGRA~1/R/R-33~1.2/bin/x64/R\" --no-site-file --no-environ --no-save --no-# restore  \\\n#  --quiet CMD INSTALL \"U:/Dev/combinatronics\" --library=\"U:/R/win-library/3.3\"  \\\n#  --install-tests \n\n# * installing *source* package 'combinatronics' ...\n# ** R\n# ** preparing package for lazy loading\n# ** help\n# *** installing help indices\n# ** building package indices\n# ** testing if installed package can be loaded\n# * DONE (combinatronics)\n# Reloading installed combinatronics\n&gt;\nUpon completion, the interactive prompt will be returned. After importing the library, we can take a look at our package documentation in the RStudio Help viewer. Running:\n&gt; library(combinatronics)\n&gt; ?rfactorial\nwill render:\n\nSimilarly for nCr:"
  },
  {
    "objectID": "posts/poisson-binomial/poisson-binomial.html",
    "href": "posts/poisson-binomial/poisson-binomial.html",
    "title": "Derivation of the Poisson Distribution as a Limiting Case of the Binomial PDF",
    "section": "",
    "text": "The Poisson distribution is a discrete probability distribution with identical mean and variance that expresses the probability of a given number of events occurring in a fixed interval of time. In this post, we derive the Poisson probability mass function beginning with the Binomial PDF.\nThe binomial distribution represents the number of successes, \\(k\\) in \\(n\\) independent Bernoulli trials, where the probability of success \\(p\\) for any given trial is \\(0 \\leq p \\leq 1\\). Formally, the binomial PDF is given by\n\\[\nf(k; p, n) = \\binom{n}{k}p^{k}(1-p)^{n-k},\n\\]\nwhere \\(\\binom{n}{k}\\) represents the binomial coefficient, which equates to\n\\[\n\\binom{n}{k} = \\frac{n!}{n!(n-k)!}.\n\\]\nThe expanded binomial PDF then appears as\n\\[\nf(k; p, n) = \\frac{n!}{n!(n-k)!}p^{k}(1-p)^{n-k}.\n\\]\nThe mean of the binomial distribution is\n\\[\n\\mu = \\lambda = np,\n\\]\nwhich can be rearranged in terms of \\(p\\) yielding\n\\[\np = \\frac{\\lambda}{n}.\n\\]\nSubstituting this expression for \\(p\\) into the binomial PDF results in\n\\[\nf(k; p, n) = \\frac{n!}{n!(n-k)!}\\Big(\\frac{\\lambda}{n}\\Big)^{k}\\Big(1-\\frac{\\lambda}{n}\\Big)^{n-k}\n\\]\nIf we let \\(n\\) grow very large and \\(p\\) grow very small, after a bit of re-expression we can formulate the binomial distribution as\n\\[\n\\lim_{n\\to\\infty , p \\to 0} f(k; p, n) = \\frac{n(n-1) \\cdots (n-k+1)}{k!} \\frac{\\lambda^{k}}{n^{k}} \\Big(1-\\frac{\\lambda}{n}\\Big)^{n} \\Big(1-\\frac{\\lambda}{n}\\Big)^{-k}.\n\\]\nSwapping denominator terms yields\n\\[\n= \\frac{n(n-1) \\cdots (n-k+1)}{n^{k}} \\frac{\\lambda^{k}}{k!} \\Big(1-\\frac{\\lambda}{n}\\Big)^{n} \\Big(1-\\frac{\\lambda}{n}\\Big)^{-k}\n\\]\nRecall from calculus that \\(e = \\lim_{k\\to\\infty} (1 + 1/k)^{k}\\) and similarly \\(e^{x} = \\lim_{k\\to\\infty} (1 + x/k)^{k}\\). Leveraging these identities, \\((1 - \\frac{\\lambda}{n})^{n}\\) becomes \\(e^{-\\lambda}\\) in the limit. With very large \\(n\\), the expression for the PDF reduces to\n\\[\n\\lim_{n\\to\\infty} f(k; p, n) = (1) \\Big( \\frac{\\lambda^{k}}{k!}\\Big)(e^{-\\lambda})(1).\n\\]\nAfter a little cleanup, we obtain the familiar expression for the Poisson density with mean \\(\\lambda\\):\n\\[\nf(k; \\lambda)  = \\frac{\\lambda^{k}}{k!}e^{-\\lambda} \\hspace{1cm} k \\in 0, 1, 2, \\cdots.\n\\]\nAn interesting property of the Poisson distribution (as well as the binomial and negative binomial distributions) is that successive probabilities can be found recursively. The recurrence relation is given by\n\\[\n\\frac{p_{k}}{p_{k-1}} = a +\\frac{b}{k}, \\hspace{2mm} k = 2,3,4, \\cdots,\n\\]\nwhere in the case of the Poisson distribution, \\(a=0\\) and \\(b=\\lambda\\), resulting in\n\\[\np_{k} = \\frac{\\lambda}{k}p_{k-1}, \\hspace{2mm} k = 1,2,3, \\cdots.\n\\]\nTo demonstrate, for a system approximated by a Poisson distribution with \\(\\lambda = 3\\), the probabilities for \\(0 \\leq k \\leq 6\\) are:\n\\[\n\\begin{align*}\np_{0} &= .049787068 \\\\\np_{1} &= .149361205 \\\\\np_{2} &= .224041808 \\\\\np_{3} &= .224041808 \\\\\np_{4} &= .168031356 \\\\\np_{5} &= .100818813 \\\\\np_{6} &= .050409407.\n\\end{align*}\n\\]\nTo use the recurrence relation, we first calculate \\(p_0\\) as above. Then:\n\\[\n\\begin{align*}\np_{0} &=.049787068 \\\\\np_{1} &=p_{0}\\frac{\\lambda}{k} =.049787068*\\frac{3}{1}=.149361205  \\\\\np_{2} &=p_{1}\\frac{\\lambda}{k} =.149361205*\\frac{3}{2}=.224041808  \\\\\np_{3} &=p_{2}\\frac{\\lambda}{k} =.224041808*\\frac{3}{3}=.224041808  \\\\\np_{4} &=p_{3}\\frac{\\lambda}{k} =.224041808*\\frac{3}{4}=.168031356  \\\\\np_{5} &=p_{4}\\frac{\\lambda}{k} =.168031356*\\frac{3}{5}=.100818813  \\\\\np_{6} &=p_{5}\\frac{\\lambda}{k} =.100818813*\\frac{3}{6}=.050409407,\n\\end{align*}\n\\]\nwhich are identical to the probabilities obtained from evaluating the Poisson PDF directly."
  },
  {
    "objectID": "posts/pca-from-scratch/pca-from-scratch.html",
    "href": "posts/pca-from-scratch/pca-from-scratch.html",
    "title": "Demonstration of Principal Component Analysis",
    "section": "",
    "text": "Principal Component Analysis (PCA) is a statistical technique used in the field of data analysis and machine learning for dimensionality reduction while preserving as much of the data’s variation as possible. It’s particularly useful when dealing with high-dimensional data, helping to simplify the data without losing the underlying structure. These components are orthogonal (meaning they are statistically independent of one another), and are ordered so that the first few retain most of the variation present in all of the original variables. There are a number of uses for PCA, including:\n\nVisualization: By reducing data to two or three principal components, PCA allows for the visualization of complex data in a 2D or 3D space.\nNoise Reduction: By eliminating components with low variance and retaining those with high variance, PCA can help in reducing noise in the dataset.\nFeature Extraction and Data Compression: PCA can help in extracting the most important features from the data, which can then be used for further analysis or machine learning modeling. It also helps in compressing the data by reducing the number of dimensions without losing significant information.\nImproving Model Performance: By reducing the dimensionality, PCA can lead to simpler models that are less prone to overfitting when training on datasets with a high number of features.\n\n\nThere are some limitations to PCA, specifically:\n\nLinear Assumptions: PCA assumes that the principal components are a linear combination of the original features, which may not always capture the structure of the data well, especially if the underlying relationships are non-linear.\nSensitivity to Scaling: Since PCA is affected by the scale of the features, different results can be obtained if the scaling of the data changes.\nData Interpretation: Interpretability of the principal components can be difficult since they are combinations of all original features.\n\n\nThe spread of a dataset can be expressed in orthonormal vectors – the principal directions of the dataset. Orthonormal means that the vectors are orthogonal to each other (i.e. they have an angle of 90 degrees) and are of size 1. By sorting these vectors in order of importance (by looking at their relative contribution to the spread of the data as a whole), we can find the dimensions of the data which explain the most variance. We can then reduce the number of dimensions to the most important ones only. Finally, we can project our dataset onto these new dimensions, called the principal components, performing dimensionality reduction without losing much of the information present in the dataset.\nIn what follows, PCA is demonstrated from scratch using Numpy and the results compared with scikit-learn to show they are identical up to a sign.\n\n\nExtracting the first k Principal Components\nWe start by 0-centering the data, then compute the Singular Value Decomposition (SVD) of the data matrix \\(A\\), where rows of \\(A\\) are assumed to be samples and columns the features. For any \\(m \\times n\\) data matrix \\(A\\), the SVD factors \\(A = U \\Sigma V^{T}\\) where:\n\n\\(U\\) = an \\(m \\times m\\) orthogonal matrix whose columns are the left singular vectors of \\(A\\).\n\\(V\\) = an \\(n \\times n\\) orthogonal matrix whose columns are the right singular vectors of \\(A\\).\n\\(\\Sigma\\) = is an \\(m \\times n\\) diagonal matrix containing the singular values of \\(A\\) in descending order along the diagonal. These values are non-negative and are the square roots of the eigenvalues of both \\(A^{T}A\\) and \\(AA^{T}\\).\n\nThe remaining steps are shown in code below:\n\n\nimport numpy as np\n\nnp.set_printoptions(suppress=True, precision=8, linewidth=1000)\n\nrng = np.random.default_rng(516)\n\n\n# Specify the number of principal components to retain.\nk = 5\n\n# Create random matrix with 100 samples and 50 features.\nXinit = rng.normal(loc=100, scale=12.5, size=(100, 50))\n\n# 0-center columns in Xinit. Each column will now have mean 0. \nX = Xinit - Xinit.mean(axis=0)\n\n# Compute SVD of 0-centered data matrix X.\n# U : (100, 100) Columns represent left singular vectors.\n# VT: (50, 50) Rows represent right singular vectors.\n# S : (50,) Represents singular values of X. \nU, S, VT = np.linalg.svd(X)\n\n# Apply dimensionality reduction (retain first k principal components).\n# Xpca1 will have shape 100 x k. \nXpca1 = X @ VT[:k].T\n\nprint(f\"Xpca1.shape: {Xpca1.shape}\")\n\nXpca1.shape: (100, 5)\n\n\nThis returns the top-5 principal components from the 0-centered data matrix \\(X\\). This transformation can also be carried out in scikit-learn as follows:\n\n\nfrom sklearn.decomposition import PCA\n\n# Call pca.fit on 0-centered data matrix.\npca = PCA(n_components=k)\npca.fit(X)\n\nXpca2 = pca.transform(X)\n\nprint(f\"Xpca2.shape: {Xpca2.shape}\")\n\n\nXpca2.shape: (100, 5)\n\n\nThe first few rows of Xpca1 and Xpca2 can be compared. Notice that they are identical up to a sign:\n\nXpca1[:10, :]\n\narray([[ -1.0024489 ,  -8.45086232,  13.05495528, -32.18002702,   6.1897014 ],\n       [ 38.32532794,   3.10016467, -34.9065906 ,  -4.45684708, -18.03986151],\n       [  2.50382601,  29.88906322, -18.83018377,  -9.64833693, -18.46252148],\n       [ -4.57804661,  -1.16308008,  22.67820152, -15.25819358, -12.15373192],\n       [ -4.01382494,  -8.4219436 ,  18.33661249,   7.327393  ,  20.1264191 ],\n       [ 15.97152692,   0.03394136, -17.74609475,  21.77608653, -23.06117564],\n       [-19.18606004, -44.7392649 , -47.83180773,  -1.30574016, -33.77155819],\n       [ 35.1510225 ,  -8.16265381,  21.78210602,  23.30058147,  26.5255693 ],\n       [ 47.57005479, -39.33886869,   0.91133253,  10.09394219,   9.24710166],\n       [-30.05011125,  13.09734398,  17.8311663 ,  13.25098519, -14.39827404]])\n\n\n\nXpca2[:10, :]\n\narray([[  1.0024489 ,  -8.45086232, -13.05495528,  32.18002702,  -6.1897014 ],\n       [-38.32532794,   3.10016467,  34.9065906 ,   4.45684708,  18.03986151],\n       [ -2.50382601,  29.88906322,  18.83018377,   9.64833693,  18.46252148],\n       [  4.57804661,  -1.16308008, -22.67820152,  15.25819358,  12.15373192],\n       [  4.01382494,  -8.4219436 , -18.33661249,  -7.327393  , -20.1264191 ],\n       [-15.97152692,   0.03394136,  17.74609475, -21.77608653,  23.06117564],\n       [ 19.18606004, -44.7392649 ,  47.83180773,   1.30574016,  33.77155819],\n       [-35.1510225 ,  -8.16265381, -21.78210602, -23.30058147, -26.5255693 ],\n       [-47.57005479, -39.33886869,  -0.91133253, -10.09394219,  -9.24710166],\n       [ 30.05011125,  13.09734398, -17.8311663 , -13.25098519,  14.39827404]])\n\n\nThe reason for the discrepancy is due to the fact that each singular vector is only uniquely determined up to sign, indeed in more generality it is only defined up to complex sign (i.e. up to multiplication by a complex number of modulus 1). For a deeper mathematical explanation of this, check out this link.\n\n\n\nAssessing Reconstruction Error\nOnce the principal components of a data matrix have been identified, we can get an idea of how well using k components approximates the original matrix. If we use all principal components, we should be able to recreate the data exactly. The objective is to identify some reduced number of components that captures enough variance in the original data while also eliminating redundant components. Thus, minimizing the reconstruction error is equivalent to maximizing the variance of the projected data. Using the pca.inverse_transform method, we can project our k-component matrix back into signal space (100 x 50 for our example), and compute the reconstruction error (the average difference between the original data matrix and our projected matrix):\n\n\nreconstruction_error = []\n\nfor jj in range(1, X.shape[1] + 1):\n    pca = PCA(n_components=jj).fit(X)\n    Xpca = pca.transform(X) \n    Xhat = pca.inverse_transform(Xpca)\n    err = np.mean(np.sum((X - Xhat)**2, axis=1))\n    reconstruction_error.append((jj, err))\n\n\nWe can then plot the recosntruction error as a function of the number of retained components:\n\n\n# Plot reconstruction error as a function of k-components.\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nxx, yy = zip(*reconstruction_error)\n\nfig, ax = plt.subplots(figsize=(8, 5), tight_layout=True)\nax.set_title(\"PCA reconstruction error vs. nbr. components\", fontsize=10)\nax.plot(xx, yy, color=\"#E02C70\", linestyle=\"--\", linewidth=1.25, markersize=4, marker=\"o\")\nax.set_xlabel(\"k\", fontsize=10)\nax.set_ylabel(\"reconstruction error\", fontsize=10)\nax.set_ylim(bottom=0)\nax.set_xlim(left=0)\nax.tick_params(axis=\"x\", which=\"major\", direction=\"in\", labelsize=8)\nax.tick_params(axis=\"y\", which=\"major\", direction=\"in\", labelsize=8)\nax.get_yaxis().set_major_formatter(mpl.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\nax.xaxis.set_ticks_position(\"none\")\nax.yaxis.set_ticks_position(\"none\")\nax.grid(True)\n\nplt.show()\n\n\n\n\n\n\n\n\nA few things to note about the reconstruction error curve:\n\nWe started with random normal data, so there was no information in the original features to begin with. When applied to real-world data, you will typically see a sharp decrease in reconstruction error after some small set of k-components.\nNotice when k = 50, the reconstruction error drops to 0. With 50 components, we are able to reproduce the original data exactly.\n\n\n\nPCA Loadings\nPCA loadings are the coefficients of the linear combination of the original variables from which the principal components are constructed. From the scikit-learn’s pca output, we simply need to access the pca.components_ attribute, the rows of which contain the eigenvectors associated with the first k principal components:\n\nimport pandas as pd\n\npca = PCA(n_components=2)\npca.fit(X)\n\ndfloadings = pd.DataFrame(pca.components_.T, columns=[\"pc1\", \"pc2\"]).sort_values(\"pc1\", ascending=False, key=abs)\ndfloadings.head(10)\n\n\n\n\n\n\n\n\npc1\npc2\n\n\n\n\n27\n0.367146\n-0.153378\n\n\n41\n0.342124\n0.050818\n\n\n39\n-0.302304\n0.010788\n\n\n34\n-0.296221\n-0.198985\n\n\n45\n0.260603\n-0.155375\n\n\n31\n0.238631\n-0.023786\n\n\n21\n0.198101\n-0.186968\n\n\n29\n-0.191818\n0.039887\n\n\n20\n0.191115\n0.011552\n\n\n37\n0.173049\n-0.032115\n\n\n\n\n\n\n\nThe interpretation of this output is that the coefficient for column index 27 is .367146, and is the largest contributor to the score for each observation when applied to the original data.\nWe can show how the loadings are used to compute the PCA scores for the first sample in the dataset. Let’s find the first principal component of our original 0-centered data matrix \\(X\\):\n\n\npca = PCA(n_components=1)\nXpca = pca.fit_transform(X)\n\nprint(Xpca[0])\n\n[1.0024489]\n\n\nThe value 1.0024489 is obtained by computing the dot product of the first principal component loading with the first row in \\(X\\):\n\n\nnp.dot(pca.components_.T.ravel(), X[0, :])\n\n1.0024488966036473\n\n\n\n\nLoading Plot\nPCA loading plots are visual tools used to interpret the results of PCA by showing how much each variable contributes to the principal components. These plots help in understanding the underlying structure of the data and in identifying which variables are most important in driving the differences between observations. They provide insight into the relationship between the variables and the principal components. In the next cell, we show how to create a loading plot for the iris dataset in scikit-learn.\n\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.decomposition import PCA\n\n# Load iris data.\ndata = load_iris()\nX = data[\"data\"]\nfeature_names = data[\"feature_names\"]\n\n\npca = PCA(n_components=2)\npca.fit(X)\nloadings = pca.components_.T\n\n\n# Create loading plot.\nfig, ax = plt.subplots(figsize=(8, 5), tight_layout=True)\nax.set_title(\"Iris dataset loading plot\", fontsize=10)\nfor ii, feature in enumerate(feature_names):\n    ax.arrow(0, 0, loadings[ii, 0], loadings[ii, 1], head_width=0.025, head_length=0.025, color=\"#E02C70\")\n    ax.text(loadings[ii, 0] * 1.05, loadings[ii, 1] * 1.05, feature, color=\"#000000\")\n\nax.set_xlim(-.5, 1.25) \nax.set_xlabel(\"pc1\", fontsize=10)\nax.set_ylabel(\"pc2\", fontsize=10)\nax.tick_params(axis=\"x\", which=\"major\", direction=\"in\", labelsize=8)\nax.tick_params(axis=\"y\", which=\"major\", direction=\"in\", labelsize=8)\nax.xaxis.set_ticks_position(\"none\")\nax.yaxis.set_ticks_position(\"none\")\nax.grid(True)\n\nplt.show()\n\n\n\n\n\n\n\n\nIn the direction of the first principal component, petal length has the largest effect, whereas sepal length is the dominant feature in the direction of the second principal component.\n\n\nUsing PCA in ML Pipeline\nHow might we use PCA in a typical machine learning workflow?\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=5).fit(Xtrain)\nXtrain_pca = pca.transform(Xtrain)\nXtest_pca = pca.transform(Xtest)\nXtrain and Xtest would then be used as the train and test sets as in any other machine learning setup. Note that we call fit on the training set, then only transform on the test set. This prevents information from the test set leaking into the training data. Xtrain_pca and Xtest_pca will have the same number of rows as Xtrain and Xtest, but will only have 5 features, which will be less than or equal to the number of features in Xtrain and Xtest."
  },
  {
    "objectID": "posts/online-updates/online-updates.html",
    "href": "posts/online-updates/online-updates.html",
    "title": "Online Mean and Variance Update without Full Recalculation",
    "section": "",
    "text": "Imagine you are responsible for maintaining the mean and variance of a dataset that is frequently updated. For small-to-moderately sized datasets, much thought might not be given to the method used for recalculation. However, with datasets consisting of hundreds of billions or trillions of observations, full recomputation of the mean and variance at each refresh may require significant computational resources that may not be available.\nFortunately it isn’t necessary to perform a full recalculation of mean and variance when accounting for new observations. Recall that for a sequence of \\(n\\) observations \\(x_{1}, \\dots x_{n}\\) the sample mean \\(\\mu_{n}\\) and variance \\(\\sigma_{n}^{2}\\) are given by:\n\\[\n\\begin{align*}\n       \\mu_{n} &= \\sum_{i=1}^{n} x_{i} \\\\\n\\sigma_{n}^{2} &= \\frac{1}{n-1}\\sum_{i=1}^{n} (x_{i} - \\mu_{n})^{2}\n\\end{align*}\n\\]\nA new observation \\(x_{n+1}\\) becomes available. To calculate the updated mean \\(\\mu_{n+1}\\) and variance \\(\\sigma_{n+1}^{2}\\) in light of this new observation without requiring full recalculation, we can use the following:\n\\[\n\\begin{align*}\n\\mu_{n+1} &= \\frac{1}{n+1}(n\\mu_{n} + x_{n+1}) \\\\\n\\sigma_{n+1}^{2} &= \\frac{n}{n+1}\\sigma_{n}^{2} + \\frac{1}{n}(x_{n+1} - \\mu_{n+1})^{2}\n\\end{align*}\n\\]\n\nDemonstration\nConsider the following values:\n\\[\n1154\\hspace{2mm}717\\hspace{2mm}958\\hspace{2mm}1476\\hspace{2mm}889\\hspace{2mm}1414\\hspace{2mm}1364\\hspace{2mm}1047\n\\]\nThe mean and variance for the observations:\n\\[\n\\begin{align*}\n       \\mu_{8} &\\approx 1127.38 \\\\\n\\sigma_{8}^{2} &\\approx 65096.48\n\\end{align*}\n\\]\nA new value, \\(1251\\) becomes available. Full recalculation of mean and variance yields:\n\\[\n\\begin{align*}\n       \\mu_{8} &\\approx \\mathbf{1141.11} \\\\\n\\sigma_{8}^{2} &\\approx \\mathbf{59372.99}\n\\end{align*}\n\\]\nThe mean and variance calculated using online update results in:\n\\[\n\\begin{align*}\n\\mu_{9} &= \\frac{1}{9}(8(1127.38) + 1251) \\approx \\mathbf{1141.11} \\\\\n\\sigma_{9}^{2} &= \\frac{8}{9} (65096.48) + \\frac{1}{8}(1251 - 1141.11)^{2} \\approx \\mathbf{59372.99},\n\\end{align*}\n\\]\nconfirming agreement between the two approaches.\nNote that the variance returned using the online update formula is the population variance. In order to return the updated unbiased sample variance, we need to multiply the variance returned by the online update formula by \\((n+1)/n\\), where \\(n\\) represents the length of the original array excluding the new observation. Thus, the updated sample variance after accounting for the new value is:\n\\[\n\\begin{align*}\ns_{n+1}^{2} &= \\frac{n+1}{n}\\big(\\frac{n}{n+1}\\sigma_{n}^{2} + \\frac{1}{n}(x_{n+1} - \\mu_{n+1})^{2}\\big) \\\\\ns_{9}^{2}   &= \\frac{9}{8}(59372.99) \\approx 66794.61\n\\end{align*}\n\\]\n\n\nImplementation\nA straightforward implementation in Python to handle online mean and variance updates, incorporating Bessel’s correction to return the unbiased sample variance is provided below:\n\nimport numpy as np\n\n\n\ndef online_mean(mean_init, n, new_obs):\n    \"\"\"\n    Return updated mean in light of new observation without\n    full recalculation.\n    \"\"\"\n    return((n * mean_init + new_obs) / (n + 1))\n\n\n\ndef online_variance(var_init, mean_new, n, new_obs):\n    \"\"\"\n    Return updated variance in light of new observation without\n    full recalculation. Includes Bessel's correction to return\n    unbiased sample variance. \n    \"\"\"\n    return ((n + 1) / n) * (((n * var_init) / (n + 1)) + (((new_obs - mean_new)**2) / n))\n\n\n\n\na0 = np.array([1154,  717,  958, 1476,  889, 1414, 1364, 1047])\n\na1 = np.array([1154,  717,  958, 1476,  889, 1414, 1364, 1047, 1251])\n\n\n# Original mean and variance.\nmean0 = a0.mean()    # 1127.38\nvariance0 = a0.var() # 65096.48\n\n# Full recalculation mean and variance with new observation.\nmean1 = a1.mean()    # 1141.11\nvariance1 = a1.var(ddof=1) # 59372.99\n\n# Online update of mean and variance with bias correction.\nmean2 = online_mean(mean0, a0.size, 1251)                     # 1141.11\nvariance2 = online_variance(variance0, mean2, a0.size, 1251)  # 66794.61\n\nprint(f\"Full recalculation mean    : {mean1:,.5f}\")\nprint(f\"Full recalculation variance: {variance1:,.5f}\")\nprint(f\"Online calculation mean    : {mean2:,.5f}\")\nprint(f\"Online calculation variance: {variance2:,.5f}\")\n\nFull recalculation mean    : 1,141.11111\nFull recalculation variance: 66,794.61111\nOnline calculation mean    : 1,141.11111\nOnline calculation variance: 66,794.61111"
  },
  {
    "objectID": "posts/logistic-regression-python/logistic-regression-python.html",
    "href": "posts/logistic-regression-python/logistic-regression-python.html",
    "title": "Estimating Logistic Regression coefficients From Scratch in Python",
    "section": "",
    "text": "In this post, I’ll demonstrate how to estimate the coefficients of a logistic regression model using the Fisher Scoring algorithm in Python. These estimates will be compared with statsmodels coefficients to ensure consistency.\nIn a generalized linear model (GLM), the response may have any distribution from the exponential family. Rather than assuming the mean is a linear function of the explanatory variables, we assume that a function of the mean, or the link function, is a linear function of the explanatory variables.\nLogistic regression is used for modeling data with a categorical response. Although it’s possible to model multinomial data using logistic regression, in this post our analysis will be limited to models targeting a dichotomous response, where the outcome can be classified as ‘Yes/No’ or ‘1/0’.\nThe logistic regression model is a GLM whose canonical link is the logit, or log-odds:\n\\[\n\\mathrm{Ln} \\big(\\frac{\\pi_{i}}{1 - \\pi_{i}} \\big) = \\beta_{0} + \\beta_{1}x_{i1} + \\ldots + \\beta_{p}x_{ip}\n\\]\nfor \\(i = (1, \\ldots , n)\\).\nSolving the logit for \\(\\pi_{i}\\), which is a stand-in for the predicted probability associated with observation \\(x_{i}\\), yields\n\\[\n\\pi_{i} = \\frac {e^{\\beta_{0} + \\beta_{1}x_{i1} + \\ldots + \\beta_{p}x_{ip}}}{1 + e^{\\beta_{0} + \\beta_{1}x_{i1} + \\ldots + \\beta_{p}x_{ip}}} = \\frac {1}{1 + e^{-(\\beta_{0} + \\beta_{1}x_{i1} + \\ldots + \\beta_{p}x_{ip})}},\n\\]\nwhere \\(-\\infty &lt; x_{i} &lt; \\infty\\) and \\(0 &lt; \\pi_{i }&lt; 1\\).\n\nParameter Estimation\nMaximum Likelihood Estimation can be used to determine the parameters of a Logistic Regression model, which entails finding the set of parameters for which the probability of the observed data is greatest. The objective is to estimate the \\(p + 1\\) unknown \\(\\beta_{0}, \\ldots ,\\beta_{p}\\).\nLet \\(Y_{i}\\) represent independent, dichotomous response values for each of \\(n\\) observations, where \\(Y_i=1\\) denotes a success and \\(Y_i=0\\) denotes a failure. The density function of a single observation \\(Y_i\\) is given by\n\\[\np(y_{i}) = \\pi_{i}^{y_{i}}(1-\\pi_{i})^{1-y_{i}},\n\\]\nand the corresponding likelihood function is\n\\[\nL(\\beta) = \\prod_{i=1}^{n} \\pi_{i}^{y_{i}}(1-\\pi_{i})^{1-y_{i}}.\n\\]\nTaking the natural log of the maximum likelihood estimate results in the log-likelihood function:\n\\[\n\\begin{align*}\nl(\\beta) &= \\mathrm{Ln}(L(\\beta)) = \\mathrm{Ln} \\Big(\\prod_{i=1}^{n} \\pi_{i}^{y_{i}}(1-\\pi_{i})^{1-y_{i}} \\Big)\n= \\sum_{i=1}^{n} y_{i} \\cdot \\mathrm{Ln}(\\pi_{i}) + (1-y_{i}) \\cdot \\mathrm{Ln}(1-\\pi_{i})\\\\\n&= \\sum_{i=1}^{n} y_{i} \\cdot \\mathrm{Ln} \\Big(\\frac {e^{\\beta_{0} + \\beta_{1}x_{i1} + \\ldots + \\beta_{p}x_{ip}}}{1 + e^{\\beta_{0} + \\beta_{1}x_{i1} + \\ldots + \\beta_{p}x_{ip}}} \\Big) + (1 - y_{i}) \\cdot \\mathrm{Ln} \\Big(\\frac {1}{1 + e^{\\beta_{0} + \\beta_{1}x_{i1} + \\ldots + \\beta_{p}x_{ip}}} \\Big)\\\\\n&= \\sum_{i=1}^{n} y_{i}(\\beta_{0} + \\beta_{1}x_{i1} + \\ldots + \\beta_{p}x_{ip}) - \\mathrm{Ln}(1 + e^{\\beta_{0} + \\beta_{1}x_{i1} + \\ldots + \\beta_{p}x_{ip}})\\\\\n\\end{align*}\n\\]\nThe first-order partial derivatives of the log-likelihood are calculated and set to zero for each \\(k = 0, 1, \\ldots, p\\)\n\\[\n\\frac {\\partial l(\\beta)}{\\partial \\beta_{k}} = \\sum_{i=1}^{n} y_{i}x_{ik} - \\pi_{i}x_{ik} = \\sum_{i=1}^{n} x_{ik}(y_{i} - \\pi_{i}) = 0,\n\\]\nwhich can be represented in matrix notation as\n\\[\n\\frac {\\partial l(\\beta)}{\\partial \\beta} = X^{T}(y - \\pi),\n\\]\nwhere \\(X^{T}\\) is a (p + 1)-by-n matrix and \\((y - \\pi)\\) an n-by-1 vector.\nThe vector of first-order partial derivatives of the log-likelihood function is referred to as the score function and is typically represented as \\(U\\).\nThese (p+1) equations are solved simultaneously to obtain the parameter estimates \\(\\hat\\beta_{0}, \\ldots ,\\hat\\beta_{p}\\).\nEach solution specifies a critical-point which will be either a maximum or a minimum. The critical point will be a maximum if the matrix of second partial derivatives is negative definite (which means every element on the diagonal of the matrix is less than zero).\nThe matrix of second partial derivatives is given by\n\\[\n\\frac{\\partial^{2} l(\\beta)}{{\\partial \\beta_{k}}{\\partial \\beta_{k}}^{T}} = - \\sum_{i=1}^{n} x_{ik}\\pi_{i}(1-\\pi_{i}){x_{ik}}^{T},\n\\]\nrepresented in matrix form as\n\\[\n\\frac{\\partial^{2} l(\\beta)}{{\\partial \\beta}{\\partial \\beta}^{T}} = -X^{T}WX,\n\\]\nwhere \\(W\\) is an n-by-n diagonal matrix of weights with each element equal to \\(\\pi_{i}(1 - \\pi_{i})\\) for logistic regression models (in general, the weights matrix \\(W\\) will have entries inversely proportional to the variance of the response).\nSince no closed-form solution exists for determining logistic regression model coefficients, iterative techniques must be employed.\n\n\nFitting the Model\nTwo distinct but related iterative methods can be utilized in determining model coefficients: the Newton-Raphson method and Fisher Scoring. The Newton-Raphson method relies on the matrix of second partial derivatives, also known as the Hessian. The Newton-Raphson update expression is:\n\\[\n\\beta^{(t+1)} = \\beta^{(t)} - (H^{(t)})^{-1}U^{(t)},\n\\]\nwhere:\n\n\\(\\beta^{(t+1)}\\) = the vector of updated coefficient estimates.\n\n\\(\\beta^{(t)}\\) = the vector of coefficient estimates from the previous iteration.\n\n\\((H^{(t)})^{-1}\\) = the inverse of the Hessian, \\(\\Big(\\frac{\\partial^{2} l(\\beta)}{{\\partial \\beta}{\\partial \\beta}^{T}}\\Big)^{-1}\\).\n\n\\(U^{(t)}\\) = the vector of first-order partial derivatives of the log-likelihood function, \\(X^{T}(y - \\pi)\\).\n\nThe Newton-Raphson method starts with an initial guess for the solution, and obtains a second guess by approximating the function to be maximized in a neighborhood of the initial guess by a second-degree polynomial, and then finding the location of that polynomial’s maximum value. This process continues until it converges to the actual solution. The convergence of \\(\\beta^{t}\\) to \\(\\hat{\\beta}\\) is usually fast, with adequate convergence frequently realized after fewer than 50 iterations.\nAn alternative method, Fisher Scoring, utilizes the expected information \\(-E\\Big(\\frac{\\partial^{2} l(\\beta)}{{\\partial \\beta}{\\partial \\beta}^{T}}\\Big)\\). Let \\(\\mathcal{I}\\) serve as a stand-in for the expected value of the information:\n\\[\n\\mathcal{I} = -E\\Big(\\frac{\\partial^{2} l(\\beta)}{{\\partial \\beta}{\\partial \\beta}^{T}}\\Big).\n\\]\nThe Fisher scoring update step replaces \\(-H^{(t)}\\) from Newton-Raphson with \\(\\mathcal{I}^{(t)}\\):\n\\[\n\\begin{align*}\n\\beta^{(t+1)} &= \\beta^{(t)} + (\\mathcal{I}^{(t)})^{-1}U^{(t)}\\\\\n&= \\beta^{(t)} + (X^{T}WX)^{-1}X^{T}(y - \\pi)\\\\\n\\end{align*}\n\\]\nwhere:\n\n\\(\\beta^{(t+1)}\\) = the vector of updated coefficient estimates.\n\n\\(\\beta^{(t)}\\) = the vector of coefficient estimates from the previous iteration.\n\n\\((\\mathcal{I}^{(t)})^{-1}\\) = the inverse of the expected information matrix, \\(-E \\Big(\\frac{\\partial^{2} l(\\beta)}{{\\partial \\beta}{\\partial \\beta}^{T}}\\Big)^{-1}\\).\n\n\\(U^{(t)}\\) = the vector of first-order partial derivatives of the log-likelihood function, \\(X^{T}(y - \\pi)\\).\n\nFor GLMs with a canonical link (of which employing the logit for logistic regression is an example), the observed and expected information are the same. When the response follows an exponential family distribution, and the canonical link function is employed, observed and expected information coincide so that Fisher scoring and Newton-Raphson are identical.\nWhen the canonical link is used, the second partial derivatives of the log-likelihood do not depend on the observation \\(y_i\\), and therefore\n\\[\n\\frac{\\partial^{2} l(\\beta)}{{\\partial \\beta}{\\partial \\beta}^{T}} = E \\Big(\\frac{\\partial^{2} l(\\beta)}{{\\partial \\beta}{\\partial \\beta}^{T}} \\Big).\n\\]\nFisher scoring has the advantage that it produces the asymptotic covariance matrix as a by-product.\nTo summarize:\n\nThe Hessian is the matrix of second partial derivatives of the log-likelihood with respect to the parameters, \\(H = \\frac{\\partial^{2} l(\\beta)}{{\\partial \\beta}{\\partial \\beta}^{T}}\\).\nThe observed information is \\(-\\frac{\\partial^{2} l(\\beta)}{{\\partial \\beta}{\\partial \\beta}^{T}}\\).\n\nThe expected information is \\(\\mathcal{I} = E\\Big(-\\frac{\\partial^{2} l(\\beta)}{{\\partial \\beta}{\\partial \\beta}^{T}}\\Big)\\).\nThe asymptotic covariance matrix is \\(\\mathrm{Var}(\\hat{\\beta}) = E\\Big(-\\frac{\\partial^{2} l(\\beta)}{{\\partial \\beta}{\\partial \\beta}^{T}}\\Big)^{-1} = (X^{T}WX)^{-1}\\).\n\nFor models employing a canonical link function:\n\nThe observed and expected information are the same, \\(\\frac{\\partial^{2} l(\\beta)}{{\\partial \\beta}{\\partial \\beta}^{T}} = E\\Big(\\frac{\\partial^{2} l(\\beta)}{{\\partial \\beta}{\\partial \\beta}^{T}}\\Big)\\).\n\\(H = -\\mathcal{I}\\), or \\(\\frac{\\partial^{2} l(\\beta)}{{\\partial \\beta}{\\partial \\beta}^{T}} = E\\Big(-\\frac{\\partial^{2} l(\\beta)}{{\\partial \\beta}{\\partial \\beta}^{T}}\\Big)\\).\nThe Newton-Raphson and Fisher Scoring algorithms yield identical results.\n\n\n\nFisher Scoring Implementation\nThe data used for our sample calculation can be obtained here. The data represents O-Ring failures in the 23 pre-Challenger space shuttle missions. TEMPERATURE will serve as the single explanatory variable which will be used to predict O_RING_FAILURE, which is 1 if a failure occurred, 0 otherwise.\nOnce the parameters have been determined, the model estimate of the probability of success for a given observation can be calculated with:\n\\[\n\\hat\\pi_{i} = \\frac {e^{\\hat\\beta_{0} + \\hat\\beta_{1}x_{i1} + \\ldots + \\hat\\beta_{p}x_{ip}}}{1 + e^{\\hat\\beta_{0} + \\hat\\beta_{1}x_{i1} + \\ldots + \\hat\\beta_{p}x_{ip}}}\n\\]\nIn the following code, we define a single function, get_params, which returns the estimated model coefficients as a (p+1)-by-1 array. In addition, the function returns the number of scoring iterations, fitted values and the variance-covariance matrix for the estimated parameters.\n\n\nimport numpy as np\n\ndef estimate_lr_params(X, y, epsilon=.001):\n    \"\"\"\n    Estimate logistic regression coefficients using Fisher Scoring.Iteration \n    ceases once changes between elements in coefficient matrix across\n    consecutive iterations is less than epsilon.\n   \n        - design_matrix      `X` : n-by-(p+1)                                \n        - response_vector    `y` : n-by-1                                   \n        - probability_vector `p` : n-by-1                                   \n        - weights_matrix     `W` : n-by-n                                    \n        - epsilon                : threshold above which iteration continues\n        - n                      : Number of observations                        \n        - (p + 1)                : Number of parameters (+1 for intercept term) \n   \n        - U: First derivative of log-likelihood with respect to                \n           each beta_i, i.e. \"Score Function\" = X^T * (y - p)        \n           \n        - I: Second derivative of log-likelihood with respect to               \n           each beta_i, i.e. \"Information Matrix\" = (X^T * W * X)      \n                                                                           \n        - X^T*W*X results in a (p + 1)-by-(p + 1) matrix.                          \n        - X^T(y - p) results in a (p+1)-by-1 matrix.                            \n        - (X^T*W*X)^-1 * X^T(y - p) results in a (p + 1)-by-1 matrix.     \n\n    Returns\n    -------\n    dict of model results        \n   \n    \"\"\"\n    def sigmoid(v): \n        return (1 / (1 + np.exp(-v)))\n    \n\n    betas0 = np.zeros(X.shape[1]).reshape(-1, 1)\n    p = sigmoid(X @ betas0)\n    W = np.diag((p * (1 - p)).ravel())\n    I = X.T @ W @ X\n    U = X.T @ (y - p)\n\n    n_iter = 0\n    \n    while True:\n        n_iter+=1        \n        betas = betas0 + np.linalg.inv(I) @ U\n        betas = betas.reshape(-1, 1)\n\n        if np.all(np.abs(betas - betas0) &lt; epsilon):\n            break\n        else:\n            p = sigmoid(X @ betas)\n            W = np.diag((p * (1 - p)).ravel())\n            I = X.T @ W @ X\n            U = X.T @ (y - p)\n            betas0 = betas\n\n    dresults = {\n        \"params\": betas.ravel(),\n        \"ypred\": sigmoid(X @ betas),\n        \"V\": np.linalg.inv(I),\n        \"n_iter\": n_iter\n        }\n\n    return(dresults)\n\n\n\nWe read in the Challenger dataset, partition the data into the design matrix and response vector, which are then passed to estimate_lr_params:\n\n\nimport pandas as pd\n\ndf = pd.read_csv(\"https://gist.githubusercontent.com/jtrive84/835514a76f7afd552c999e4d9134baa8/raw/6dac51b80f892ef051174a46766eb53c7b609ebd/Challenger.csv\")\n\nX0 = df[[\"TEMPERATURE\"]].values\nX = np.concatenate([np.ones(X0.shape[0]).reshape(-1, 1), X0], axis=1)\ny = df[[\"O_RING_FAILURE\"]].values\n\ndresults = estimate_lr_params(X, y)\n\ndresults\n\n{'params': array([15.04290163, -0.23216274]),\n 'ypred': array([[0.43049313],\n        [0.22996826],\n        [0.27362106],\n        [0.32209405],\n        [0.37472428],\n        [0.1580491 ],\n        [0.12954602],\n        [0.22996826],\n        [0.85931657],\n        [0.60268105],\n        [0.22996826],\n        [0.04454055],\n        [0.37472428],\n        [0.93924781],\n        [0.37472428],\n        [0.08554356],\n        [0.22996826],\n        [0.02270329],\n        [0.06904407],\n        [0.03564141],\n        [0.08554356],\n        [0.06904407],\n        [0.82884484]]),\n 'V': array([[ 5.44406534e+01, -7.96333573e-01],\n        [-7.96333573e-01,  1.17143602e-02]]),\n 'n_iter': 5}\n\n\nestimate_lr_params returns a dictionary consisting of the following keys:\n\n\"params\": Estimated parameters.\n\"ypred\": Fitted values.\n\n\"V\": Variance-covariance matrix of the parameter estimates.\n\n\"n_iter\": Number of Fisher scoring iterations.\n\nFor the Challenger dataset, our implementation of Fisher scoring results in a model with \\(\\hat \\beta_{0} = 15.0429\\) and \\(\\hat \\beta_{1} = -0.2322\\). In order to predict new probabilities of O-Ring Failure based on temperature, we use:\n\\[\n\\hat{\\pi} = \\frac {1}{1 + e^{-(15.0429 -0.2322 \\times \\mathrm{TEMPERATURE})}}.\n\\]\nNegative coefficients correspond to features that are negatively associated with the probability of a positive outcome, with the reverse being true for positive coefficients.\nLets compare the results of our implementation against the estimates produced by statsmodels:\n\n\nimport statsmodels.formula.api as smf\n\nmdl = smf.logit(\"O_RING_FAILURE ~ TEMPERATURE\", data=df).fit()\n\n\nprint(f\"\\nmdl.params:\\n{mdl.params}\\n\")\nprint(f\"mdl.cov_params():\\n{mdl.cov_params()}\\n\")\nprint(f\"mdl.predict(df):\\n{mdl.predict(df)}\\n\")\n\nOptimization terminated successfully.\n         Current function value: 0.441635\n         Iterations 7\n\nmdl.params:\nIntercept      15.042902\nTEMPERATURE    -0.232163\ndtype: float64\n\nmdl.cov_params():\n             Intercept  TEMPERATURE\nIntercept    54.444275    -0.796387\nTEMPERATURE  -0.796387     0.011715\n\nmdl.predict(df):\n0     0.430493\n1     0.229968\n2     0.273621\n3     0.322094\n4     0.374724\n5     0.158049\n6     0.129546\n7     0.229968\n8     0.859317\n9     0.602681\n10    0.229968\n11    0.044541\n12    0.374724\n13    0.939248\n14    0.374724\n15    0.085544\n16    0.229968\n17    0.022703\n18    0.069044\n19    0.035641\n20    0.085544\n21    0.069044\n22    0.828845\ndtype: float64\n\n\n\nThe values produced using the statsmodels align closely with the results from estimate_lr_params.\nA feature of logistic regression models is that the predictions preserve the data’s marginal probabilities. If you aggregate the fitted values from the model, the total will equal the number of positive outcomes in the original target vector:\n\n\n# estimate_lr_params.\ndresults[\"ypred\"].sum()\n\n7.000000000274647\n\n\n\n\n# statsmodels.\nmdl.predict(df).sum()\n\n7.0000000000000036\n\n\nWe have 7 positive instances in our dataset, and the total probability aggregates to 7 in both sets of predictions."
  },
  {
    "objectID": "posts/logistic-from-scratch-r/logistic-from-scratch-r.html",
    "href": "posts/logistic-from-scratch-r/logistic-from-scratch-r.html",
    "title": "Estimating Logistic Regression coefficients From Scratch in R",
    "section": "",
    "text": "In this post, we highlight the parameter estimation routines called behind the scenes upon invocation of R’s glm function. Specifically, we’ll focus on how parameters of a logistic regression model are estimated when fit to data having a binary response.\nR’s glm function is used to fit generalized linear models, specified by giving a symbolic description of the linear predictor and a description of the error distribution. This function conceals a good deal of the complexity behind a simple interface, making it easy to overlook the calculations that estimate a model’s coefficients. The goal of this post is to shed some light on the mechanics of those calculations."
  },
  {
    "objectID": "posts/logistic-from-scratch-r/logistic-from-scratch-r.html#using-the-model-to-calculate-probabilities",
    "href": "posts/logistic-from-scratch-r/logistic-from-scratch-r.html#using-the-model-to-calculate-probabilities",
    "title": "Estimating Logistic Regression coefficients From Scratch in R",
    "section": "Using The Model to Calculate Probabilities",
    "text": "Using The Model to Calculate Probabilities\nTo apply the model generated by glm to a new set of explanatory variables, use the predict function. Pass a list or data.frame of explanatory variables to predict, and for logistic regression models, be sure to set type=\"response\" to ensure probabilities are returned. For example:\n# New inputs for Logistic Regression model.\n&gt; tempsDF &lt;- data.frame(TEMPERATURE=c(24, 41, 46, 47, 61))\n\n&gt; predict(logistic.fit, tempsDF, type=\"response\")\n\n        1         2         3         4         5 \n0.9999230 0.9960269 0.9874253 0.9841912 0.7070241"
  },
  {
    "objectID": "posts/introduction-to-metropolis-hastings/introduction-to-metropolis-hastings.html",
    "href": "posts/introduction-to-metropolis-hastings/introduction-to-metropolis-hastings.html",
    "title": "Introduction to Markov Chain Monte Carlo - The Metropolis-Hastings Algorithm",
    "section": "",
    "text": "Markov Chain Monte Carlo (MCMC) is a class of algorithms used to sample from probability distributions when direct sampling is difficult or inefficient. It leverages Markov chains to explore the target distribution and Monte Carlo methods to perform repeated random sampling. MCMC algorithms are widely used in the insurance industry, particularly in areas involving risk assessment, pricing, reserving, and capital modeling. Markov Chain Monte Carlo is an alternative to rejection sampling, which can be inefficient when dealing with high-dimensional probability distributions. MCMC is considered a Bayesian approach to statistical inference since it incorporates both prior knowledge and observed data into the estimation of the posterior distribution.\nThe Metropolis-Hastings algorithm is a method used to generate a sequence of samples from a probability distribution for which direct sampling might be difficult. It is a particiular variant of MCMC, which approximates a desired distribution by creating a chain of values that resemble samples drawn from that distribution. The algorithm generates a sequence of samples by proposing new candidates and deciding whether to accept or reject them based on a ratio of probabilities from the target distribution.\nBefore getting into the details of Metropolis-Hastings, a few key definitions:\nLikelihood:  The apriori assumption specifying the distribution from which the data are assumed to originate. For example, if we assume losses follow an exponential distribution within unknown parameter \\(\\theta\\), this is equivalent to specifying an exponential likelihood. Symbolically, the likelihood is represented as \\(f(x|\\theta)\\).\nPrior: Sticking with the exponential likelihood example, once we’ve proposed the likelihood, we need to specify a distribution for each parameter of the likelihood. In the case of the exponential there is only a single parameter, \\(\\theta\\). Typically when selecting prior distributions, it should have the same domain as the parameter itself. When parameterizing the exponential distribution, we know that \\(0 &lt; \\theta &lt; \\infty\\), so the prior distribution should be valid on \\((0, \\infty)\\). Valid distributions for \\(\\theta\\) are gamma, lognormal, pareto, weibull, etc. Invalid distributions would be any discrete distribution or the normal distribution. Symbolically, the prior is represented as \\(f(\\theta)\\).\nPosterior:  This is the expression which encapsulates the power, simplicity and flexibility of the Bayesian approach and is given by:\n\\[\n\\mathrm{Posterior} \\propto \\mathrm{Likelihood} \\times \\mathrm{Prior}\n\\]\nThe posterior is represented as \\(f(\\theta|x)\\), so the above expression becomes:\n\\[\nf(\\theta|x) \\propto f(x|\\theta) f(\\theta).\n\\]\nWe can update the proportionality to direct equality by the inclusion of a normalizing constant, which ensures the expression on the RHS integrates to 1:\n\\[\nf(\\theta|x) = \\frac{f(x|\\theta)  f(\\theta)}{f(x)}.\n\\]"
  },
  {
    "objectID": "posts/introduction-to-metropolis-hastings/introduction-to-metropolis-hastings.html#severity-modeling",
    "href": "posts/introduction-to-metropolis-hastings/introduction-to-metropolis-hastings.html#severity-modeling",
    "title": "Introduction to Markov Chain Monte Carlo - The Metropolis-Hastings Algorithm",
    "section": "Severity Modeling",
    "text": "Severity Modeling\nMCMC approaches can be leveraged to estimate severity or size-of-loss curves for a given line of business based on past claim history. Severity estimates are used in multiple actuarial contexts, especially reserving and capital modeling. Imagine we have loss data we believe originates from an exponential distribution with unknown rate parameter:\n\n266, 934, 138\n\nWe again assume a conjugate relationship between prior and posterior distributions:\n\nLikelihood: Losses are exponentially distributed with unknown rate \\(\\lambda\\).\nPrior: Gamma with \\(\\alpha_{0}\\), \\(\\beta_{0}\\).\nPosterior: Gamma with \\(\\alpha^{'} = \\alpha_{0} + n\\) and \\(\\beta^{'} = \\beta_{0} + \\sum_{i=1}^{n} x_{i}\\).\nPosterior predictive: Lomax (shifted Pareto with support beginning at zero) with \\(\\alpha^{'}, \\beta^{'}\\). The expected value of the posterior predictive distribution is \\(\\frac{\\beta^{'}}{\\alpha^{'} - 1}\\).\n\nWe judgmentally set \\(\\alpha_{0} = 2\\) and \\(\\beta_{0} = 1,000\\). Prior and posterior predictive means are computed in the next cell.\n\n\ny = [266, 934, 138]\n\n# Judgmentally select a0 and b0.\na0 = 2\nb0 = 1000\nn = len(y)\na_posterior = a0 + n\nb_posterior = b0 + np.sum(y)\nprior_mean = a0 * b0\npost_pred_mean = b_posterior / (a_posterior - 1)\n\nprint(f\"a0              : {a0}\")\nprint(f\"b0              : {b0}\")\nprint(f\"a_posterior     : {a_posterior}.\")\nprint(f\"b_posterior     : {b_posterior}.\")\nprint(f\"Empirical mean  : {np.mean(y)}\")\nprint(f\"Prior mean      : {prior_mean}\")\nprint(f\"Post. pred. mean: {post_pred_mean:.2f}\")\n\na0              : 2\nb0              : 1000\na_posterior     : 5.\nb_posterior     : 2338.\nEmpirical mean  : 446.0\nPrior mean      : 2000\nPost. pred. mean: 584.50\n\n\n\nUsing Metropolis-Hastings, the mean of generated samples should match the posterior predictive mean obtained from the analytical expression (584.50 above). Adapting the sampling code from the previous model, an exponential distribution is used to generate proposals, since the exponential scale parameter must be strictly greater than 0. We have the following:\n\n\"\"\"\nImplementation of Metropolis-Hastings algorithm for exponential likelihood\nwith gamma prior.\nGoal is to recover the posterior distribution of the unknown parameter lambda. \n\"\"\"\nimport numpy as np\nfrom scipy.stats import expon, norm, gamma, lomax\n\nrng = np.random.default_rng(516)\n\nnbr_samples = 10000\ny = [266, 934, 138]\na0 = 2\nb0 = 1000\n\n\n# Array to hold posterior samples, initialized with prior mean.\nsamples = np.zeros(nbr_samples)\nsamples[0] = np.mean(y)\n\n# Initialize prior density.\nprior = gamma(a=a0, loc=0, scale=b0)\n\n# Track the number of accepted samples. \naccepted = 0\n\nfor ii in range(1, nbr_samples):\n\n    # Get most recently accepted sample.\n    theta = samples[ii - 1]\n\n    # Generate sample from proposal distribution.\n    theta_star = rng.exponential(scale=theta)\n\n    # Compute numerator and denominator of acceptance ratio.\n    numer = np.prod(expon(scale=theta_star).pdf(y)) * prior.pdf(theta_star)\n    denom = np.prod(expon(scale=theta).pdf(y)) * prior.pdf(theta)\n    ar = numer / denom\n\n    # Generate random uniform sample.\n    u = rng.uniform(low=0, high=1)\n    \n    # Check whether theta_star should be added to samples by comparing ar with u.\n    if ar &gt;= u:\n        theta = theta_star\n        accepted+=1\n\n    # Update samples array.\n    samples[ii] = theta\n\n    if ii % 1000 == 0:\n        print(f\"{ii}: theta_star: {theta_star:.5f}, ar: {ar:.5f}, curr_rate: {accepted / ii:.5f}\")\n\nacc_rate = accepted / nbr_samples\n\nprint(f\"\\nAcceptance rate    : {acc_rate:.3f}.\")\nprint(f\"Posterior sample mean: {samples.mean():.3f}.\")\n\n1000: theta_star: 82.99100, ar: 0.00008, curr_rate: 0.45900\n2000: theta_star: 544.00186, ar: 1.07857, curr_rate: 0.46450\n3000: theta_star: 246.98682, ar: 0.33871, curr_rate: 0.47433\n4000: theta_star: 73.75990, ar: 0.00002, curr_rate: 0.47900\n5000: theta_star: 451.83333, ar: 0.97910, curr_rate: 0.47880\n6000: theta_star: 607.07687, ar: 1.45090, curr_rate: 0.47833\n7000: theta_star: 1509.11070, ar: 0.74937, curr_rate: 0.48186\n8000: theta_star: 677.30894, ar: 1.04365, curr_rate: 0.48187\n9000: theta_star: 740.90659, ar: 1.58914, curr_rate: 0.48156\n\nAcceptance rate    : 0.481.\nPosterior sample mean: 586.353.\n\n\n\nVisualizing the histogram of posterior samples along with the traceplot:\n\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5), tight_layout=True) \n\nax[0].set_title(\n    \"Posterior samples (exponential likelihood)\", \n    color=\"#000000\", loc=\"center\", fontsize=9\n)\nax[0].hist(\n    samples, 50, density=True, alpha=1, color=\"#ff7595\", \n    edgecolor=\"#FFFFFF\", linewidth=1.0\n)\nax[0].axvline(\n    samples.mean(), color=\"#000000\", linewidth=1.5, linestyle=\"--\", \n    label=\"posterior mean\"\n)\n\nax[0].set_xlabel(\"\")\nax[0].set_ylabel(\"\")\nax[0].set_xlim(0, 2500)\nax[0].xaxis.set_major_formatter(mpl.ticker.StrMethodFormatter(\"{x:,.0f}\"))\nax[0].tick_params(axis=\"x\", which=\"major\", direction='in', labelsize=7)\nax[0].tick_params(axis=\"x\", which=\"minor\", direction='in', labelsize=7)\nax[0].tick_params(axis=\"y\", which=\"major\", direction='in', labelsize=7)\nax[0].tick_params(axis=\"y\", which=\"minor\", direction='in', labelsize=7)\nax[0].xaxis.set_ticks_position(\"none\")\nax[0].yaxis.set_ticks_position(\"none\")\nax[0].grid(True)   \nax[0].set_axisbelow(True) \nax[0].legend(loc=\"upper right\", fancybox=True, framealpha=1, fontsize=\"medium\")\n\nax[1].set_title(\"Traceplot\", color=\"#000000\", loc=\"center\", fontsize=9)\nax[1].plot(samples, color=\"#000000\", linewidth=.25, linestyle=\"-\")\nax[1].xaxis.set_major_formatter(mpl.ticker.StrMethodFormatter(\"{x:,.0f}\"))\nax[1].set_xlabel(\"sample\", fontsize=9)\nax[1].set_ylabel(r\"$\\hat \\beta$\")\nax[1].tick_params(axis=\"x\", which=\"major\", direction='in', labelsize=7)\nax[1].tick_params(axis=\"y\", which=\"major\", direction='in', labelsize=7)\nax[1].xaxis.set_ticks_position(\"none\")\nax[1].yaxis.set_ticks_position(\"none\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nThe distribution of posterior samples resembles a gamma distribution, which we expect.\nNext, to generate posterior predictive samples, we randomly sample from an exponential distribution parameterized using each scale parameter. This is accomplished in the next cell:\n\n\"\"\"\nGenerate posterior predictive samples, one random draw per posterior scale sample.\n\"\"\"\n\npost_pred_samples = rng.exponential(scale=samples)\n\nprint(f\"Posterior predictive mean (cp): {post_pred_mean:.5f}\")\nprint(f\"Posterior predictive mean (mh): {post_pred_samples.mean():.5f}\")\n\nPosterior predictive mean (cp): 584.50000\nPosterior predictive mean (mh): 585.71307\n\n\n\nWe can overlay the posterior predictive distribution with the histogram of posterior predictive samples and assess how well they match:\n\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\npp = lomax(c=a_posterior, scale=b_posterior)\nxx = np.linspace(0, pp.ppf(.995), 1000)\nyy = pp.pdf(xx)\n\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 5.5), tight_layout=True) \n\nax.set_title(\"Posterior predictive samples with analytical density\", color=\"#000000\", loc=\"center\", fontsize=10)\n\nax.hist(\n    post_pred_samples, 100, density=True, alpha=1, color=\"#ff7595\", \n    edgecolor=\"#FFFFFF\", linewidth=1.0, label=\"Posterior predictive samples\"\n)\nax.plot(\n    xx, yy, alpha=1, color=\"#000000\", linewidth=1.5, linestyle=\"--\", \n    label=r\"$\\mathrm{Lomax}($\" + f\"{a_posterior:.0f}, {b_posterior:.0f}\" + r\"$)$\"\n)\nax.axvline(\n    post_pred_mean, color=\"green\", linewidth=1.5, linestyle=\"-.\", \n    label=\"posterior preditive mean\"\n)\n\nax.set_xlim(0, 5000)\nax.set_xlabel(\"\")\nax.set_ylabel(\"\")\nax.tick_params(axis=\"x\", which=\"major\", direction='in', labelsize=7)\nax.tick_params(axis=\"x\", which=\"minor\", direction='in', labelsize=7)\nax.tick_params(axis=\"y\", which=\"major\", direction='in', labelsize=6)\nax.tick_params(axis=\"y\", which=\"minor\", direction='in', labelsize=6)\nax.xaxis.set_major_formatter(mpl.ticker.StrMethodFormatter(\"{x:,.0f}\"))\nax.xaxis.set_ticks_position(\"none\")\nax.yaxis.set_ticks_position(\"none\")\nax.grid(True)   \nax.set_axisbelow(True) \nax.legend(loc=\"upper right\", fancybox=True, framealpha=1, fontsize=\"medium\")\nplt.show()\n\n\n\n\n\n\n\n\n\nThe plot shows the two distributions align well. Finally, we can compare quantiles of our posterior predictive samples with the analytical density to see how well they agree in the extreme left and right tails:\n\n\nimport pandas as pd\n\nq = [.01, .025, .05, .10, .25, .50, .75, .90, .95, .99, .995, .999]\n\ndf = pd.DataFrame({\n    \"q\": q,\n    \"cp\": pp.ppf(q), \n    \"mh\": np.quantile(post_pred_samples, q)\n})\n\ndf[\"error\"] = 100 * (df[\"cp\"] - df[\"mh\"]) / df[\"cp\"] \n\ndf.head(12)\n\n\n\n\n\n\n\n\nq\ncp\nmh\nerror\n\n\n\n\n0\n0.010\n4.704263\n4.841719\n-2.921931\n\n\n1\n0.025\n11.868630\n12.136623\n-2.257991\n\n\n2\n0.050\n24.108192\n23.368946\n3.066368\n\n\n3\n0.100\n49.789318\n47.340357\n4.918647\n\n\n4\n0.250\n138.465340\n135.870227\n1.874197\n\n\n5\n0.500\n347.656754\n347.845394\n-0.054260\n\n\n6\n0.750\n747.009495\n736.928440\n1.349522\n\n\n7\n0.900\n1367.480284\n1387.431296\n-1.458962\n\n\n8\n0.950\n1918.479107\n1958.620885\n-2.092375\n\n\n9\n0.990\n3534.790477\n3488.275714\n1.315913\n\n\n10\n0.995\n4408.064760\n4380.679896\n0.621245\n\n\n11\n0.999\n6969.745648\n6872.765923\n1.391439\n\n\n\n\n\n\n\n\nIn the table above:\n\nq represents the target quantile.\ncp represents analytical quantiles from the conjugate prior posterior predictive distribution.\nmh represents quantiles from the Metropolis-Hastings generated posterior predictive samples.\nerror represents the percent deviation from analytical quantiles.\n\nEven at q=0.999, cp and mh differ by less than 1.50%.\nUnfortunately, most distributional relationships used in practice are not conjugate. But by leveraging conjugate relationships we were able to demonstrate that when the same likelihood, prior and loss data are used, Metropolis-Hastings will yield distributional estimates of the posterior predictive distribution very to close to the analytical distribution.\nWhile implementing your own MCMC sampler is a great way to gain a better understanding of the inner workings of Markov Chain Monte Carlo, in practice it is almost always preferrable to an optimized MCMC library such as PyStan or PyMC3. These will be explored in a future post."
  },
  {
    "objectID": "posts/gof-python-scipy/gof-python-scipy.html",
    "href": "posts/gof-python-scipy/gof-python-scipy.html",
    "title": "Assessing Model Goodness-of-Fit in Python with Scipy",
    "section": "",
    "text": "This article explores techniques that can be used to assess how well a model fits a dataset. Specifically, we’ll demonstrate how to produce the following visualizations:\n\nQ-Q Plot: Compares two probability distributions by plotting their quantiles against each other.\n\nP-P Plot: Compares two cumulative distribution functions against each other.\n\nHistogram: Plot density histogram with parametric distribution overlay.\n\nIn addition, the following tests will be introduced:\n\nKolmogorov-Smirnov: Test the equality of continuous, one-dimensional probability distributions.\n\nAnderson-Darling: Test whether a given sample is drawn from a given probability distribution.\n\nShapiro-Wilk: Test the null hypothesis that the data is drawn from a normal distribution.\n\nThe same dataset will be used throughout the post, provided below:\n\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import stats\n\nnp.set_printoptions(suppress=True, precision=8)\n\ndat = np.asarray([\n    62.55976, -14.71019, -20.67025, -35.43758, -10.65457,  21.55292, \n    41.26359,   0.33537, -14.43599, -40.66612,   6.45701, -40.39694, \n     55.1221,  24.50901,   6.61822, -29.10305,   6.21494,  15.25862,  \n    13.54446,   2.48212,  -2.34573, -21.47846,   -5.0777,  26.48881, \n    -8.68764,  -5.49631,  42.58039,  -6.59111, -23.08169,  19.09755, \n   -21.35046,   0.24064,  -3.16365, -37.43091,  24.48556,    2.6263,  \n    31.14471,   5.75287,  -46.8529, -14.26814,   8.41045,  18.11071, \n   -30.46438,  12.22195, -31.83203,  -8.09629,  52.06456, -24.30986, \n   -25.62359,   2.86882,  15.77073,  31.17838, -22.04998\n    ])\n\nThe task is to assess how well our data fits a normal distribution parameterized with mean and variance computed using:\n\\[\n\\begin{align*}\n\\bar{x} &= \\frac{1}{n}\\sum_{i=1}^{n} x_{i} \\\\\ns^{2} &= \\frac{1}{n-1}\\sum_{i=1}^{n} (x_{i} - \\bar{x})^2\n\\end{align*}\n\\]\nKeep in mind that although we’re testing how well othe data can be approximated by a normal distribution, many of the tests we highlight (with the exception of Shapiro-Wilk) can assess the quality of fit for many different parametric models.\nWe begin with visual assessments of goodness-of-fit.\n\nQ-Q Plot\nThe Q-Q plot compares two probability distributions by plotting their quantiles against each other. We compare standard normal quantiles (x-axis) against the empirical quantiles from the dataset of interest (y-axis). If the two distributions are similar, the points in the Q-Q plot will approximately lie on a straight line. There isn’t a hard and fast rule to determine how much deviation from the straight line is too much, but if the distributions are very different, it will be readily apparent in the Q-Q plot. We can construct a Q-Q plot from scratch using matplotlib as follows:\n\n\"\"\"\nGenerate qq-plot comparing data against standard normal distribution.\n\"\"\"\n\nline_color = \"#E02C70\"\ndat = np.sort(dat)\ncdf = np.arange(1, dat.size + 1) / dat.size\nndist = stats.norm(loc=0, scale=1)\ntheo = ndist.ppf(cdf)\n\n\n# Remove observations containing Inf.\nx, y = zip(*[tt for tt in zip(theo, dat) if np.Inf not in tt and -np.Inf not in tt])\n\n# Obtain coefficients for best fit regression line.\nb1, b0, _, _, _ = stats.linregress(x, y)\nyhat = [b1 * ii + b0 for ii in x]\n\n# Determine upper and lower axis bounds.\nxmin, xmax = min(x), max(x)\nymin, ymax = int(min(y) - 1), int(max(y) + 1)\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 5), tight_layout=True)\nax.set_title(\n    \"Q-Q Plot: Data vs. Standard Normal Distribution\",\n    color=\"#000000\", loc=\"center\", fontsize=10, weight=\"bold\"\n    )\nax.scatter(x, y, color=\"#FFFFFF\", edgecolor=\"#000000\", linewidth=.75, s=45)\nax.plot(x, yhat, color=line_color, linewidth=1.25)\nax.set_xlim(left=xmin, right=xmax)\nax.set_ylim(bottom=ymin, top=ymax)\nax.set_ylabel(\"Empirical Quantiles\", fontsize=9, color=\"#000000\")\nax.set_xlabel(\"Standard Normal Quantiles\", fontsize=9, color=\"#000000\")\nax.tick_params(axis=\"x\", which=\"major\", labelsize=8)\nax.tick_params(axis=\"y\", which=\"major\", labelsize=8)\nax.xaxis.set_ticks_position(\"none\")\nax.yaxis.set_ticks_position(\"none\")\nax.grid(True)   \nax.set_axisbelow(True) \nplt.show()\n\n\n\n\n\n\n\n\nThe points seem to mostly follow a straight line, but there are observations that deviate from strict linearity. However, there’s nothing here that disqualifies our dataset from being modeled with normal distribution.\n\n\nP-P Plot\nThe P-P plot compares two cumulative distribution functions against each other. To produce a P-P plot, we plot the theoretical percentiles (x-axis) against empirical percentiles (y-axis), so that each axis ranges from 0-1. The line of comparison is the 45 degree line running from (0,0) to (1,1). The distributions are equal if and only if the plot falls on this line: any deviation indicates a difference between the distributions. The code to generate a P-P plot is provided below:\n\n\"\"\"\nCreate P-P plot, which compares theoretical normal percentiles (x-axis) against \nempirical percentiles (y-axis).\n\"\"\"\ndat = np.sort(dat)\ndat_mean = dat.mean()\ndat_std = dat.std(ddof=1)\n\n# Standardize dat.\nsdat = (dat - dat_mean) / dat_std\ncdf = np.arange(1, dat.size + 1) / dat.size\nndist = stats.norm(loc=0, scale=1)\ntheo = ndist.cdf(sdat)\nx, y = theo, cdf\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 5), tight_layout=True)\nax.set_title(\n    \"P-P Plot: Empricial CDF vs. Standard Normal CDF\",\n    color=\"#000000\", loc=\"center\", fontsize=10, weight=\"bold\"\n    )\n\nax.scatter(x, y, color=\"#FFFFFF\", edgecolor=\"#000000\", linewidth=.75, s=45)\nax.plot([0, 1], [0, 1], color=line_color, linewidth=1.25)\nax.set_xlim(left=0, right=1)\nax.set_ylim(bottom=0, top=1)\nax.set_ylabel(\"Empirical Cumulative Distribution\", fontsize=9,color=\"#000000\")\nax.set_xlabel(\"Standard Normal Distribution\", fontsize=9, color=\"#000000\")\nax.tick_params(axis=\"x\", which=\"major\", labelsize=8)\nax.tick_params(axis=\"y\", which=\"major\", labelsize=8)\nax.xaxis.set_ticks_position(\"none\")\nax.yaxis.set_ticks_position(\"none\")\nax.grid(True)   \nax.set_axisbelow(True) \nplt.show()\n\n\n\n\n\n\n\n\nAlthough the observations follow the linear trend in general, the data overall appear somewhat above the reference line \\(y=x\\). This may be attributable to the mean of dat being greater than 0. However, this doesn’t eliminate the possibility of our data representing a sample from a normal population. We expect some deviation from the expected normal percentiles, which we see in the P-P plot.\n\n\nHistogram with Parametric Overlay\nFor the next diagnostic we create a histogram which represents the density of the empirical data overlaid with a parameterized normal distribution.\n\n\"\"\"\nPlot histogram with best-fit normal distribution overlay.\n\"\"\"\ndist = stats.norm(loc=dat_mean, scale=dat_std)\nxdist = np.arange(dat.min(), dat.max(), .01)\nydist = dist.pdf(xdist)\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 5), tight_layout=True)\nax.set_title(\n    \"Empirical Data w/ Parametric Overlay\", color=\"#000000\",\n    loc=\"center\", fontsize=10, weight=\"bold\"\n    )\nax.hist(\n    dat, 13, density=True, alpha=1, color=\"#E02C70\", \n    edgecolor=\"#FFFFFF\", linewidth=1.0\n    )\n\nax.plot(xdist, ydist, color=\"#000000\", linewidth=1.75, linestyle=\"--\")\nax.set_xlabel(\"\")\nax.set_ylabel(\"\")\nax.tick_params(axis=\"x\", which=\"major\", labelsize=7)\nax.tick_params(axis=\"y\", which=\"major\", labelsize=7)\nax.xaxis.set_ticks_position(\"none\")\nax.yaxis.set_ticks_position(\"none\")\nax.grid(True)\nax.set_axisbelow(True)\nplt.show()\n\n\n\n\n\n\n\n\nThe data appear to follow a pattern roughly the shape outlined by a best-fit normal density.\n\n\nKolmogorov-Smirnov Test\nThe Kolmogorov-Smirnov Test is different than the previous set of visualizations in that it produces a metric used to assess the level of agreement between target and reference distributions, but a visual diagnostic can be obtained as well.\nSuppose that we have a set of empirical data that we assume originates from some distribution \\(F\\). The Kolmogorov-Smirnov statistic is used to test:\n\n\\(H_{0}\\) : the samples come from \\(F\\)\n\nagainst:\n\n\\(H_{1}\\) : The samples do not come from \\(F\\)\n\nThe test compares the empirical distribution function of the data, \\(F_{obs}\\), with the cumulative distribution function associated with the null hypothesis, \\(F_{exp}\\) (the expected CDF).\nThe Kolmogorov-Smirnov statistic is given by\n\\[\nD_{n} = max|F_{exp}(x) - F_{obs}(x)|.\n\\]\nAssuming the data are ordered such that \\(x_{1}\\) represents the the minimum value in the dataset and \\(x_{n}\\) the maximum value, the empirical CDF can be represented as\n\\[\nF_{obs}(x_{i}) = \\frac{i}{n},\n\\]\nwhere \\(n\\) is the number of observations in the dataset.\nFor each observation, compute the absolute differences between \\(F_{exp}(x)\\) and \\(F_{obs}(x)\\). The Kolmogorov-Smirnov statistic \\(D_{n}\\) is the maximum value from the vector of absolute differences. This value represents the maximum absolute distance between the expected and observed distribution functions. \\(D_{n}\\) is then compared to a table of critical values to assess whether to reject or fail to reject \\(H_{0}\\).\nBefore computing the statistic, we first demonstrate how to generate the one-sample Kolmogorov-Smirnov comparison plot:\n\n\"\"\"\nKolmogorov-Smirnov test visualization.\n\"\"\"\ndat = np.sort(dat)\ndat_mean = dat.mean()\ndat_std = dat.std(ddof=1)\nsdat = (dat - dat_mean) / dat_std\ncdf = np.arange(1, dat.size + 1) / dat.size\ndist = stats.norm(loc=0, scale=1)\n\n# Generate Kolmogorov-Smirnov comparison plot.\n# y0 : Values from reference distribution.\n# y1 : Values from empirical distribution.\necdfpairs = zip(sdat, cdf)\necdfpairs = [ii for ii in ecdfpairs if np.Inf not in ii and -np.Inf not in ii]\nx, y1 = zip(*ecdfpairs)\nx = np.asarray(x, dtype=float)\ny0 = dist.cdf(x)\ny1 = np.asarray(y1, dtype=float)\n\nabsdiffs = np.abs(y1 - y0)\nindx = np.argwhere(absdiffs == absdiffs.max()).ravel()[0]\nxann, y0ann, y1ann  = x[indx], y0[indx], y1[indx]\nypoint = (y1ann + y0ann) / 2\nxy, xyp = (xann, ypoint), (xann + .3, ypoint - .1)\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 5), tight_layout=True)\nxmin, xmax, ymin, ymax = min(x), max(x), 0, 1\nax.set_title(\n    \"Kolmogorov-Smirnov Illustration\", fontsize=10,\n    loc=\"center\", color=\"#000000\", weight=\"bold\"\n    )\nax.set_xlim(left=xmin, right=xmax)\nax.set_ylim(bottom=ymin, top=ymax)\nax.plot(x, y0, color=\"#000000\", linewidth=1.5, linestyle=\"--\", label=\"Reference CDF\")\nax.step(x, y1, color=\"#f33455\", linewidth=1.5, where=\"pre\", label=\"Empirical CDF\")\nax.tick_params(axis=\"x\", which=\"major\", labelsize=8)\nax.tick_params(axis=\"y\", which=\"major\", labelsize=8)\nax.set_ylabel(\"CDF\", fontsize=9)\nax.set_xlabel(\"z\", fontsize=9)\nax.xaxis.set_ticks_position(\"none\")\nax.yaxis.set_ticks_position(\"none\")\nax.grid(True)\nax.set_axisbelow(True)\nplt.annotate(\n    \"Maximum Absolute Distance\", xy=xy, xytext=xyp,\n    arrowprops=dict(facecolor=\"black\", width=1.5, shrink=0.025, headwidth=6.5)\n    )\nax.legend(\n    frameon=1, loc=\"upper left\", fontsize=\"medium\", fancybox=True, framealpha=1\n    )\nplt.show()\n\n\n\n\n\n\n\n\nThe Kolmogorov-Smirnov statistic is computed as the greatest absolute distance between the empirical and expected CDFs. Computing the statistic is straightforward:\n\n\ndat = np.sort(dat)\ncdf = np.arange(1, dat.size + 1) / dat.size\ndat_mean = dat.mean()\ndat_std = dat.std(ddof=1)\n\n# Parameterized expected normal distribution.\nexpnorm = stats.norm(loc=dat_mean, scale=dat_std)\nexpcdf = expnorm.cdf(dat)\n\n# Compute difference between datcdf and expcdf.\nabsdiffs = np.abs(cdf - expcdf)\nD0 = absdiffs.max()\n\nD0\n\n0.07194182492411011\n\n\nWe can compare our value of \\(D\\) with the value obtained from scipy.stats.kstest, which takes as arguments the empirical dataset and a callable representing the CDF of the expected distribution, and returns the D-statistic as well as the p-value associated with the computed D-statistic (note that critical values depend on the number of observations). The manually computed result is given by D0, the result returned from scipy.stats.kstest by D1:\n\n\ndat = np.sort(dat)\ncdf = np.arange(1, dat.size + 1) / dat.size\ndat_mean = dat.mean()\ndat_std = dat.std(ddof=1)\n\n# Parameterized expected normal distribution.\nexpnorm = stats.norm(loc=dat_mean, scale=dat_std)\nexpcdf  = expnorm.cdf(dat)\nabsdiffs = np.abs(cdf - expcdf)\nD0 = absdiffs.max()\nD1, p1 = stats.kstest(dat, expnorm.cdf)\n\nprint(f\"Our D         : {D0:.8}\")\nprint(f\"Scipy kstest D: {D1:.8}\")\nprint(f\"kstest p-value: {p1:.8}\")\n\nOur D         : 0.071941825\nScipy kstest D: 0.071941825\nkstest p-value: 0.92828027\n\n\nThe p-value (the second element of the 2-tuple returned by scipy.stats.kstest) is 0.9283. How should this result be interpreted?\nFor the one-sample Kolmogorov-Smirnov test, the null hypothesis is that the distributions are the same. Thus, the lower the p-value the greater the statistical evidence you have to reject the null hypothesis and conclude the distributions are different. The test only lets you speak of your confidence that the distributions are different, not the same, since the test is designed to find the probability of Type I error. Therefore, if \\(D\\) is less than the critical value, we do not reject the null hypothesis (corresponds to a large p-value). If \\(D\\) is greater than the critical value, we reject the null hypothesis (corresponds to a small p-value).\nGiven our p-value of 0.9326, we do not have sufficient evidence to reject the null hypothesis that the distributions are the same.\n\n\nThe Anderson-Darling Test\nThe Anderson-Darling test tests the null hypothesis that a sample is drawn from a population that follows a particular distribution. It makes use of the fact that when given a hypothesized underlying distribution and assuming the data is a sample from this distribution, the CDF of the data can be assumed to follow a uniform distribution. The statistic itself can be expressed as:\n\\[\n\\begin{align*}\nA^{2} &= -n - S, \\hspace{2mm} \\text{where} \\\\\nS &= \\sum_{i=1}^{n} \\frac{2i-1}{n} \\Big[Ln(F(y_{i})) + Ln(1 - F(y_{n+1-i})) \\Big]\n\\end{align*}\n\\]\nThe function scipy.stats.anderson takes as arguments the empirical dataset and a distribution to test against (one of “norm”, “expon”, “logistic”, “gumbel”, “gumbel_l” or “gumbel_rexponential”), and returns the Anderson-Darling test statistic, the critical values for the specified distribution and the significance levels associated with the critical values. For example, to test whether our dataset follows a normal distribution, we run the following:\n\n\n# Perform Anderson-Darling test.\nA, crit, sig = stats.anderson(dat, dist=\"norm\")\n\nprint(f\"A   : {A}\")\nprint(f\"crit: {crit}\")\nprint(f\"sig : {sig}\")\n\nA   : 0.22442637404651578\ncrit: [0.54  0.615 0.738 0.861 1.024]\nsig : [15.  10.   5.   2.5  1. ]\n\n\nAccording to the scipy.stats.anderson documentation, if the returned statistic is larger than the critical values, then for the corresponding significance level the null hypothesis (that the data come from the chosen distribution) can be rejected. Since our statistic 0.224426 is smaller than all critical values, we do not have sufficient evidence to reject the null hypothesis that the data come from a normal distribution.\nA table of Anderson-Darling critical values can be found here.\n\n\nShapiro-Wilk Test\nThe Shapiro-Wilk test null hypothesis is that the sample is drawn from a normally distributed population. The function scipy.stats.shapiro takes the empirical dataset as it’s sole argument, and similar to scipy.stats.kstest returns a 2-tuple containing the test statistic and p-value.\n\n\n# Perform Shapiro-Wilk test.\nW, p = stats.shapiro(dat)\n\nprint(f\"W: {W}\")\nprint(f\"p: {p}\")\n\nW: 0.9804516434669495\np: 0.5324737429618835\n\n\nIf the p-value is less than the chosen alpha level, then the null hypothesis is rejected, and there is evidence that the data tested are not normally distributed. If the p-value is greater than the chosen alpha level, then the null hypothesis that the data came from a normally distributed population can not be rejected (e.g., for an alpha level of 0.05, a data set with a p-value of 0.05 rejects the null hypothesis that the data are from a normally distributed population). Our p-value is 0.532, so we cannot reject the null hypothesis.\nA table of Shapiro-Wilk critical values can be downloaded here."
  },
  {
    "objectID": "posts/getting-started-with-folium/getting-started-with-folium.html",
    "href": "posts/getting-started-with-folium/getting-started-with-folium.html",
    "title": "Introduction to Folium",
    "section": "",
    "text": "Learning geospatial data science is crucial in today’s data-driven world for several reasons. Geospatial data science enables individuals to understand and analyze complex spatial phenomena, including natural disasters, urbanization, climate change, and environmental degradation. By gaining familiarity with geospatial analysis techniques, individuals can gain insights into spatial patterns, relationships and processes, which is essential for making informed decisions.\nAs the availability and complexity of geospatial data continue to grow with advancements in technology and data collection methods, the demand for skilled geospatial data scientists is expected to rise. Therefore, investing in learning geospatial data science equips individuals with valuable skills that are not only relevant today but also increasingly essential for future career success."
  },
  {
    "objectID": "posts/getting-started-with-folium/getting-started-with-folium.html#folium",
    "href": "posts/getting-started-with-folium/getting-started-with-folium.html#folium",
    "title": "Introduction to Folium",
    "section": "Folium",
    "text": "Folium\nFolium is a Python library used for visualizing geospatial data interactively on web maps. Leveraging the capabilities of Leaflet.js, Folium allows users to create maps directly within Python code, making it an accessible and powerful tool for geospatial visualization and analysis.\nWith Folium, users can create various types of interactive maps, including point maps, choropleth maps, heatmaps, and vector overlays, by simply specifying geographic coordinates and map styling options. The library provides intuitive APIs for customizing map features such as markers, popups, tooltips, legends, and map layers, enabling users to create visually appealing and informative maps with ease.\nFolium integrates with other popular Python libraries such as Pandas and Matplotlib, allowing users to visualize geospatial data stored in DataFrame objects or plot data directly onto Folium maps. It also supports various tile providers and basemaps, enabling users to choose from a wide range of map styles and sources.\n\nCreating Interactive Maps in Folium\nCreating maps with folium is straightforward. We simply pass the latitude and longitude of the point of interest (POI) and specify a zoom level. We can then drop a marker on the point of interest, and interact with the map however we’d like.\nWe can get the latitude and longitude for a given POI by performing a google search. Latitude ranges from -90 to 90 degrees, longitude from -180 to 180 degrees. The latitude and longitude for the DMACC Ankeny campus is (41.5996, -93.6276), which is (latitude, longitude). Note that for US coordinates, the longitude will always be negative. An illustration is provided below:\n\n\n\nimg01\n\n\n\nTo illustrate, let’s render a map over the park I used to play at as a child (Durkin Park on the southwest side of Chicago). Note that zoom level provides more detail as the number gets larger. A zoom level of 4 would show the entire US; a zoom level of 17 would render roughly a city block:\n\n\nimport folium\n\n# Latitude and longitude for Durkin Park, 84th & Kolin Ave, Chicago IL. \nlat = 41.739\nlon = -87.729\nzoom = 18\n\nm = folium.Map(location=[lat, lon], zoom_start=zoom)\nfolium.Marker(location=[lat, lon]).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nA few things to note about the code used to render the map:\n\nWe start by importing the folium library.\nThe lat/lon for Durkin Park was obtained by a simple google search.\nI used a level 18 zoom but this is not necessary since the map is dynamic and can be resized.\nTo add the marker to the map, we call .add_to(m).\nWe included m by itself in the last line of the cell in order for the map to render. Without doing this, the map would not display.\n\nWe can change the color of the marker by passing an additional argument into folium.Marker. I’ll place a second marker in another park I used to visit when I was younger, Scottsdale Park. I’ll make this second marker red.\n\n\n# Durkin Park coordinates.\nlat0 = 41.739\nlon0 = -87.729\n\n# Scottsdale Park coordinates. \nlat1 = 41.7416\nlon1 = -87.7356\n\n# Center map at midway point between parks.\nmid_lat = (lat0 + lat1) / 2\nmid_lon = (lon0 + lon1) / 2\n\n# Specify zoom level. \nzoom = 16\n\n# Initialize map.\nm = folium.Map(location=[mid_lat, mid_lon], zoom_start=zoom)\n\n# Add Durkin Park marker.\nfolium.Marker(\n    location=[lat0, lon0],\n    popup=\"Durkin Park\",\n    ).add_to(m)\n\n# Add Scottsdale Park marker.\nfolium.Marker(\n    location=[lat1, lon1],\n    popup=\"Scottsdale Park\",\n    icon=folium.Icon(color=\"red\")\n    ).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nNotice that the popup argument was supplied to folium.Marker. Now when we click on the markers, whatever text we supply to popup will be shown on the map.\nWe can connect the markers in the map by using folium.PolyLine. We pass it a list of lat/lon pairs, and it draws a line connecting the points. Let’s connect the two parks with a green line:\n\n\n# Durkin Park coordinates.\nlat0 = 41.739\nlon0 = -87.729\n\n# Scottsdale Park coordinates. \nlat1 = 41.7416\nlon1 = -87.7356\n\n# Center map at midway point between parks.\nmid_lat = (lat0 + lat1) / 2\nmid_lon = (lon0 + lon1) / 2\n\n# Specify zoom level. \nzoom = 16\n\n# Initialize map.\nm = folium.Map(location=[mid_lat, mid_lon], zoom_start=zoom)\n\n# Add Durkin Park marker.\nfolium.Marker(\n    location=[lat0, lon0],\n    popup=\"Durkin Park\",\n    ).add_to(m)\n\n# Add Scottsdale Park marker.\nfolium.Marker(\n    location=[lat1, lon1],\n    popup=\"Scottsdale Park\",\n    icon=folium.Icon(color=\"red\")\n    ).add_to(m)\n\n# Connect parks with green line. \npoints = [(lat0, lon0), (lat1, lon1)]\nfolium.PolyLine(points, color=\"green\").add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nOne final point: We can replace the standard markers with circle markers by using folium.CircleMarker. radius controls the size of the markers and color/fill_color set the color of the marker:\n\n\nm = folium.Map(location=[mid_lat, mid_lon], zoom_start=zoom)\n\n# Add Durkin Park circle marker.\nfolium.CircleMarker(\n    location=[lat0, lon0], \n    radius=7, \n    popup=\"Durkin Park\",\n    color=\"red\", \n    fill_color=\"red\", \n    fill=True,\n    fill_opacity=1\n    ).add_to(m)\n\n# Add Scottsdale Park marker.\nfolium.CircleMarker(\n    location=[lat1, lon1], \n    radius=7, \n    popup=\"Scottsdale Park\",\n    color=\"red\", \n    fill_color=\"red\", \n    fill=True,\n    fill_opacity=1\n    ).add_to(m)\n\n\n# Connect parks with green line. \npoints = [(lat0, lon0), (lat1, lon1)]\nfolium.PolyLine(points, color=\"green\").add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nThe International Space Station (ISS) is a collaborative effort among multiple nations, serving as a hub for scientific research and international cooperation in space exploration. The ISS orbits the Earth at an astonishing speed of approximately 17,500 miles per hour, completing an orbit around the planet approximately every 90 minutes.\nThe coords list in the next cell represents the position as latitude-longitude pairs of the ISS sampled every minute for 20 minutes. We can render each of the 20 points as red circle markers connected by a red dashed line. Note that it is not necessary to call folium.CircleMarker 20 times: Use a for loop to iterate over the coords list.\n\n\ncoords = [\n    (50.4183, -35.337),\n    (49.3934, -29.7562),\n    (48.0881, -24.4462),\n    (46.5282, -19.4374),\n    (44.7411, -14.743),\n    (42.7364, -10.3267),\n    (40.5727, -6.2481),\n    (38.2576, -2.4505),\n    (35.8123, 1.0896),\n    (33.2554, 4.3975),\n    (30.6031, 7.4986),\n    (27.8697, 10.4178),\n    (25.0674, 13.1786),\n    (22.197, 15.8122), \n    (19.2887, 18.3195),\n    (16.3407, 20.7295),\n    (13.3611, 23.059), \n    (10.3562,  25.325),\n    (7.3323, 27.5427), \n    (4.2953, 29.7267)\n    ]\n\n\nlats, lons = zip(*coords)\nmid_lat = sum(lats) / len(lats)\nmid_lon = sum(lons) / len(lons)\n\nm = folium.Map(location=[mid_lat, mid_lon], zoom_start=4)\n\nfor lat, lon in coords:\n\n    folium.CircleMarker(\n        location=[lat, lon], \n        radius=5, \n        color=\"red\", \n        fill_color=\"red\", \n        fill=True,\n        fill_opacity=1\n        ).add_to(m)\n\n\n# Connect coords with red dashed line.\nfolium.PolyLine(coords, color=\"red\", dash_array=\"5\").add_to(m)\n\nm\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "posts/gcn-prop-model/gcn-prop-model.html",
    "href": "posts/gcn-prop-model/gcn-prop-model.html",
    "title": "Understanding the Graph Convolutional Network Propogation Model",
    "section": "",
    "text": "Convolutional Neural networks (CNNs) have been found to be very effective at tasks such as facial recognition, video analysis, anomaly detections and semantic parsing. An image can be considered a graph with a very regular structure (i.e., pixels are considered nodes, with edges connecting adjacent nodes). A natural idea is to extend convolutions to general graphs for tasks such as graph, node or edge classification. However, it is not immediately clear how one would go about extending the concept of convolutions in the context of CNNs to graphs.\nIn the Kipf and Welling paper, Semi-Supervised Classification with Graph Convolutional Networks, the authors propose a method to approximate the spectral graph convolution via truncated Chebyshev polynomials, resulting in an efficient and scalable method to train graph neural networks. In this post, we walkthrough the graph convolutional network (GCN) propagation model, which we also implement in pytorch geometric. Thomas Kipf’s original PyTorch implementation is available here."
  },
  {
    "objectID": "posts/gcn-prop-model/gcn-prop-model.html#derivation-of-propagation-model",
    "href": "posts/gcn-prop-model/gcn-prop-model.html#derivation-of-propagation-model",
    "title": "Understanding the Graph Convolutional Network Propogation Model",
    "section": "Derivation of Propagation Model",
    "text": "Derivation of Propagation Model\nThe authors begin with an expression to perform spectral convolutions on graphs:\n\\[\ng_{\\theta} \\star x = U g_{\\theta} U^{T} x,\n\\]\nwhere \\(U\\) is the matrix of eigenvectors of the normalized Laplacian \\(L = I_{n} - D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}\\), and \\(U^{T}x\\) represents the graph Fourier transform of \\(x\\). However, this as expensive operation, since the spectral decomposition is \\(\\mathcal{O}(n^{3})\\) in practice. In addition, it isn’t immediately clear how one would go about incorporating this operation within the context of forward/back-propagation.\nCiting a paper by Hammond et. al, they propose an approximation to \\(g_{\\theta} \\star x\\) via a truncated expansion in terms of Chebyshev polynomials \\(T_{k}(x)\\) up to \\(K^{th}\\) order:\n\\[\ng_{\\theta} \\star x \\approx \\sum_{k=0}^{K} \\theta^{'}_{k} T_{k}(\\tilde{L})x \\hspace{.75em}(*),\n\\]\nwhere \\(\\tilde{L} = \\frac{2}{\\lambda_{\\mathrm{max}}}L - I_{N}\\) and \\(\\theta^{'}\\) is a vector of Chebyshev coefficients. Note that for the Chebyshev polynomial expansion, \\(T_{0}(x) = 1\\), \\(T_{1}(x) = x\\) and all other terms are recursively defined as \\(T_{k}(x) = 2xT_{k-1}(x) - T_{k-2}(x)\\).\nBy leveraging \\({(*)}\\) to approximate graph convolutions, the runtime complexity becomes linear in terms of the number of edges (\\(\\mathcal{O}(|\\mathcal{E}|)\\) instead of cubic in the number of nodes as is the case for the full spectral graph convolution.\nFrom this point, the authors make three simplifications/assumptions which result in the propagation model introduced earlier:\n\\[\nH^{(l+1)} = \\sigma(\\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)}).\n\\]\nThe first two simplifications are \\(K=1\\) and \\(\\lambda_{\\mathrm{max}} = 2\\). Expanding \\(({*})\\) with \\(K=1\\) and \\(\\lambda_{\\mathrm{max}} = 2\\) results in:\n\\[\ng_{\\theta} \\star x \\approx \\theta^{'}_{0}x + \\theta^{'}_{1}(L - I_{N})x = \\theta^{'}_{0}x - \\theta^{'}_{1}D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}x\n\\]\nThe third simplification sets \\(\\theta = \\theta^{'}_{0} = -\\theta^{'}_{1}\\). This constrains the number of parameters to address overfitting and to minimize the number of multiplications per layer. Doing so results in:\n\\[\ng_{\\theta} \\star x \\approx \\theta \\big(I_{N} + D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}} \\big)x.\n\\]\nTo address the issue of exploding/vanishing gradients during training they apply a renormalization trick, essentially swapping \\(A\\) and \\(D\\) with \\(\\tilde{A}\\) and \\(\\tilde{D}\\). This ensures eigenvalues are the the range [-1, 1]. We can show this is the case for our example using the adjacency matrix associated with our 5 node graph:\n\n\nimport numpy as np\nimport networkx as nx\n\nnp.set_printoptions(suppress=True, precision=5)\n\n\nG = nx.Graph()\nedges = [(1, 2), (1, 3), (1, 4), (1, 5), (2, 5)]\n\nG.add_edges_from(edges)\nA = nx.adjacency_matrix(G).todense()\nI = np.identity(5)\nD = np.diag(np.sum(A, axis=1))\n\n# Check range of eigenvalues without renormalization.\nDp = np.linalg.inv(np.power(D, .5))\nevals0, _ = np.linalg.eig(I + Dp @ A @ Dp)\n\n# Check range of eigenvalues after renormalization trick. \nAtilde = A + I\nDtilde = np.diag(np.sum(Atilde, axis=1))\nDtildep = np.linalg.inv(np.power(Dtilde, .5))\nevals1, _ = np.linalg.eig(Dtildep @ Atilde @ Dtildep)\n\nprint(f\"\\neigenvalues without renormalization: {evals0}\")\nprint(f\"eigenvalues with renormalization   : {evals1}\")\n\n\neigenvalues without renormalization: [0.19098 2.      1.30902 0.5     1.     ]\neigenvalues with renormalization   : [-0.22526  1.       0.59192  0.5      0.     ]\n\n\nBy replacing \\(\\big(I_{N} + D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}} \\big)\\) with \\(\\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}}\\), we end up with:\n\\[\nZ = \\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}X\\Theta,\n\\]\nwhere \\(\\Theta\\) is a c-by-f matrix of filter parameters and \\(Z\\) is the n-by-f convolved signal matrix.\nThe final propagation model has nothing to do with the spectral decomposition: No eigenvectors/eigenvalues are actually computed. The approach was motivated by spectral convolutions on graphs, but is not a spectral method itself.\n\nPyTorch Geometric GCN Implementation\nIn what follows, we demonstrate how to set up a 2-layer GCN for node classification using PyTorch Geometric and the Cora citation network.\n\n\nimport random\n\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport numpy as np\nimport pandas as pd\n\nimport torch\nfrom torch_geometric.datasets import Planetoid\nimport torch_geometric.transforms as T\nfrom torch_geometric.utils import to_networkx\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndataset = Planetoid(root='/tmp/Cora', name='Cora')\nsplit = T.RandomNodeSplit(num_val=0.15, num_test=0.10)\ngraph = split(dataset[0])\n\n\n\n\nDistribution of classes in Cora\nThe Cora dataset is a citation network, where each node represents a paper belonging to one of the following 7 classes. Our goal is to predict the labels assigned to each node in the network:\n\n\ndclasses = {\n    0: \"Theory\",\n    1: \"Reinforcement_Learning\", \n    2: \"Genetic_Algorithms\",\n    3: \"Neural_Networks\",\n    4: \"Probabilistic_Methods\",\n    5: \"Case_Based\",\n    6: \"Rule_Learning\"\n    }\n\ndf = (\n    pd.DataFrame(graph.y.tolist()) \n    .groupby(0, as_index=False).size()\n    .rename({0: \"id\", \"size\": \"n\"}, axis=1)\n    )\n\ndf[\"desc\"] = df[\"id\"].map(dclasses)\ndf[\"prop\"] = df[\"n\"] / df[\"n\"].sum()\ndf[[\"id\", \"desc\", \"n\", \"prop\"]].head(7)\n\n\n\n\n\n\n\n\nid\ndesc\nn\nprop\n\n\n\n\n0\n0\nTheory\n351\n0.129616\n\n\n1\n1\nReinforcement_Learning\n217\n0.080133\n\n\n2\n2\nGenetic_Algorithms\n418\n0.154357\n\n\n3\n3\nNeural_Networks\n818\n0.302068\n\n\n4\n4\nProbabilistic_Methods\n426\n0.157312\n\n\n5\n5\nCase_Based\n298\n0.110044\n\n\n6\n6\nRule_Learning\n180\n0.066470\n\n\n\n\n\n\n\n\n\nGCN Model\nNext we implement GCN using PyTorch Geometric. The original PyTorch implementation is available here.\n\n\nimport torch.nn as nn\nfrom torch_geometric.nn import GCNConv\nimport torch.nn.functional as F\n\n\nclass GCN(nn.Module):\n    def __init__(self, num_features, num_classes):\n        super().__init__()\n        self.conv1 = GCNConv(num_features, 64)\n        self.conv2 = GCNConv(64, num_classes)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        output = self.conv2(x, edge_index)\n        return(output)\n\n\ndef train_classifier(model, graph, optimizer, criterion, n_epochs=100):\n    \"\"\"\n    Training loop.\n    \"\"\"\n    tm, vm = graph.train_mask, graph.val_mask\n    for epoch in range(n_epochs):\n        model.train()\n        optimizer.zero_grad()\n        output = model(graph)\n        loss = criterion(output[tm], graph.y[tm])\n        loss.backward()\n        optimizer.step()\n\n        # Validate. \n        model.eval()\n        pred = model(graph).argmax(dim=1)\n        correct = (pred[vm] == graph.y[vm]).sum()\n        acc = correct / vm.sum()\n\n        if epoch % 10 == 0:\n            print(f\"Epoch {epoch}: train loss={loss:.3f}, val. acc={acc:.3f}.\")\n\n    return(model)\n\nTraining our network:\n\n\n# Capture activations from last hidden layer to re-create plot\n# from paper.\nactivations = {}\ndef get_activation(name):\n    def hook(gcn, input, output):\n        activations[name] = output.detach()\n    return(hook)\n\n# Put graph on GPU if available.\ngraph = graph.to(device)\ngcn = GCN(num_features=1433, num_classes=7).to(device)\ngcn.conv2.register_forward_hook(get_activation(\"conv2\"))\noptimizer = torch.optim.Adam(gcn.parameters(), lr=0.01, weight_decay=1e-4)\ncriterion = nn.CrossEntropyLoss()\ngcn = train_classifier(gcn, graph, optimizer, criterion)\n\n# Compute test accuracy.\ngcn.eval()\npred = gcn(graph).argmax(dim=1)\ncorrect = (pred[graph.test_mask] == graph.y[graph.test_mask]).sum()\ntest_acc = correct / graph.test_mask.sum()\nprint(f\"\\nTest accuracy: {test_acc:.3f}\\n\")\n\nEpoch 0: train loss=1.932, val. acc=0.303.\nEpoch 10: train loss=0.377, val. acc=0.862.\nEpoch 20: train loss=0.181, val. acc=0.867.\nEpoch 30: train loss=0.119, val. acc=0.857.\nEpoch 40: train loss=0.088, val. acc=0.852.\nEpoch 50: train loss=0.069, val. acc=0.845.\nEpoch 60: train loss=0.058, val. acc=0.847.\nEpoch 70: train loss=0.050, val. acc=0.850.\nEpoch 80: train loss=0.044, val. acc=0.855.\nEpoch 90: train loss=0.040, val. acc=0.855.\n\nTest accuracy: 0.893\n\n\n\n\n\nDisplay hidden layer activations with t-SNE\nFinally, we can display a lower dimensional representation of the final hidden layer node embeddings using t-SNE. The resulting plot looks similar to figure 1b from the original paper, but experimenting with the t-SNE perplexity parameter will probably get you even closer:\n\nfrom sklearn.manifold import TSNE\n\nX = activations[\"conv2\"].cpu().numpy()\nXemb = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=30).fit_transform(X)\nprint(f\"Xemb.shape: {Xemb.shape}\")\n\nfig, ax = plt.subplots(1, 1, tight_layout=True)\nax.scatter(Xemb[:,0], Xemb[:,1], c=pred.cpu(), alpha=.75)\nax.axis(\"off\")\nplt.show();\n\nXemb.shape: (2708, 2)\n\n\n\n\n\nLower dimensional representation of the final hidden layer using t-SNE"
  },
  {
    "objectID": "posts/em-from-scratch-r/em-from-scratch-r.html",
    "href": "posts/em-from-scratch-r/em-from-scratch-r.html",
    "title": "Expectation Maximization from Scratch in R",
    "section": "",
    "text": "Lets create a dataset which appears to be a mixture of two separate distributions. Such a dataset can be created using the following code:\n# Creating simulated bimodal data observations.\noptions(scipen=9999)\nset.seed(516)\n\nx = c(\n    rnorm(31, mean=75, sd=17.5) + rnorm(31, mean=0, sd=5.5), \n    rnorm(23, mean=175, sd=25) + rnorm(23, mean=0, sd=10)\n    )\nRunning plot(density(x)) generates a plot of the kernel density estimate:\n\nOur goal is to fit a 2-component Gaussian Mixture Model (GMM) to a dataset consisting of \\(N\\) observations using Expectation Maximization with parameters \\(\\mu_{k}, \\sigma_{k}, \\pi_{k}\\) where \\(k \\in [1, 2]\\).\nThe probability density of a K-component GMM is given by\n\\[\nf(\\boldsymbol{x}) = \\sum_{k=1}^{K}\\pi_{k} \\cdot \\mathcal{N}(\\boldsymbol{x}| \\mu_{k}, \\sigma^2_{k}),\n\\]\nwhere in the 2 component case, \\(K=2\\) (however, expressions will be in terms of \\(K\\) for generality). From the density, an expression for the log-likelihood immediately follows:\n\\[\n\\mathrm{Ln}\\hspace{.10em}f(X|\\pi, \\mu, \\sigma) = \\sum_{j=1}^{N}\\mathrm{Ln}\\hspace{.10em} \\Big[\\sum_{k=1}^{K} \\pi_{k} \\cdot\\mathcal{N}(\\boldsymbol{x}| \\mu_k, \\sigma^2_k)\\Big].\n\\]\nIn the log-likelihood, \\(\\mu_{k}\\), \\(\\sigma_{k}\\) and \\(\\pi_{k}\\) represent the mean, standard deviation and mixture coefficient respectively for component \\(k\\). The inner summation (indexed by \\(k\\)) iterates over mixture components while the outer summation (indexed by \\(j\\)) iterates over each observation in the data. Note that for each component \\(k\\), \\(0 &lt;= \\pi_{k} &lt;= 1\\), and for a \\(K\\)-component GMM, vectors \\(\\boldsymbol{\\mu}\\), \\(\\boldsymbol{\\sigma}\\) and \\(\\boldsymbol{\\pi}\\) will each have length \\(K\\).\nTaking the derivative of the log-likelihood w.r.t. \\(\\boldsymbol{\\mu}\\), \\(\\boldsymbol{\\sigma}\\) and \\(\\boldsymbol{\\pi}\\), setting equal to zero and re-arranging yields update expressions for the parameters of interest:\nFor the mean of component \\(k\\), \\(\\mu^{'}_{k}\\):\n\\[\n\\mu^{'}_{k} = \\sum_{j=1}^{N} \\frac{x_{j} \\cdot \\pi_{k}  \\cdot \\mathcal{N}(x_{j}| \\mu_{k}, \\sigma^2_{k})} {\\pi_{k} \\cdot \\mathcal{N}(x_{j}| \\mu_{k}, \\sigma^2_{k})}\n\\]\nFor the standard deviation of component \\(k\\), \\(\\sigma^{'}_{k}\\):\n\\[\n\\sigma^{'}_{k} =  \\sqrt{\\sum_{j=1}^{N} \\frac{(x_{j} - \\mu^{'}_{k})^{2} \\cdot \\pi_{k} \\cdot \\mathcal{N}(x_{j}| \\mu_{k}, \\sigma^2_r)} {\\pi_{k} \\cdot \\mathcal{N}(x_{j}| \\mu_k, \\sigma^2_k)}}\n\\]\nFor the mixture probability of component \\(k\\), \\(\\pi_{k}\\), first note that the posterior probability of a single observation \\(x\\) originating from component \\(k=z\\) is given by\n\\[\nf(z|x) = \\frac{f(k=z) \\cdot f(x|k=z)}{\\sum f(k) \\cdot f(x|k)} = \\frac{\\pi_{z} \\cdot \\mathcal{N}(x| \\mu_{z}, \\sigma^{2}_{z})}{\\sum_{k=1}^{K} \\pi_{k} \\cdot \\mathcal{N}(x| \\mu_{k}, \\sigma^{2}_{k})}.\n\\]\n\\(\\pi^{'}_{k}\\) is updated by aggregating the probabilities for component \\(k\\) across all observations, then dividing by the total number of observations:\n\\[\n\\pi^{'}_{k} = \\frac{1}{N}\\sum_{j=1}^{N} f(k|x_{j}) = \\frac{1}{N}\\sum_{j=1}^{N} \\frac{\\pi_{k} \\cdot \\mathcal{N}(x_{j}| \\mu^{'}_k, \\sigma^{2*}_k)}{\\sum_{r=1}^K \\pi_{r} \\cdot \\mathcal{N}(x_{j}| \\mu^{'}_{r}, \\sigma^{2'}_{r})}\n\\]\nWe can summarize Expectation Maximization as follows:\n\n(E-step): Using current parameter values \\((\\mu_{k}, \\sigma_{k}, \\pi_{k})\\), estimate the posterior probabilities of each mixture component \\(\\pi^{'}_{k}\\).\n(M-step): Using updated posterior probabilities, re-estimate component means and standard deviations \\(\\mu^{'}_{k}\\), \\(\\sigma^{'}_{k}\\).\n\n\nImplementation\nThe code that follows represents an implementation of the Expectation Maximization algorithm for a two-component Gaussian Mixture Model. The initial estimates of \\(\\mu\\), \\(\\sigma\\) and \\(\\pi\\) are obtained using k-means. We create a number of data structures to which parameter estimates and log-likelihood are saved at each iteration. Finally, parameter estimates are saved to the paramsDF0 data.table for comparison with estimates from the mixtools library:\n# Implementation of the Expectation Maximization algorithm. \nlibrary(\"data.table\")\nlibrary(\"foreach\")\noptions(scipen=9999)\nset.seed(516)\n\n# Generate bimodal data to fit via GMM. \nx = c(\n    rnorm(31, mean=75, sd=17.5) + rnorm(31, mean=0, sd=5.5),\n    rnorm(23, mean=175, sd=25) + rnorm(23, mean=0, sd=10)\n    )\n\nn = length(x)  # Number of observations.\nr = 2          # Number of components in Gaussian Mixture Model.\nmaxIter = 500  # Maximum number of EM iterations.\ntol = 1e-8     # Log-likelihood exceedance threshold.\n\n# Use k-means to obtain initial parameter estimates.\nkm = kmeans(x, r)\n\n# Create data.table with original observations and cluster assignments. \nkmeansDF = setorder(\n    data.table(x=x, group=km$cluster, stringsAsFactors=FALSE),\n    \"group\"\n    )\n\n# Bind reference to initial values for rmu, rsd and rpi.\nrmu = kmeansDF[,.(x=mean(x)), by=\"group\"]$x\nrsd = kmeansDF[,.(x=sd(x)), by=\"group\"]$x\nrpi = kmeansDF[,.N, by=\"group\"]$N / n\n\n# Collect log-likelihood and updated parameter estimates at each iteration.\nllEM = rep(0, maxIter)\nmuEM = matrix(rep(0, r * maxIter), ncol=r)\nsdEM = matrix(rep(0, r * maxIter), ncol=r)\npiEM = matrix(rep(0, r * maxIter), ncol=r)\n\n# Initialize muEM, sdEM and piEM.\nmuEM[1,] = rmu\nsdEM[1,] = rsd\npiEM[1,] = rpi\n\n# Iteration tracker.\njj = 2\n\n# Expectation Maximization iteration.\n# All m-prefixed variables have dimension (n x r).\n# All r-prefixed variables have length r.\nwhile (jj&lt;=maxIter) {\n\n    # Expectation step. \n    mcomp = foreach(ii=1:r, .combine=\"cbind\") %do% {\n        rpi[ii] * dnorm(x, rmu[ii], rsd[ii])\n    }\n    \n    # Determine likelihood contribution for each observation.\n    # mcompSum is a vector of length n. \n    mcompSum = apply(mcomp, MARGIN=1, sum, na.rm=TRUE)\n    \n    # Compute mixture probabilities for each observation. Summing across\n    # columns, each row will equal 1.0.\n    mpi = foreach(ii=1:ncol(mcomp), .combine=\"cbind\") %do% {\n        mcomp[,ii] / mcompSum\n    }\n        \n    # Maximization step.\n    rmu = as.numeric((t(x) %*% mpi) / apply(mpi, MARGIN=2, sum))\n    \n    rsd = foreach(ii=1:length(rmu), .combine=\"c\") %do% {\n        denom = sum(mpi[,ii][is.finite(mpi[,ii])], na.rm=TRUE)\n        numer = mpi[,ii] * (x - rmu[ii])^2\n        numer = sum(numer[is.finite(numer)], na.rm=TRUE)\n        sqrt(numer / denom)\n    }\n    \n    rpi = foreach(ii=1:ncol(mpi), .combine=\"c\") %do% {\n        sum(mpi[,ii][is.finite(mpi[,ii])], na.rm=TRUE) / n\n    }\n    \n    # Update llEM, muEM, sdEM and piEM.\n    llEM[jj] = sum(log(mcompSum)); muEM[jj,] = rmu; sdEM[jj,] = rsd; piEM[jj,] = rpi\n\n    message(\n        \"[\", jj, \"] ll=\", llEM[jj], \" (dll=\", abs(llEM[jj] - llEM[jj-1]), \").\"\n        )\n    \n    if (abs(llEM[jj] - llEM[jj-1])&lt;tol) {\n        break\n    }\n    \n    jj = jj + 1\n}\n\n# Extract last populated row from muEM, sdEM and piEM.\nparamsDF0 = rbindlist(\n    list(\n        data.table(\n            parameter=\"mean\", w=seq(length(muEM[jj,])), value=muEM[jj,],\n            stringsAsFactors=FALSE\n            ),\n         data.table(\n            parameter=\"sd\", w=seq(length(sdEM[jj,])), value=sdEM[jj,],\n            stringsAsFactors=FALSE\n            ),\n         data.table(\n            parameter=\"w\", w=seq(length(piEM[jj,])), value=piEM[jj,],\n            stringsAsFactors=FALSE\n            )\n        ), fill=TRUE\n    )\n\nparamsDF0 = dcast(\n    paramsDF0, parameter ~ paste0(\"w_\", w), fun.aggregate=sum, \n    value.var=\"value\", fill=NA_real_\n    )\nThe code generates a status message at each iteration indicating the current log-likelihood estimate (ll) as well as the change in log-likelihood from the previous estimate (dll). Results are given below.\n[2] ll=-276.872839784171 (dll=276.872839784171).\n[3] ll=-276.8390507688 (dll=0.0337890153707576).\n[4] ll=-276.83590305353 (dll=0.0031477152705861).\n[5] ll=-276.835432239165 (dll=0.000470814365030492).\n[6] ll=-276.835356881754 (dll=0.0000753574105374355).\n[7] ll=-276.835344544138 (dll=0.0000123376157148414).\n[8] ll=-276.835342506099 (dll=0.00000203803961085214).\n[9] ll=-276.835342168222 (dll=0.000000337876826961292).\n[10] ll=-276.835342112125 (dll=0.0000000560970079277467).\n[11] ll=-276.835342102806 (dll=0.00000000931919430513517).\nparamsDF0 reflects the final estimates of \\(\\mu_{i}\\), \\(\\sigma_{i}\\) and \\(\\pi_{i}\\):\n&gt; paramsDF0\n   parameter         w_1        w_2\n1:      mean 181.2244438 81.7632674\n2:        sd  30.6704024 16.0083545\n3:         w   0.4325322  0.5674678\n\n\n\nComparison with mixtools Library\nmixtools is a third-party R library used to estimate Gaussian Mixture Models. Using out data and initial estimates of \\(\\mu\\), \\(\\sigma\\) and \\(\\pi\\), let’s compare the results of our implementation vs. the normalmixEM function provided by mixtools:\nlibrary(\"data.table\")\nlibrary(\"mixtools\")\n\ngmm = normalmixEM(\n    x, k=2, lambda=piEM[1,], mu=muEM[1,],sigma=sdEM[1,]\n    )\n\n# Create data.table containing normalmixEM parameter estimates.\nparamsDF1 = data.table(\n    parameter=c(\"mu\", \"sigma\", \"pi\"), stringsAsFactors=FALSE\n  )\nfor (ii in 1:r) {\n    paramsDF1[[paste0(\"comp\", ii)]] = c(gmm$mu[ii], gmm$sigma[ii], gmm$lambda[ii])\n}\nLet’s compare paramsDF0 with paramsDF1:\n&gt; paramsDF0\n   parameter        w_1         w_2\n1:        mu 81.7632674 181.2244438\n2:     sigma 16.0083545  30.6704024\n3:        pi  0.5674678   0.4325322\n\n&gt; paramsDF1\n   parameter      comp1       comp2\n1:        mu 81.7632740 181.2244778\n2:     sigma 16.0083538  30.6704046\n3:        pi  0.5674678   0.4325322\nWe find the results to be almost identical.\nFinally, we can create a density plot to illustrate the adequacy of the GMM fit to the data:\n# Plot comparing empirical data with estimated mixture model.\nexhibitPath = \"C:/Users/i103455/Repos/Tutorials/Supporting/em2.png\"\npng(file=exhibitPath)\nhist(x, prob=TRUE, breaks=23, xlim=c(min(x), max(x), main=\"GMM Estimate via EM\")\nxx = seq(from=min(x), to=max(x), length.out=500)\nyy = rpi[1] * dnorm(xx, mean=rmu[1], sd=rsd[1]) + rpi[2] * dnorm(xx, mean=rmu[2], sd=rsd[2])\nlines(xx, yy, col=\"#E02C70\", lwd=2)\ndev.off()\nWhich results in:\n\nWe see that the model serves as a good representation of the underlying data.\n\n\nConclusion\nOne final note regarding Expectation Maximization estimates: This is taken from Christopher Bishop’s Pattern Recognition and Machine Learning, Chapter 9:\n\nA K-component mixture model will have a total of \\(K!\\) equivalent solutions corresponding to the \\(K!\\) ways of assigning \\(K\\) sets of parameters to \\(K\\) components. In other words, for any given point in the space of parameter values there will be a further \\(K! - 1\\) additional points all of which give rise to exactly the same distribution. This problem is known as identifiability.\n\nThis means that for certain datasets, the parameters estimates from our implementation may differ from those found via mixtools, but for the purposes of finding a good density model, the difference is irrelevant since any of the equivalent solutions is as good as any other. Just be sure to perform a visual adequacy assessment to ensure the differences in parameter estimates do indeed result in identical or nearly identical probability densities."
  },
  {
    "objectID": "posts/dynamic-product-r/dynamic-product-r.html",
    "href": "posts/dynamic-product-r/dynamic-product-r.html",
    "title": "Computing the Product of a Variable Number of Columns using data.table",
    "section": "",
    "text": "Lets say you have a data.table with rating factors as columns associated with different coverages. For a simple example, let’s assume a 2-coverage, 2-factor rating plan with a single BI factor bi_occupancy_factor and a single property factor prop_tiv_factor. Also assume factors have been attached to a sample cohort of policies based on the risk characteristics of the exposure. We construct the table below:\nThe resulting table looks like:\nOur goal is to create a new column which represents a combined property factor. With only a single property factor, this is trivial, since we can simply set the combined property factor to prop_tiv_factor:\nResulting in:\nAssume the decision has been made to incorporate a new property rating factor into the plan, prop_age_bld_factor. Our revised initial data.table becomes:\nCreating our combined property rating factor is still straightforward: This time, we simply set prop_factor to the product of prop_tiv_factor and prop_age_bld_factor for every observation in DF:\nWhich yields:\nNext the request is made to incorporate another property rating factor, this time prop_deductible_factor. Our dataset now becomes:\nWe could continue as before, updating prop_factor to include prop_deductible_factor in the product. However, this approach is not scalable, and each time you go into the code to make changes, you increase the likelihood of introducing errors. We need a more general solution to the problem, specifically a method which allows us to perform an operation on some logical grouping of columns, where the exact number (of columns) may not be known until the moment of execution."
  },
  {
    "objectID": "posts/dynamic-product-r/dynamic-product-r.html#using-reduce-and-like",
    "href": "posts/dynamic-product-r/dynamic-product-r.html#using-reduce-and-like",
    "title": "Computing the Product of a Variable Number of Columns using data.table",
    "section": "Using Reduce and %like%",
    "text": "Using Reduce and %like%\nThe Reduce function successively applies a function to the elements of an object from left to right or right to left, respectively. The simplest example is using Reduce in place of sum:\n&gt; Reduce(`+`, c(1, 2, 3, 4, 5))\n[15]\nWe can also replicate prod\n&gt; Reduce(`*`, c(1, 2, 3, 4, 5))\n[1] 120\nReduce is capable of working with more complex data structures as well.\nThe %like% function comes from data.table, and works similar to SQL’s LIKE operator. It is essentially shorthand for a regular expression pattern matching subroutine, which returns a TRUE/FALSE based on whether or not the target matches the pattern. To demonstrate, we identify which month names end in y:\n&gt; month.name %like% \"y$\"\n[1]  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\nTo return the actual month names ending in y, use the previous expression as a mask:\n&gt; month.name[month.name %like% \"y$\"]\n[1] \"January\"  \"February\" \"May\"      \"July\"\nNote that in regular expression parlance, $ means to match y only when it occurs at the end of the string. Similarly, ^ indicates the match must occur at the beginning of the string. In addition, .+ matches one or more characters, which can be letters, numbers, punctuation or whitespace. We can leverage Reduce and %like% within the context of data.table to take the product of a potentially variable number of factor columns. Using our latest version of DF with 3 prop_*_factor columns, we have:\nDF[,prop_factor:=\n    Reduce(`*`, .SD), .SDcols=names(DF)[names(DF) %like% \"^prop.+factor$\"]\n    ]\nTo break this down a bit, note that the value assigned to .SDcols is doing nothing more than filtering to only those columns starting with prop and ending with factor. We can see this by running the expression in isolation:\n&gt; names(DF)[names(DF) %like% \"^prop.+factor$\"\n[1] \"prop_tiv_factor\"        \"prop_age_bld_factor\"    \"prop_deductible_factor\"\n.SD is one of data.table’s special symbols, which serve as shortcuts for frequently used operations. In this particular case, .SD is a stand-in for the columns we’re interested in taking the product of, and .SDcols represents the names of the columns over which the operation is to be applied.\nIn data.table, := represents the “assignment-by-reference” operator. Notice that when a column is defined via :=, the update is made by reference, so it isn’t necessary to re-assign the column name to the table as would typically be necessary when working with standard data.frame objects.\nDoes our solution generalize to any number of columns? Let’s add a few more factors and test it out:\nDF = data.table(\n    policyno=paste0(\"0000\", 1:5), locationno=rep(1, 5), \n    prop_tiv_factor=c(1.112, 1.255, 1.3125, 1.075, 1.3125),\n    bi_occupancy_factor=c(1.015, 1.0675, 1.0925, 1.0675, 1.0475),\n    prop_age_bld_factor=c(1.173, 1.21235, 1.0935, 1.2815, 1.1115),\n    prop_deductible_factor=c(1.025, 1.025, 1.755, 1.025, 1.1665),\n    prop_nbr_stories_factor=c(1.015, 1.015, 1.015, 1.015, 1.1175),\n    prop_age_roof_factor=c(1.033, 1.0373, 1.3573, 1.033, 1.0373),\n    bi_protect_class_factor=c(1., 1., 1., 1.25, 1.25),\n    prop_construction_factor=c(1.0235, 1.0744, 1.1985, 1.0235, 1.0744),\n    stringsAsFactors=FALSE\n    )\nWhich gives us\npolicyno locationno prop_tiv_factor bi_occupancy_factor prop_age_bld_factor prop_deductible_factor prop_nbr_stories_factor\n1:    00001          1          1.1120              1.0150             1.17300                 1.0250                  1.0150\n2:    00002          1          1.2550              1.0675             1.21235                 1.0250                  1.0150\n3:    00003          1          1.3125              1.0925             1.09350                 1.7550                  1.0150\n4:    00004          1          1.0750              1.0675             1.28150                 1.0250                  1.0150\n5:    00005          1          1.3125              1.0475             1.11150                 1.1665                  1.1175\n   prop_age_roof_factor bi_protect_class_factor prop_construction_factor\n1:               1.0330                    1.00                   1.0235\n2:               1.0373                    1.00                   1.0744\n3:               1.3573                    1.00                   1.1985\n4:               1.0330                    1.25                   1.0235\n5:               1.0373                    1.25                   1.0744\nOur logic should return the product of prop_tiv_factor, prop_age_bld_factor, prop_deductible_factor, prop_nbr_stories_factor, prop_age_roof_factor and prop_construction_factor without modifying our earlier implementation. Let’s compare the results of our dynamic, general expression vs. explicitly specifying the column names to multiply:\n# Compare results from explicit and implicit column multiplication.\nDF[,prop_factor:=\n    Reduce(`*`, .SD), .SDcols=names(DF)[names(DF) %like% \"^prop.+factor$\"]\n    ]\nprop_factor1 = DF[,prop_factor]\n\n# Compute product specifying  prop-prefixed columns explicitly.\nprop_factor2 = DF$prop_tiv_factor DF$prop_age_bld_factor * DF$prop_deductible_factor * \nDF$prop_nbr_stories_factor * DF$prop_age_roof_factor * DF$prop_construction_factor\nComparing results gives\n&gt; prop_factor1\n[1] 1.434765 1.764136 4.158868 1.515323 2.119393\n&gt; prop_factor2\n[1] 1.434765 1.764136 4.158868 1.515323 2.119393"
  },
  {
    "objectID": "posts/distance-to-coastline/distance-to-coastline.html",
    "href": "posts/distance-to-coastline/distance-to-coastline.html",
    "title": "Determining Distance to Coastline for Policy Locations Using GeoPandas",
    "section": "",
    "text": "Knowing the distance to coastline for an exposure is crucial for insurance rating applications because it helps insurers assess the risk of hazards like hurricanes, storm surges, and flooding, which are much more prevalent in coastal areas. This information allows insurers to make informed decisions about pricing, underwriting and reinsurance. Properties closer to the coast are generally at higher risk, leading to higher premiums for these properties. Insurance rating plans may use distance to coastline directly as an explanatory variable, with factors inversely proportional to distance to coastline.\nThis article walks through how GeoPandas can be used to calculate distance to coastline for a collection of simulated latitude-longitude pairs in the Florida region, and how these exposure locations can be assigned to different risk levels based on the distance calculation.\n\n\nCoastal Shapefiles\nThe United States Census Bureau provides shapefiles for state, county and ZCTA boundaries as well as roads, rails an coastlines (see full list here). Shapefiles are a widely-used geospatial vector data format that store the geometric location and attribute information of geographic features, which can be represented as points, lines, or polygons.\nWe being by downloading the COASTLINE zip archive available on the Census Bureau’s FTP site. The COASTLINE shapefile is loaded into GeoPandas (the STATE shapefile is also loaded for later use). We limit our analysis to the continental United States and filter out the Great Lakes. Inspecting the first few records:\n\n\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\n\nnp.set_printoptions(suppress=True, precision=5)\npd.options.mode.chained_assignment = None\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\n\n\ncoastline_shp = \"tl_2024_us_coastline.zip\"\nus_shp = \"tl_2024_us_state.zip\"\n\n\n# Bounding box of lower 48 states. Remove Great Lakes.\nxmin, ymin, xmax, ymax = -125, 24.6, -65, 50\ncoast = gpd.read_file(coastline_shp)\ncoast = coast.cx[xmin:xmax, ymin:ymax]\ncoast = coast[coast.NAME!=\"Great Lakes\"].reset_index(drop=True)\n\n# State boundaries.\nstates = gpd.read_file(us_shp)[[\"NAME\", \"geometry\"]]\nstates = states.cx[xmin:xmax, ymin:ymax].reset_index(drop=True)\n\nprint(f\"coast.shape : {coast.shape}\")\nprint(f\"states.shape: {states.shape}\")\n\ncoast.head(10)\n\ncoast.shape : (2916, 3)\nstates.shape: (49, 2)\n\n\n\n\n\n\n\n\n\nNAME\nMTFCC\ngeometry\n\n\n\n\n0\nAtlantic\nL4150\nLINESTRING (-80.88368 32.03912, -80.88365 32.0...\n\n\n1\nAtlantic\nL4150\nLINESTRING (-70.66800 41.51199, -70.65663 41.5...\n\n\n2\nAtlantic\nL4150\nLINESTRING (-76.58108 38.09572, -76.58184 38.0...\n\n\n3\nAtlantic\nL4150\nLINESTRING (-73.75518 40.58565, -73.75517 40.5...\n\n\n4\nAtlantic\nL4150\nLINESTRING (-76.15615 38.63324, -76.15070 38.6...\n\n\n5\nAtlantic\nL4150\nLINESTRING (-76.53289 39.20776, -76.53298 39.2...\n\n\n6\nAtlantic\nL4150\nLINESTRING (-73.93653 40.56644, -73.93594 40.5...\n\n\n7\nAtlantic\nL4150\nLINESTRING (-81.10208 29.42706, -81.10215 29.4...\n\n\n8\nAtlantic\nL4150\nLINESTRING (-71.89236 41.32922, -71.89293 41.3...\n\n\n9\nAtlantic\nL4150\nLINESTRING (-75.31239 38.94595, -75.31239 38.9...\n\n\n\n\n\n\n\n\nThe coastline shapefile is comprised of ~3,000 LINESTRING objects. Let’s get a count of geometries by NAME:\n\n\ncoast[\"NAME\"].value_counts().sort_index()\n\nNAME\nAtlantic     941\nGulf         647\nPacific     1328\nName: count, dtype: int64\n\n\n\n\nWe can visualize the coastline by calling the coast GeoDataFrame’s plot method:\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 6), tight_layout=True)\nax.set_title(\"Lower 48 Coastline\", fontsize=9)\ncoast.plot(ax=ax, edgecolor=\"red\", linewidth=1.0)\nax.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\nTo overlay the coastline along with state boundaries, download the STATE shapefile from the Census Bureau’s FTP site and plot them together:\n\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 6), tight_layout=True)\nax.set_title(\"Lower 48 States with Coastline\", fontsize=9)\ncoast.plot(ax=ax, edgecolor=\"red\", linewidth=1.50, linestyle=\"--\")\nstates.boundary.plot(ax=ax, edgecolor=\"black\", linewidth=0.50)\nax.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\nLet’s next generate synthetic latitude-longitude pairs from within the Florida bounding envelope. The envelope bounds can be obtained from Florida’s geometry as follows:\n\n\n\n# Get bounding box for each state.\nstates[\"bbox\"] = states.geometry.map(lambda gg: gg.envelope.bounds)\n\n# Put coordinates in separate columns.\nstates[[\"lon0\", \"lat0\", \"lon1\", \"lat1\"]] = pd.DataFrame(states.bbox.tolist(), index=states.index)\n\nstates.head()\n\n\n\n\n\n\n\n\nNAME\ngeometry\nbbox\nlon0\nlat0\nlon1\nlat1\n\n\n\n\n0\nWest Virginia\nPOLYGON ((-77.75438 39.33346, -77.75422 39.333...\n(-82.644591, 37.20154, -77.719519, 40.638801)\n-82.644591\n37.201540\n-77.719519\n40.638801\n\n\n1\nFlorida\nMULTIPOLYGON (((-83.10874 24.62949, -83.10711 ...\n(-87.634896, 24.396308, -79.974306, 31.000968)\n-87.634896\n24.396308\n-79.974306\n31.000968\n\n\n2\nIllinois\nPOLYGON ((-87.89243 38.28285, -87.89334 38.282...\n(-91.513079, 36.970298, -87.019935, 42.508481)\n-91.513079\n36.970298\n-87.019935\n42.508481\n\n\n3\nMinnesota\nPOLYGON ((-95.31991 48.99892, -95.31778 48.998...\n(-97.239093, 43.499361, -89.483385, 49.384479)\n-97.239093\n43.499361\n-89.483385\n49.384479\n\n\n4\nMaryland\nPOLYGON ((-75.75600 39.24607, -75.75579 39.243...\n(-79.487651, 37.886605, -74.986282, 39.723037)\n-79.487651\n37.886605\n-74.986282\n39.723037\n\n\n\n\n\n\n\n\nLet’s draw the bounding region using folium:\n\n\nimport folium \n\n# Florida bounding box. \nlon0, lat0, lon1, lat1 = states[states.NAME==\"Florida\"].bbox.item()\n\nmlat, mlon = (lat0 + lat1) / 2, (lon0 + lon1) / 2\n\nm = folium.Map(\n    location=[mlat, mlon], \n    zoom_start=6, \n    zoom_control=True, \n    no_touch=True,\n    tiles=\"OpenStreetMap\"\n    )\n\nfolium.Rectangle(\n    [(lat0, lon0), (lat1, lon1)], \n    fill_color=\"blue\", fill_opacity=.05\n    ).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nSampling from the bounding region highlighted above will result in many points in the Gulf. Let’s narrow the sampling space:\n\n\nlon0, lat0, lon1, lat1 = (-80.5, 26, -81.75, 28)\n\nmlat, mlon = (lat0 + lat1) / 2, (lon0 + lon1) / 2\n\nm = folium.Map(\n    location=[mlat, mlon], \n    zoom_start=7, \n    zoom_control=True, \n    no_touch=True,\n    tiles=\"OpenStreetMap\"\n    )\n\nfolium.Rectangle(\n    [(lat0, lon0), (lat1, lon1)], \n    fill_color=\"blue\", fill_opacity=.05\n    ).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n# Sample within bounds defined by lat0, lon0, lat1, lon1. \n\nnbr_locations = 50\n\nrng = np.random.default_rng(516)\n\nrlats = rng.uniform(low=lat0, high=lat1, size=nbr_locations)\nrlons = rng.uniform(low=lon1, high=lon0, size=nbr_locations)\npoints = list(zip(rlats, rlons))\n\n\nVisualizing the synthetic locations:\n\n\nm = folium.Map(location=[mlat, mlon], zoom_start=8)\n\nfor lat, lon in points:\n\n    folium.CircleMarker(\n        location=[lat, lon], \n        radius=5, \n        color=\"red\", \n        fill_color=\"red\", \n        fill=True,\n        fill_opacity=1\n        ).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nNext the points list needs to be represented as a GeoDataFrame, using the generated points as the geometry. We set \"crs=EPSG:4326\" representing longitude-latitude pairs. A policy_id is included as an identifier for each point.\n\n\ndfpoints = pd.DataFrame({\n    \"policy_id\": [str(ii).zfill(7) for ii in range(len(points))],\n    \"lat\": rlats, \n    \"lon\": rlons, \n})\n\n# Create GeoDataFrame.\npoints = gpd.GeoDataFrame(\n    dfpoints,\n    geometry=gpd.points_from_xy(dfpoints.lon, dfpoints.lat),\n    crs=\"EPSG:4326\"\n)\n\npoints.head(10)\n\n\n\n\n\n\n\n\npolicy_id\nlat\nlon\ngeometry\n\n\n\n\n0\n0000000\n26.158425\n-81.697845\nPOINT (-81.69785 26.15842)\n\n\n1\n0000001\n27.308367\n-80.743510\nPOINT (-80.74351 27.30837)\n\n\n2\n0000002\n26.050160\n-81.226294\nPOINT (-81.22629 26.05016)\n\n\n3\n0000003\n26.896432\n-80.682348\nPOINT (-80.68235 26.89643)\n\n\n4\n0000004\n27.407736\n-81.556209\nPOINT (-81.55621 27.40774)\n\n\n5\n0000005\n26.551877\n-80.902803\nPOINT (-80.90280 26.55188)\n\n\n6\n0000006\n26.301071\n-81.521000\nPOINT (-81.52100 26.30107)\n\n\n7\n0000007\n27.435718\n-81.499145\nPOINT (-81.49914 27.43572)\n\n\n8\n0000008\n27.784378\n-80.590765\nPOINT (-80.59076 27.78438)\n\n\n9\n0000009\n27.635399\n-81.389836\nPOINT (-81.38984 27.63540)\n\n\n\n\n\n\n\n\nWith both the coastline shapefile and point data represented as GeoDataFrames, we execute the sjoin_nearest spatial join to get the distance from each point to the nearest coastline. First we need to set the crs to a projected coordinate system so the distances are returned in units of meters instead of degrees. Projected coordinate systems use linear units like meters or feet, which makes it easier to perform precise spatial measurements. Here we opt for the Conus Albers equal area conic projection (EPSG:5069).\nIn the call to sjoin_nearest, we specify “meters” for the distance_col argument. This column will hold the distance to the coastline for each point in points in units of meters. A miles column is added after the join.\n\n\n# Convert from GPS to  Conus Albers. \npoints = points.to_crs(\"EPSG:5069\")\ncoast = coast.to_crs(\"EPSG:5069\")\n\n# Perform spatial join. Covert meters to miles. \ngdf = gpd.sjoin_nearest(points, coast, how=\"left\", distance_col=\"meters\")\ngdf[\"miles\"] = gdf[\"meters\"] * 0.000621371\n\n# Get min, max and average distance to coast line.\nmin_dist = gdf.miles.min()\nmax_dist = gdf.miles.max()\navg_dist = gdf.miles.mean()\n\nprint(f\"min. distance to coastline: {min_dist}\")\nprint(f\"max. distance to coastline: {max_dist}\")\nprint(f\"avg. distance to coastline: {avg_dist}\")\n\nmin. distance to coastline: 0.20197535859716287\nmax. distance to coastline: 65.35183791451661\navg. distance to coastline: 35.26215239057489\n\n\n\n\n\nRate Group Based on Distance to Coastline\nLet’s imagine a hypothetical rating plan that uses the following distances from the coastline to determine rates:\n\n0 - 5 miles: very high risk\n5 - 25 miles: high risk\n25 - 50 miles: medium risk\ngreater than 50 miles: low risk\n\n\nA rolling join via merge_asof can be used to select the last row in the right DataFrame (group thresholds) whose on key is less than or equal to gdf’s key, which will be “miles” in both DataFrames.\n\n\n# Create dfgroups DataFrame. \ndfgroups = pd.DataFrame({\n    \"risk\": [\"very high\", \"high\", \"medium\", \"low\"],\n    \"miles\": [0., 5., 25., 50.]\n})\n\n# Assign risk group to each policy location.\ngdf = gdf.sort_values(\"miles\", ascending=True)\ngdf = pd.merge_asof(gdf, dfgroups, on=\"miles\", direction=\"backward\")\n\ngdf.head(10)\n\n\n\n\n\n\n\n\npolicy_id\nlat\nlon\ngeometry\nindex_right\nNAME\nMTFCC\nmeters\nmiles\nrisk\n\n\n\n\n0\n0000016\n27.972984\n-80.506574\nPOINT (1523126.311 669355.565)\n176\nAtlantic\nL4150\n325.047932\n0.201975\nvery high\n\n\n1\n0000000\n26.158425\n-81.697845\nPOINT (1436901.833 453402.683)\n2813\nGulf\nL4150\n11201.767881\n6.960454\nhigh\n\n\n2\n0000015\n26.383000\n-81.709033\nPOINT (1432089.414 477631.030)\n2817\nGulf\nL4150\n14922.894431\n9.272654\nhigh\n\n\n3\n0000045\n26.189330\n-81.661985\nPOINT (1439966.962 457301.849)\n2813\nGulf\nL4150\n15292.789125\n9.502496\nhigh\n\n\n4\n0000008\n27.784378\n-80.590765\nPOINT (1518283.776 647462.840)\n211\nAtlantic\nL4150\n16412.307856\n10.198132\nhigh\n\n\n5\n0000037\n26.366454\n-81.662935\nPOINT (1436947.116 476528.844)\n2813\nGulf\nL4150\n18665.871130\n11.598431\nhigh\n\n\n6\n0000030\n27.473270\n-80.505108\nPOINT (1532219.823 614970.878)\n10\nAtlantic\nL4150\n20055.344308\n12.461809\nhigh\n\n\n7\n0000017\n26.086354\n-81.489777\nPOINT (1458848.059 448748.018)\n2813\nGulf\nL4150\n24280.862758\n15.087424\nhigh\n\n\n8\n0000044\n27.226453\n-80.508471\nPOINT (1536305.431 588071.282)\n10\nAtlantic\nL4150\n29966.032034\n18.620023\nhigh\n\n\n9\n0000006\n26.301071\n-81.521000\nPOINT (1452153.395 471583.300)\n2813\nGulf\nL4150\n30913.531896\n19.208772\nhigh\n\n\n\n\n\n\n\n\nCounting the number of policies per risk group:\n\n\ngdf.risk.value_counts().sort_index()\n\nrisk\nhigh         12\nlow          10\nmedium       27\nvery high     1\nName: count, dtype: int64\n\n\n\nFinally, we can assign each risk a different color and visualize the resulting risk groups with folium:\n\n\n# Colors for each group.\ndcolors = {\n    \"very high\": \"#f40002\",\n    \"high\": \"#ff721f\",\n    \"medium\": \"#fafc15\",\n    \"low\": \"#268a6d\"\n}\n\ngdf[\"color\"] = gdf[\"risk\"].map(dcolors)\n\nm = folium.Map(location=[mlat, mlon], zoom_start=8)\n\nfor tt in gdf.itertuples():\n    lat, lon, color = tt.lat, tt.lon, tt.color\n    folium.CircleMarker(\n        location=[lat, lon],\n        radius=6, \n        color=color, \n        fill_color=color, \n        fill=True,\n        fill_opacity=1\n        ).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "posts/dijkstras-shortest-paths/dijkstras-shortest-paths.html",
    "href": "posts/dijkstras-shortest-paths/dijkstras-shortest-paths.html",
    "title": "Using Dijkstra’s Algorithm to Find All Shortest Paths in a Graph",
    "section": "",
    "text": "I always had trouble spelling Dijkstra correctly until someone pointed out “It’s just D, followed by i-j-k, then stra”. Once it was pointed out that i-j-k is represented sequentially in alphabetical order, spelling Dijkstra becomes trival - almost fun!\nDijkstra’s algorithm is one of the most well-known and studied graph algorithms for finding the shortest path between a set of vertices. For a specified starting node, the algorithm finds the shortest path between the source vertex and all other vertices in the graph. Dijkstra’s cannot handle graphs with negative edge weights: For graphs with negative weight edges, Floyd-Warshall or Bellman Ford can be used. But for graphs with positive edge weights, Dijkstra’s has better worst case performance than more general alternatives (O((n + m)log(n)) for Dijkstra vs. O(mn) for Bellman-Ford and O(n^3) for Floyd-Warshall).\nHowever, Dijkstra’s only returns a single shortest path. In many cases a single shortest path is enough. But there are plenty of applications in which knowledge of alternate minimum weight paths can be useful. If a graph has multiple shortest paths, it is necessary to add supplementary logic to identify and return all such paths. The modified routine to return all shortest paths from graph G is described below:\n\nRun Dijkstra’s starting from start node a. Returns shortest path and distance to every node from vertex a.\nRun Dijkstra’s starting from end node f. Returns shortest path and distance to every node from vertex f.\nLet dist_a2f = A shortest path from vertex a to vertex f.\nFor (u, v) in G.edges:\nLet w_uv = Weight of edge (u, v).\nLet dist_a2u = Distance from start vertex a to vertex u.\nLet dist_f2v = Distance from end vertex f to vertex v.\nIf dist_a2u + w_uv + dist_f2v == dist_a2f:\nThen edge (u, v) is on some minimal weight path.\n\nLets use networkx to create a graph with multiple shortest paths:\n\n\n# Create network with multiple minimal weight paths.\nimport matplotlib.pyplot as plt\nimport networkx as nx\n\nseed = 518\nG = nx.Graph()\n\nG.add_edge(\"a\", \"b\", weight=4)\nG.add_edge(\"b\", \"c\", weight=2)\nG.add_edge(\"a\", \"c\", weight=6)\nG.add_edge(\"c\", \"d\", weight=1)\nG.add_edge(\"d\", \"e\", weight=3)\nG.add_edge(\"e\", \"f\", weight=2)\nG.add_edge(\"c\", \"f\", weight=6)\nG.add_edge(\"a\", \"d\", weight=9)\nG.add_edge(\"f\", \"d\", weight=10)\n\nedges = [(u, v) for (u, v, d) in G.edges(data=True)]\n\ncolor_map = []\nfor g in G:\n    if g==\"a\":\n        color_map.append(\"green\")\n    elif g==\"f\":\n        color_map.append(\"red\")\n    else:\n        color_map.append(\"blue\")\n\n\n# edge weight labels.\nedge_labels = nx.get_edge_attributes(G, \"weight\")\n\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 6), tight_layout=True)\npos = nx.spring_layout(G, seed=seed)\nnx.draw_networkx_nodes(G, pos, node_size=800, node_color=color_map)\nnx.draw_networkx_edges(G, pos, edgelist=edges, width=2, alpha=0.5, edge_color=\"grey\", style=\"solid\")\nnx.draw_networkx_labels(G, pos, font_size=16, font_color=\"white\", font_weight=\"bold\", font_family=\"sans-serif\")\nnx.draw_networkx_edge_labels(G, pos, edge_labels, font_size=12)\nplt.axis(\"off\")\nplt.show();\n\n\n\n\n\n\n\n\nG has three shortest paths of weight/distance 12:\n\na - c - f\na - b - c - f\na - c - d - e - f\n\nRunning single_source_dijkstra from networkx returns the distance to each vertex in a graph from a specified starting vertex (in our example, vertex a). single_source_dijkstra returns two dictionaries: The first stores distances to each vertex; the second stores the shortest path from the start node to all other vertices (d2v0 and p2v0 in the code below):\n\n\nd2v0, p2v0 = nx.single_source_dijkstra(G, \"a\")\nprint(f\"d2v0: {d2v0}\")\nprint(f\"p2v0: {p2v0}\")\n\nd2v0: {'a': 0, 'b': 4, 'c': 6, 'd': 7, 'e': 10, 'f': 12}\np2v0: {'a': ['a'], 'b': ['a', 'b'], 'c': ['a', 'c'], 'd': ['a', 'c', 'd'], 'f': ['a', 'c', 'f'], 'e': ['a', 'c', 'd', 'e']}\n\n\nsingle_source_dijkstra only returns a - c - f. To also identify a - b - c - f and a - c - d - e - f, we implement the steps from our pseudocode:\n\n\n# Idenitfy all shortests paths in graph G.\nall_shortest_paths = set()\n\n# d2v: Distance to vertex (from start node a).\n# p2v: Shortest path to vertext (from start node a).\nd2v0, p2v0 = nx.single_source_dijkstra(G, \"a\")\nd2v1, p2v1 = nx.single_source_dijkstra(G, \"f\")\nall_shortest_paths.add(tuple(p2v0[\"f\"]))\n\n# Iterate through all edges. Check for additional shortest paths. \n\n# Distance of a shortest path from a to f. \ndist_a2f = d2v0[\"f\"]\n\nfor u, v, dw in G.edges(data=True):\n    \n    # Get weight of current edge spanning (u, v)\n    w_uv = dw[\"weight\"]\n    \n    # Get distance from start vertex to u.\n    dist_a2u = d2v0[u]\n    \n    # Get distance from end vertex to v.\n    dist_f2v = d2v1[v]\n    \n    if dist_a2u + w_uv + dist_f2v == dist_a2f:\n        \n        # Edge uv is on some minimal weight path. Append to all_shortest_paths.\n        pp = tuple(p2v0[u] + p2v1[v][::-1])\n        all_shortest_paths.add(pp)\n\nShortest paths are stored in a set of ordered tuples, with each tuple representing the vertices of some path of minimum weight. Printing the contents of all_shortest_paths yields:\n\nall_shortest_paths\n\n{('a', 'b', 'c', 'f'), ('a', 'c', 'd', 'e', 'f'), ('a', 'c', 'f')}\n\n\nThe time complexity for two separate invocations of Dijkstra’s is 2 * O((n + m)log(n)), and O(m + n) to iterate through the adjacency list checking for intermediate edges falling on a path of minimum weight."
  },
  {
    "objectID": "posts/data-table-special-symbols/data-table-special-symbols.html",
    "href": "posts/data-table-special-symbols/data-table-special-symbols.html",
    "title": "Introduction to Special Symbols in data.table",
    "section": "",
    "text": "The data.table library is an R package that provides an enhanced version of R’s builtin data.frame. There are many available resources that introduce data.table’s syntax and provide examples of typical usage scenarios. In this post, we’ll cover one aspect of the data.table API: Special symbols.\nBefore proceeding, we should highlight data.table’s general syntactical form, that when understood, can be used to translate data.table expressions of arbitrary complexity into 3 steps:\nWhich translates to:\nThe following annotated diagram is included as part of data.table’s documentation:\nFor those familiar with SQL, the three expressions that describe the general form can be thought of as components of a SQL query: “SELECT from j, WHERE i, GROUP BY by”. Although the SQL analog of the original interpretation may not hold for every possible data.table operation, it remains a valid proxy for many scenarios encountered in practice. Although this applies to data.table in general, a thorough understanding of the general form will be especially useful when exploring ways to leverage special symbols in your own code.\nThe data.table special symbols covered here are defined as follows:"
  },
  {
    "objectID": "posts/data-table-special-symbols/data-table-special-symbols.html#n",
    "href": "posts/data-table-special-symbols/data-table-special-symbols.html#n",
    "title": "Introduction to Special Symbols in data.table",
    "section": ".N",
    "text": ".N\nThe special symbol .N can be used to return the last row or number of rows of a data.table. Referring to the R sample dataset CO2, what follows are a few examples using .N:\n&gt; library(\"data.table\")\n\n&gt; DT = CO2; setDT(DT)\n\n&gt; head(DT)\n   Plant   Type  Treatment conc uptake\n1:   Qn1 Quebec nonchilled   95   16.0\n2:   Qn1 Quebec nonchilled  175   30.4\n3:   Qn1 Quebec nonchilled  250   34.8\n4:   Qn1 Quebec nonchilled  350   37.2\n5:   Qn1 Quebec nonchilled  500   35.3\n6:   Qn1 Quebec nonchilled  675   39.2\n\n### Return last record of CO2 dataset.\n&gt; DT[.N]\n   Plant        Type Treatment conc uptake\n1:   Mc3 Mississippi   chilled 1000   19.9\n\n\n### Return the second-to-last record of CO2 dataset.\n&gt; DT[.N-1]\n   Plant        Type Treatment conc uptake\n1:   Mc3 Mississippi   chilled  675   18.9\n\n\n### Return the number of records in CO2 dataset.\n&gt; DT[,.N]\n[1] 84\n\n\n### Return the number of records for each unique value of Treatment.\n&gt; DT[, .N, by=Treatment]\n    Treatment  N\n1: nonchilled 42\n2:    chilled 42\n\n\n### Return the number of records for each unique combination of Treatment` & Plant.\n&gt; DT[, .N, keyby=\"Treatment,Plant\"]\n\n     Treatment Plant N\n 1: nonchilled   Qn1 7\n 2: nonchilled   Qn2 7\n 3: nonchilled   Qn3 7\n 4: nonchilled   Mn3 7\n 5: nonchilled   Mn2 7\n 6: nonchilled   Mn1 7\n 7:    chilled   Qc1 7\n 8:    chilled   Qc3 7\n 9:    chilled   Qc2 7\n10:    chilled   Mc2 7\n11:    chilled   Mc3 7\n12:    chilled   Mc1 7\n.N can be used to enumerate records, creating a row counter field:\n&gt; (DT[,CNTR:=1:.N])\n\n    Plant        Type  Treatment conc uptake CNTR\n 1:   Qn1      Quebec nonchilled   95   16.0    1\n 2:   Qn1      Quebec nonchilled  175   30.4    2\n 3:   Qn1      Quebec nonchilled  250   34.8    3\n 4:   Qn1      Quebec nonchilled  350   37.2    4\n 5:   Qn1      Quebec nonchilled  500   35.3    5\n                          .\n                          .\n                          .\n79:   Mc3 Mississippi    chilled  175   18.0   79\n80:   Mc3 Mississippi    chilled  250   17.9   80\n81:   Mc3 Mississippi    chilled  350   17.9   81\n82:   Mc3 Mississippi    chilled  500   17.9   82\n83:   Mc3 Mississippi    chilled  675   18.9   83\n84:   Mc3 Mississippi    chilled 1000   19.9   84\nSimiliarly, enumeration can be applied by group:\n(DT[,CNTR:=1:.N, by=Plant])\n\n    Plant        Type  Treatment conc uptake CNTR\n 1:   Qn1      Quebec nonchilled   95   16.0    1\n 2:   Qn1      Quebec nonchilled  175   30.4    2\n 3:   Qn1      Quebec nonchilled  250   34.8    3\n 4:   Qn1      Quebec nonchilled  350   37.2    4\n 5:   Qn1      Quebec nonchilled  500   35.3    5\n 6:   Qn1      Quebec nonchilled  675   39.2    6\n 7:   Qn1      Quebec nonchilled 1000   39.7    7\n 8:   Qn2      Quebec nonchilled   95   13.6    1\n 9:   Qn2      Quebec nonchilled  175   27.3    2\n10:   Qn2      Quebec nonchilled  250   37.1    3\n11:   Qn2      Quebec nonchilled  350   41.8    4\n12:   Qn2      Quebec nonchilled  500   40.6    5\n13:   Qn2      Quebec nonchilled  675   41.4    6\n14:   Qn2      Quebec nonchilled 1000   44.3    7"
  },
  {
    "objectID": "posts/data-table-special-symbols/data-table-special-symbols.html#sd",
    "href": "posts/data-table-special-symbols/data-table-special-symbols.html#sd",
    "title": "Introduction to Special Symbols in data.table",
    "section": ".SD",
    "text": ".SD\n.SD (subset of x’s data) contains all columns of the data.table except the grouping columns by default. Referring again to the CO2 dataset, we can use .SD to print the contents of a data.table:\n&gt; library(\"data.table\")\n\n&gt; DT = CO2; setDT(DT)\n\n&gt; DT[,.SD]\n\n    Plant        Type  Treatment conc uptake\n 1:   Qn1      Quebec nonchilled   95   16.0\n 2:   Qn1      Quebec nonchilled  175   30.4\n 3:   Qn1      Quebec nonchilled  250   34.8\n 4:   Qn1      Quebec nonchilled  350   37.2\n                    .\n                    .\n                    .\n83:   Mc3 Mississippi    chilled  675   18.9\n84:   Mc3 Mississippi    chilled 1000   19.9\nAlthough uncommon, .SD can be used for sub-setting columns. Next, we retrieve Treatment and conc from the CO2 dataset using three approaches, demonstrating each method returns an identical subset:\n# Using .SD to retrieve Treatment & conc.\n&gt; DT1 = DT[, .SD, .SDcols=c(\"Treatment\", \"conc\")]\n\n&gt; head(DT1)\n    Treatment conc\n1: nonchilled   95\n2: nonchilled  175\n3: nonchilled  250\n4: nonchilled  350\n5: nonchilled  500\n6: nonchilled  675\n\n&gt; DT2 = DT[, .(Treatment, conc)]\n\n# If column names are available only as a character vector,\n# i.e., c(\"Treatment\", \"conc\") include `with=FALSE`.\n&gt; DT3 = DT[, c(\"Treatment\", \"conc\"), with=FALSE]\n\n# Test for equality amongst DT1, DT2 & DT3.\n&gt; identical(DT1, DT3); identical(DT2, DT3)\n[1] TRUE\n[1] TRUE\n.SD can be used to retrieve the first 3 rows for all fields partitioned by Plant:\n&gt; (DT1 = DT[,.SD[1:3], by=Plant])\n\n    Plant        Type  Treatment conc uptake\n 1:   Qn1      Quebec nonchilled   95   16.0\n 2:   Qn1      Quebec nonchilled  175   30.4\n 3:   Qn1      Quebec nonchilled  250   34.8\n 4:   Qn2      Quebec nonchilled   95   13.6\n 5:   Qn2      Quebec nonchilled  175   27.3\n 6:   Qn2      Quebec nonchilled  250   37.1\n 7:   Qn3      Quebec nonchilled   95   16.2\n 8:   Qn3      Quebec nonchilled  175   32.4\n 9:   Qn3      Quebec nonchilled  250   40.3\n                      .\n                      .\n                      .\n28:   Mc1 Mississippi    chilled   95   10.5\n29:   Mc1 Mississippi    chilled  175   14.9\n30:   Mc1 Mississippi    chilled  250   18.1\n31:   Mc2 Mississippi    chilled   95    7.7\n32:   Mc2 Mississippi    chilled  175   11.4\n33:   Mc2 Mississippi    chilled  250   12.3\n34:   Mc3 Mississippi    chilled   95   10.6\n35:   Mc3 Mississippi    chilled  175   18.0\n36:   Mc3 Mississippi    chilled  250   17.9    \nIt is possible to retrieve the 1st, 2nd, second-to-last and last records for each unique value present in Type:\n&gt; DT[, .SD[c(1, 2, .N-1, .N)], by=Type]\n\n          Type Plant  Treatment conc uptake\n1:      Quebec   Qn1 nonchilled   95   16.0\n2:      Quebec   Qn1 nonchilled  175   30.4\n3:      Quebec   Qc3    chilled  675   39.6\n4:      Quebec   Qc3    chilled 1000   41.4\n5: Mississippi   Mn1 nonchilled   95   10.6\n6: Mississippi   Mn1 nonchilled  175   19.2\n7: Mississippi   Mc3    chilled  675   18.9\n8: Mississippi   Mc3    chilled 1000   19.9\n.SD can be used along with .SDcols to convert fields with a factor datatype to character type. .SDcols is frequently used with .SD to specify a subset of the columns of .SD to be used in j. If .SDcols is present, it is generally bound to a vector of field names, and the operation in j will be applied only to the fields associated with the names specified in .SDcols:\n### First, list original field datatypes.\n&gt; lapply(DT, class)\n\n$Plant\n[1] \"ordered\" \"factor\" \n\n$Type\n[1] \"factor\"\n\n$Treatment\n[1] \"factor\"\n\n$conc\n[1] \"numeric\"\n\n$uptake\n[1] \"numeric\"\n\n\n### Convert fields with factor datatype (Plant, Type & Treatment) \n### to character type. Isolate factor fieldnames.\n&gt; factorFieldnames = names(Filter(is.factor, DT))\n\n### Convert factor datatypes to character types.\n&gt; DT[,(factorFieldnames):=lapply(.SD, as.character), .SDcols=factorFieldnames]\n\n### Verify datatypes have been successfully coerced.\n&gt; lapply(DT, class)\n\n$Plant\n[1] \"character\"\n\n$Type\n[1] \"character\"\n\n$Treatment\n[1] \"character\"\n\n$conc\n[1] \"numeric\"\n\n$uptake\n[1] \"numeric\"\nTwo things to note: First, because we used the assignment by reference operator, :=, to recast data types, it is not necessary to re-assign the result of the type-coercion back to DT. This is because when using :=, operations are performed in-place and by reference, therefore the result of a particular action are visible immediately in DT. Second, since factorFieldnames is enclosed by parentheses preceding :=, the result is assigned to the columns specified in factorFieldnames. If parentheses we left out, a new field named factorFieldnames would be added to DT, which is not the desired behavior.\n.SD can be used to calculate the average of any fields of interest overall or by group:\n### Calculate the overall average of conc & uptake.\n&gt; (avgDT1 = DT[, lapply(.SD, mean), .SDcols=c(\"conc\", \"uptake\")])\n\n   conc  uptake\n1:  435 27.2131\n\n\n### Calculate the average of conc & uptake by Plant.\n&gt; (avgDT2 = DT[, lapply(.SD, mean), .SDcols=c(\"conc\", \"uptake\"), by=Plant])\n\n    Plant conc   uptake\n 1:   Qn1  435 33.22857\n 2:   Qn2  435 35.15714\n 3:   Qn3  435 37.61429\n 4:   Qc1  435 29.97143\n 5:   Qc2  435 32.70000\n 6:   Qc3  435 32.58571\n 7:   Mn1  435 26.40000\n 8:   Mn2  435 27.34286\n 9:   Mn3  435 24.11429\n10:   Mc1  435 18.00000\n11:   Mc2  435 12.14286\n12:   Mc3  435 17.30000\nOne final example using .SD: As mentioned earlier in the post, .SDcols is used to specify the columns of the data.table that .SD will operate on. Alternatively, we can use .SDcols to exclude fields from the operation performed by .SD. Referring to the example in which we calculated the average of conc & uptake in the CO2 dataset, we can instead specify the fields .SD should not average over. Here’s an alternative approach that results in excluding Plant, Type and Treatment from the average calculation:\nDT = CO2; setDT(DT)\n\n# Calculate the average of conc & uptake by excluding Plant, Type & Treatment.\n&gt; (avgDT = DT[, lapply(.SD, mean), .SDcols=!c(\"Plant\", \"Type\", \"Treatment\")]))\n\n   conc  uptake\n1:  435 27.2131\nClearly in this example it is not advantageous to specify the fields not included in the average, but situations do arise in which this functionality turns out to be very convenient."
  },
  {
    "objectID": "posts/data-table-special-symbols/data-table-special-symbols.html#by",
    "href": "posts/data-table-special-symbols/data-table-special-symbols.html#by",
    "title": "Introduction to Special Symbols in data.table",
    "section": ".BY",
    "text": ".BY\n.BY is a list containing a length 1 vector for each item in by. There are few examples in which the .BY special symbol is used, so we’ll reproduce the example included in the data.table vignette:\nlibrary(\"data.table\")\n\n&gt; DT = data.table(\n           x=rep(c(\"b\", \"a\", \"c\"), each=3), \n           v=c(1, 1, 1, 2, 2, 1, 1, 2, 2), \n           y=c(1, 3, 6), \n           a=1:9, \n           b=9:1\n           )\n\n&gt; head(DT)\n\n   x v y a b\n1: b 1 1 1 9\n2: b 1 3 2 8\n3: b 1 6 3 7\n4: a 2 1 4 6\n5: a 2 3 5 5\n6: a 1 6 6 4\n\n\n&gt; X = data.table(\n      x=c(\"c\",\"b\"), \n      v=8:7, \n      foo=c(4,2)\n      )\n\n&gt; head(X)\n\n   x v foo\n1: c 8   4\n2: b 7   2\n\n\n# Join within each group.\n&gt; X[, DT[.BY, y, on=\"x\"], by=x]\n\n   x V1\n1: c  1\n2: c  3\n3: c  6\n4: b  1\n5: b  3\n6: b  6"
  },
  {
    "objectID": "posts/data-table-special-symbols/data-table-special-symbols.html#i",
    "href": "posts/data-table-special-symbols/data-table-special-symbols.html#i",
    "title": "Introduction to Special Symbols in data.table",
    "section": ".I",
    "text": ".I\n.I is an integer vector equal to seq_len(nrow(x)). But we can instead use .I to return a vector containing the row indices of the records in a data.table that meet the condition specified in i. In the next example, we demonstrate how to return the row indices from CO2 having conc&gt;500 using .I:\n### Return row indicies in which conc &gt; 500.\n&gt; DT[,.I[conc &gt; 500]]\n[1]  6  7 13 14 20 21 27 28 34 35 41 42 48 49 55 56 62 63 69 70 76 77 83 84\nUsing .I after the condition specified in i returns a vector of length equal to the number of records meeting the i’s criteria:\n&gt; DT[conc &gt; 500, .I]\n\n[1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\nNote that use of .I may have been supplanted in favor of data.table’s which parameter. Instead of using .I, we can use which=TRUE to return the row indices of records meeting the condition specified in i:\n&gt; DT[conc &gt; 500, which=TRUE]\n[1]  6  7 13 14 20 21 27 28 34 35 41 42 48 49 55 56 62 63 69 70 76 77 83 84"
  },
  {
    "objectID": "posts/data-table-special-symbols/data-table-special-symbols.html#grp",
    "href": "posts/data-table-special-symbols/data-table-special-symbols.html#grp",
    "title": "Introduction to Special Symbols in data.table",
    "section": ".GRP",
    "text": ".GRP\n.GRP is an integer, length 1, containing a simple group counter. If used without assignment to a new field, .GRP will return an enumerated data.table containing the unique combination of fields specified in by. If we specify the key of the CO2 dataset as “Plant”, “Type”, “Treatment”, we can return the unique combinations of these fields by referencing the data.table’s key in the by clause:\n### Set key on DT.\n&gt; setkeyv(DT, c(\"Plant\", \"Type\", \"Treatment\"))\n\n### Return enumerated data.table listing unique combinations of fields specified in by.\n&gt; DT[, .GRP, by=key(DT)])\n\n    Plant        Type  Treatment GRP\n 1:   Qn1      Quebec nonchilled   1\n 2:   Qn2      Quebec nonchilled   2\n 3:   Qn3      Quebec nonchilled   3\n 4:   Qc1      Quebec    chilled   4\n 5:   Qc3      Quebec    chilled   5\n 6:   Qc2      Quebec    chilled   6\n 7:   Mn3 Mississippi nonchilled   7\n 8:   Mn2 Mississippi nonchilled   8\n 9:   Mn1 Mississippi nonchilled   9\n10:   Mc2 Mississippi    chilled  10\n11:   Mc3 Mississippi    chilled  11\n12:   Mc1 Mississippi    chilled  12\nNotice that fields not specified in by are omitted from the output.\nWe can also use .GRP to indicate which unique group a record belongs to for each record in the parent data.table. We use the := operator to define a new column (by reference) indicating each record’s associated group:\n&gt; (DT[,GROUP_ID:=.GRP, by=key(DT)])\n\n    Plant        Type  Treatment conc uptake GROUP_ID\n 1:   Qn1      Quebec nonchilled   95   16.0        1\n 2:   Qn1      Quebec nonchilled  175   30.4        1\n 3:   Qn1      Quebec nonchilled  250   34.8        1\n 4:   Qn1      Quebec nonchilled  350   37.2        1\n 5:   Qn1      Quebec nonchilled  500   35.3        1\n 6:   Qn1      Quebec nonchilled  675   39.2        1\n 7:   Qn1      Quebec nonchilled 1000   39.7        1\n 8:   Qn2      Quebec nonchilled   95   13.6        2\n 9:   Qn2      Quebec nonchilled  175   27.3        2\n10:   Qn2      Quebec nonchilled  250   37.1        2\n                          .\n                          .\n                          .\n79:   Mc1 Mississippi    chilled  175   14.9       12\n80:   Mc1 Mississippi    chilled  250   18.1       12\n81:   Mc1 Mississippi    chilled  350   18.9       12\n82:   Mc1 Mississippi    chilled  500   19.5       12\n83:   Mc1 Mississippi    chilled  675   22.2       12\n84:   Mc1 Mississippi    chilled 1000   21.9       12"
  },
  {
    "objectID": "posts/creating-gh-pages-blog-with-quarto/creating-gh-pages-blog-with-quarto.html",
    "href": "posts/creating-gh-pages-blog-with-quarto/creating-gh-pages-blog-with-quarto.html",
    "title": "Creating a Blog with Quarto and GitHub Pages",
    "section": "",
    "text": "Quarto is a publishing system that allows for the creation of reproducible documents, presentations and websites using both R and Python. Quarto supports Jupyter Notebooks, making it possible to write content in Jupyter notebooks which can then converted to HTML. The static HTML files can be hosted free of charge on GitHub. GitHub Pages is a feature that allows you to host static websites directly from a GitHub repository. In what follows I walkthrough the setup and configuration of a technical blog managed by Quarto and hosted on GitHub Pages.\n\nCreate new Quarto project\nOnce quarto has been installed, create a new project using the Quarto CLI. For the purposes of demonstration, we will refer to this as dat303-blog, but you can name it anything (just don’t include whitespace). A folder with that name will be created in the Git client’s current working directory. I like to keep all my repositories in a Repos folder, so I’ll first navigate to Repos using the cd command (note that in the examples that follow, lines starting with # are comments and should not be run. Lines starting with $ represent the command line prompt and should be run from the Git client):\n# Switch into desired directory to create dat303-blog folder.\n$ cd T:/Repos\n\nNext, we run the quarto create-project command, which will create a folder named dat303-blog located at T:/Repos/at303-blog:\n$ quarto create-project dat303-blog --type website:blog\n\nThis folder will contain a posts folder which will eventually contain our blog content, and a number of additional files:\n\n_quarto.yaml: Contains the title of our blog, links to our GitHub/social media accounts and styling options. By default, _quarto.yaml looks like:\n# contents of _quarto.yaml.\nproject:\ntype: website\n\nwebsite:\ntitle: \"dat303-blog\"\nnavbar:\n    right:\n    - about.qmd\n    - icon: github\n        href: https://github.com/\n    - icon: twitter\n        href: https://twitter.com\nformat:\nhtml:\n    theme: cosmo\n    css: styles.css\n\nThe theme is initially set to “cosmo”. It can be changed to any valid bootswatch theme. The full list of available themes can be found here.\nabout.qmd: File to provide information about yourself.\nprofile.jpg: Replace this with a personal photo with the same name (profile.jpg), or update the name of the photo in about.qmd for the image key-value pair.\n\n\n\n\nCreate .gitignore\nA .gitignore file is used in Git to specify which files or directories should be ignored by Git when you make a commit. This means that any files or directories listed in the .gitignore file won’t be tracked by Git, which is useful for excluding files that are not necessary for version control. From VSCode, create a file named .gitignore and save it to the dat303-blog folder. Add the following lines to the .gitignore:\n/.quarto/\n/_site/\n.*\nSave your changes.\n\n\n\nInitialize Git project in dat303-blog directory\nEnsure that the current working directory of the Git client is dat303-blog, then run git init:\n$ cd T:/Repos/dat303-blog\n$ git init\n\n\n\nAdding new content\nIn the posts directory, we will create a new subdirectory for each post. We use all lowercase with words separated by dashes to make it easy to navigate between pages.\nFor example, I might create a solving-normal-equations directory under posts. Within the directory, I would create a new jupyter notebook named solving-normal-equations.ipynb.\nIn the very first cell of solving-normal-equations.ipynb, change the cell type to raw (click on the lower right of the cell and change “Python” to “raw”), and add the following header detail:\n---\ntitle: Solving the Normal Equations\ndate: 2024-09-02\ndescription: An investigation into solving the normal equations with Python\ncategories: [Python]\n---\n\nBe sure to include the three dashes at the top and bottom of the cell as in the example above.\nPopulate the remaining cells of your notebook with your inline commentary, code and plots, etc. Be sure to save your changes.\n\n\n\nCommitting changes locally\nSaving our changes with Git is a two-step process: We first stage any changes via git add, then commit them using git commit. Whenever running git commit, you are required to include a commit message, which comes after the -m flag. Assuming the notebook has been saved locally, run:\n$ git add --all\n$ git commit -m \"Added solving-normal-equations article.\"\n\n\n\nCreating repository on GitHub\nFrom GitHub, click on the + and select New Repository. In the Repository name field, enter [username].github.io. In this example, it would be jtrive.github.io:\n\n\n\nnew-repo\n\n\n\nAdd a description and be sure to keep the repository Public. Do not add a README or a .gitignore (we already created this). Click on Create Repository.\nIn the next window, be sure to click on SSH at the top. You’ll see something similar to:\n\n\nSince we already created our repository locally and committed our first change, we are going to focus on the second box, …or push an existing repository from the command line. Copy the first line starting with git remote add origin ... and paste it into the Git client and hit enter. In my case, it looked like:\n$ git remote add origin git@github.com:jtrive/jtrive.github.io.git\nDon’t worry about the other two commands: We have to do things a little different since we’re using Quarto.\n\n\n\nCreate gh-pages branch\nVerify that no changes are pending in your blog directory by running git status:\n$ git status\nOn branch master\nnothing to commit, working tree clean\n\nWe need to create a separate gh-pages branch to host our blog. Note that this is a one-time action. From the dat303-blog directory, run the following commands (make sure all changes are committed before running this!):\n$ git checkout --orphan gh-pages\n$ git reset --hard\n$ git commit --allow-empty -m \"Initializing gh-pages branch.\"\n$ git push origin gh-pages\n\n\n\nPublishing content with Quarto\nFrom the Git client, checkout the master branch, then run quarto publish gh-pages:\n$ git checkout master\n$ quarto publish gh-pages\n\nType Y when prompted:\n$ quarto publish gh-pages\n? Update site at git@github.com-jtrive:jtrive/jtrive.github.io.git? (Y/n) » Y\n\nUpon completion, navigate to jtrive.github.io. You’ll see something like:\n\n\nYou can remove the Post With Code and Welcome to My Blog subdirectories under posts to drop those entries. Navigating to Solving the Normal Equations, we see:\n\nLooks pretty good!\n\n\n\nSteady-State Workflow\nMany of the initial configuration steps are one-time actions. Once you’ve setup your blog as described, the typical workflow will be the following:\n\nCreate a new folder in the posts directory, using lowercase letters/numbers with words separated by dashes.\nCreate a Jupyter notebook in this directory with the same name and .ipynb extension.\nChange the first cell of the notebook to raw, and add title information as shown below. Be sure the first and last lines are three dashes, ---:\n ---\n title: Solving the Normal Equations\n date: 2024-02-09\n description: An investigation into solving the normal equations with Python\n categories: [Python]\n ---\nCreate your blog post (narrative text, code, plots, equations, etc.). Save your changes.\nFrom the Git client, navigate to the blog directory, then add and commit your changes:\n $ cd /path/to/blog\n $ git add --all\n $ git commit -m \"Added second blog post.\"\nEnsure master branch is checkout (it should already be), then run the following two commands:\n $ git checkout master\n $ quarto publish gh-pages\nView your published content at [username].github.io. If my username is jtrive, my content will be available at jtrive.github.io."
  },
  {
    "objectID": "posts/correl-reserves-r/correl-reserves-r.html",
    "href": "posts/correl-reserves-r/correl-reserves-r.html",
    "title": "Generating Correlated Boostrap Reserve Estimates",
    "section": "",
    "text": "In this post, a technique to estimate total reserves accounting for correlation between lines of business is introduced. We focus on workers compensation, commercial auto, product liability and other liability data sourced from the CAS Loss Reserves Database. We’ll demonstrate how to account for correlation between lines, and show how changes to the correlation assumption affects the total reserve estimate.\nThe CAS Loss Reserves Database represents a set of triangles intended for use in claims reserving studies. The data includes major personal and commercial lines of business from U.S. property casualty insurers. The claims data comes from Schedule P in the National Association of Insurance Commissioners (NAIC) database. NAIC Schedule P contains information on claims for major personal and commercial lines for all P&C insurers that write business in US.\nThe data can be downloaded from the CAS website directly using data.table’s fread. We perform some preprocessing to normalize column names and assign like columns the same name in each table.\nlibrary(\"data.table\")\nlibrary(\"foreach\")\nlibrary(\"ChainLadder\")\nlibrary(\"ggplot2\")\n\nDF1 = fread(\"https://www.casact.org/sites/default/files/2021-04/wkcomp_pos.csv\")   # workers compensation\nDF2 = fread(\"https://www.casact.org/sites/default/files/2021-04/comauto_pos.csv\")  # commercial auto\nDF3 = fread(\"https://www.casact.org/sites/default/files/2021-04/prodliab_pos.csv\") # product liability\nDF4 = fread(\"https://www.casact.org/sites/default/files/2021-04/othliab_pos.csv\")  # other liability\n\nnames(DF1) = tolower(names(DF1))\nnames(DF2) = tolower(names(DF2))\nnames(DF3) = tolower(names(DF3))\nnames(DF4) = tolower(names(DF4))\n\nsetnames(\n    DF1, c(\"incurloss_d\", \"bulkloss_d\", \"accidentyear\", \"developmentlag\"),\n    c(\"incurloss\", \"bulkloss\", \"origin\", \"dev\")\n    )\nsetnames(\n    DF2, c(\"incurloss_c\", \"bulkloss_c\", \"accidentyear\", \"developmentlag\"),\n    c(\"incurloss\", \"bulkloss\", \"origin\", \"dev\")\n    )\nsetnames(\n    DF3, c(\"incurloss_r1\", \"bulkloss_r1\", \"accidentyear\", \"developmentlag\"),\n    c(\"incurloss\", \"bulkloss\", \"origin\", \"dev\")\n    )\nsetnames(\n    DF4, c(\"incurloss_h1\", \"bulkloss_h1\", \"accidentyear\", \"developmentlag\"),\n    c(\"incurloss\", \"bulkloss\", \"origin\", \"dev\")\n    )\n\ndfList = list(wkcomp=DF1, comauto=DF2, prodliab=DF3, othliab=DF4) \n\nEach dataset contains loss data indexed by grcode, which is a company id. We need to find a company with losses in DF1, DF2, DF3 and DF4. This can be accomplished with the following:\ngrcodes = Reduce(\n    function(v1, v2) intersect(v1, v2),\n    lapply(dfList, function(DF) unique(DF[,grcode]))\n    )\ngrnamesDF = unique(\n    DF1[grcode %in% grcodes, .(grcode, grname)]\n    )\nsetorderv(grnamesDF, c(\"grcode\"), c(1))\n\nWhich yields:\n    grcode                              grname\n 1:    337                  California Cas Grp\n 2:    715               West Bend Mut Ins Grp\n 3:   1066                  Island Ins Cos Grp\n 4:   1538              Farmers Automobile Grp\n 5:   1767                  State Farm Mut Grp\n 6:   2143   Farmers Alliance Mut & Affiliates\n 7:   5185                    Grinnell Mut Grp\n 8:   7080        New Jersey Manufacturers Grp\n 9:   9466                      Lumber Ins Cos\n10:  10048    Hyundai Marine & Fire Ins Co Ltd\n11:  11126 Yasuda Fire & Marine Ins Co Of Amer\n12:  13439                 Partners Mut Ins Co\n13:  13528              Brotherhood Mut Ins Co\n14:  13587                  Chicago Mut Ins Co\n15:  14044                Goodville Mut Cas Co\n16:  14257                      IMT Ins Co Mut\n17:  14370                  Lebanon Mut Ins Co\n18:  14508         Michigan Millers Mut Ins Co\n19:  15024                Preferred Mut Ins Co\n20:  18791                 Virginia Mut Ins Co\n21:  23663            National American Ins Co\n22:  26433                   Harco Natl Ins Co\n23:  28258             Continental Natl Ind Co\n24:  35408                  Sirius Amer Ins Co\n25:  38300    Samsung Fire & Marine Ins Co Ltd\n26:  38733                   Alaska Nat Ins Co\n27:  44091 Dowa Fire & Marine Ins Co Ltd Us Br\n    grcode                              grname\n\nLet’s go with 1767, which represents State Farm. In the next code block, we subset each data.table to only those records with grcode==\"1767\", then create runoff triangles for each line of business:\nGRCODE = 1767 \ngrList = lapply(dfList, function(DF) DF[grcode==GRCODE,])\n\ntriData = foreach(\n    ii=1:length(grList), .inorder=TRUE, .errorhandling=\"stop\",\n    .final=function(ll) setNames(ll, names(grList))\n) %do% {\n    currLOB = names(grList)[[ii]]\n    DFInit = grList[[ii]]\n    DF = DFInit[dev&lt;=max(origin) - origin + 1,]\n    DF[,value:=incurloss - bulkloss]\n    as.triangle(DF[,.(origin, dev, value)])\n}\n\nTriangles for each lob are presented below:\n&gt; triData\n$wkcomp\n      dev\norigin      1      2      3      4      5      6      7      8      9     10\n  1988  50758  94150 106804 113733 120148 123986 127650 128622 129791 130625\n  1989  65423 110204 131509 140383 147011 150266 152264 155017 155979     NA\n  1990  68719 141501 165694 181789 189149 194315 196897 201780     NA     NA\n  1991  82409 165813 199016 213698 222994 229774 232413     NA     NA     NA\n  1992  97138 183451 208163 220275 227404 234320     NA     NA     NA     NA\n  1993 106508 167688 195533 212777 220063     NA     NA     NA     NA     NA\n  1994  93736 141067 160848 173457     NA     NA     NA     NA     NA     NA\n  1995  81309 116739 135447     NA     NA     NA     NA     NA     NA     NA\n  1996  66073  92365     NA     NA     NA     NA     NA     NA     NA     NA\n  1997  56003     NA     NA     NA     NA     NA     NA     NA     NA     NA\n\n$prodliab\n      dev\norigin   1   2   3    4    5    6    7    8    9   10\n  1988 696 737 881 1002 1379 1451 1741 1814 1818 1850\n  1989 428 351 617  718  761  788  797  802  804   NA\n  1990  57  77  92  135  197  235  250  263   NA   NA\n  1991  23 121 140  141  172  189  190   NA   NA   NA\n  1992  48 109 101  107  131  130   NA   NA   NA   NA\n  1993 119 133 150  211  278   NA   NA   NA   NA   NA\n  1994  21  60  59  100   NA   NA   NA   NA   NA   NA\n  1995  57  53  54   NA   NA   NA   NA   NA   NA   NA\n  1996  10  11  NA   NA   NA   NA   NA   NA   NA   NA\n  1997  20  NA  NA   NA   NA   NA   NA   NA   NA   NA\n\n$comauto\n      dev\norigin      1      2      3      4      5      6      7      8      9     10\n  1988 110231 152848 168137 180062 186150 188142 189352 191307 191867 194000\n  1989 121678 158218 176744 188127 192966 196104 199178 199655 200949     NA\n  1990 123376 175239 201955 214113 219988 223308 225841 226373     NA     NA\n  1991 117457 162601 183338 198607 203398 205870 206957     NA     NA     NA\n  1992 124611 166788 189771 201033 206826 212361     NA     NA     NA     NA\n  1993 137902 185952 209357 220428 226541     NA     NA     NA     NA     NA\n  1994 150582 194528 216205 231077     NA     NA     NA     NA     NA     NA\n  1995 150511 194730 215037     NA     NA     NA     NA     NA     NA     NA\n  1996 142301 184283     NA     NA     NA     NA     NA     NA     NA     NA\n  1997 143970     NA     NA     NA     NA     NA     NA     NA     NA     NA\n\n$othliab\n      dev\norigin     1      2      3      4      5      6      7      8      9     10\n  1988 22417  58806  77536 103003 112976 120070 124641 126954 127444 128036\n  1989 24740  55381  76543  97608 113777 124341 126171 128952 132618     NA\n  1990 19432  63891  94243 119678 124938 129990 133964 133949     NA     NA\n  1991 25821  84453 136275 159204 169820 172446 181744     NA     NA     NA\n  1992 38377  98045 138205 154554 171701 177467     NA     NA     NA     NA\n  1993 53001 150478 196273 224523 232681     NA     NA     NA     NA     NA\n  1994 50848 127767 187297 233255     NA     NA     NA     NA     NA     NA\n  1995 59140 149648 215701     NA     NA     NA     NA     NA     NA     NA\n  1996 71637 159561     NA     NA     NA     NA     NA     NA     NA     NA\n  1997 82937     NA     NA     NA     NA     NA     NA     NA     NA     NA\n\nNext, for each triangle call the BootChainLadder function (available in the ChainLadder library), running 5000 iterations and retaining only the total IBNR samples (discarding IBNR simulations by accident year). We replace simulated values less than or equal to 1 with 1:\nibnrSimsDF = foreach(\n    ii=1:length(triData), .inorder=TRUE, .errorhandling=\"stop\",\n    .combine=\"cbind.data.frame\", .final=setDT\n) %do% {\n    tri = triData[[ii]]\n    bcl = BootChainLadder(tri, R=5000, process.distr=\"gamma\")\n    lobSims = bcl$IBNR.Totals\n    lobSims[lobSims&lt;1] = 1\n    lobSims\n}\n\n# Set names of each column in simsDataDF to associated LOB.\nnames(ibnrSimsDF) = names(triData)\n\nInspecting ibnrSimsDF yields:\n&gt; head(ibnrSimsDF)\n     wkcomp prodliab  comauto  othliab\n1: 213282.5 309.9531 207524.1 836339.0\n2: 185281.3 453.1356 228032.9 876116.3\n3: 178462.7 263.7076 246759.9 633045.5\n4: 204928.1 169.7184 246953.0 641145.2\n5: 168382.3 408.6908 213764.4 717701.9\n6: 158486.8 194.0509 227606.5 711641.2\n\nibnrSimsDF contains 5000 rows, with the value in each row representing the total simulated reserve need across all accident years for the lob in question. It is possible to produce histograms of the simulated total IBNR using ggplot2. The code that follows generates a faceted quad-plot of the sampling distribution of total IBNR for each lob, with a vertical dashed red line marking the location of the distribution mean. We first transform ibnrSimsDF into a ggplot2-compatible format (which is ggDF):\n# Create faceted quad-plot representing sampling distribution of total IBNR.\nggDF = data.table::melt(\n    ibnrSimsDF, measure.vars=names(ibnrSimsDF), value.name=\"ibnr\", \n    variable.name=\"lob\", variable.factor=FALSE\n    )\n\n# Add mean.ibnr for huistogram overlay.\nggDF[,mean.ibnr:=mean(ibnr, na.rm=TRUE), by=\"lob\"]\n\nggplot(ggDF, aes(x=ibnr)) + \n    geom_histogram(bins=35, color=\"black\", fill=\"white\") + \n    geom_vline(\n        aes(xintercept=mean.ibnr), color=\"red\", linetype=\"dashed\", size=1\n        ) +\n    theme(\n        axis.title.y=element_blank(), axis.text.y=element_blank(),\n        axis.ticks.y=element_blank(), axis.title.x=element_blank()\n        ) + \n    scale_x_continuous(\n        labels=function(x) format(x, big.mark=\",\", scientific=FALSE)\n        ) +\n    facet_wrap(~lob, scales=\"free\")\n\nRunning the code above produces:\n\n\n\nQuantifying Total Reserve Variability\nIf all we are trying to do is determine the expected value of the reserve run-off, we can calculate the expected value for each lob separately and add all the expectations together. However, if we are trying to quantify a value other than the mean (such as the 75th percentile), we cannot simply sum across lines of business. If we do so, we will overstate the aggregate reserve need. The only time the sum of each lob’s 75th percentile would be appropriate for the aggregate reserve indication is when all lines are fully correlated with each other, which is highly unlikely.\nTo account for correlation between lobs, we rely on the rank correlation methodology described in Two Approaches to Calculating Correlated Reserve Indications Across Multiple Lines of Business. The methodology is carried out through a two-step process:\nIn the first step, a stochastic reserving technique is used to generate N possible reserve runoffs from each data triangle being analyzed (this is what we have in ibnrSimsDF). In the second step, a correlation matrix is specified, where individual elements of the correlation matrix describe the association between different pairs of lobs. With the correlation matrix \\(\\Sigma\\), carry out the following steps:\n\nCompute the Cholesky decomposition of \\(\\Sigma\\), that is, find the unique lower triangular matrix \\(A\\) such that \\(AA^{T} = \\Sigma\\).\nCompute \\(Z = (z_{1}, \\dots, z_{n})^{T}\\), a vector whose components are \\(n\\) independent standard normal variates (for our example, \\(n=5000\\).)\nLet \\(X = \\mu + AZ\\). Since \\(Z\\) represents independent draws from the standard normal distribution, the value of the mean vector \\(\\mu\\) is 0. Therefore correlated random draws are obtained by matrix multiplying \\(A\\) with \\(Z\\).\n\nFor the correlation matrix, we’ll initially assume no correlation between lobs (all off-diagonal elements=0). Later\nwe’ll compare estimated reserve need as a function of changing correlation.\nThe correlation matrix can be initialized as follows:\nsigma = matrix(\n    c(c(1, 0, 0, 0), \n      c(0, 1, 0, 0),\n      c(0, 0, 1, 0),\n      c(0, 0, 0, 1)),\n    nrow=4, \n    dimnames=list(names(ibnrSimsDF), names(ibnrSimsDF))\n    )\n\nWhich looks like the following:\n         wkcomp prodliab comauto othliab\nwkcomp        1        0       0       0\nprodliab      0        1       0       0\ncomauto       0        0       1       0\nothliab       0        0       0       1\n\nThe next code block implements steps 1-3:\nA = t(chol(sigma))\nZ = matrix(rnorm(ncol(A) * 5000), nrow=5000, ncol=ncol(A))\nX = Z %*% A\n\nChecking out the first few records of X yields:\n&gt; head(X)\n         wkcomp    prodliab    comauto    othliab\n[1,]  0.2256225  0.66492692  0.8239846 -1.5497317\n[2,]  0.1101583  0.60652201 -0.9572046 -0.5200923\n[3,] -0.5961369  0.13732270 -1.5355783  1.0622470\n[4,]  0.6863108 -1.02719480  0.1086142 -0.4941367\n[5,]  1.3918400  0.09805293  0.3412182 -0.1409186\n[6,]  0.5547157  1.57012447  0.1263973  0.7135559\n\nFor each column in X, we need to obtain the rank of each correlated random draw. This can be accomplished by running:\nrankX = foreach(ii=1:ncol(X), .combine=\"cbind\") %do% { rank(X[,ii]) }\ncolnames(rankX) = colnames(sigma)\nInspecting the first few records from rankX yields:\n&gt; head(rankX)\n     wkcomp prodliab comauto othliab\n[1,]   2971     3758    3975     293\n[2,]   2751     3658     856    1493\n[3,]   1393     2759     288    4335\n[4,]   3785      782    2746    1544\n[5,]   4619     2684    3178    2221\n[6,]   3569     4687    2784    3866\n\nTo prepare for the rank correlation step, we need to order the total IBNR simulations from smallest to largest within each lob column:\n# Order total bootstrapped ibnr samples from smallest to largest. \norderedSimsDF = foreach(\n    ii=1:length(names(ibnrSimsDF)), .combine=\"cbind.data.frame\",\n    .final=setDT\n) %do% {\n    currLOB = names(ibnrSimsDF)[[ii]]\n    sort(ibnrSimsDF[[currLOB]])\n}\n\nnames(orderedSimsDF) = names(ibnrSimsDF)\nThen for each rank in rankX, we lookup the corresponding position-wise element from orderedSimsDF. This ensures that the rank order correlations between lobs are the same as the correlations imposed on the random normal samples. For example, the first row of rankX is:\n  wkcomp prodliab  comauto  othliab \n    2971     3758     3975      293 \n\nThen using orderedSimsDF, we lookup the 2971st element under wkcomp, the 3758th element under prodliab, the 3975th element under comauto and the 293rd element under othliab. This can be accomplished as follows:\n# Get correlated IBNR samples.\ncorrIBNR = foreach(\n    ii=1:length(names(orderedSimsDF)), .combine=\"cbind\"\n) %do% {\n    currLOB = names(orderedSimsDF)[[ii]]\n    lobIndx = rankR[,currLOB]\n    orderedSimsDF[lobIndx, get(currLOB)]\n}\n\ncolnames(corrIBNR) = names(orderedSimsDF)\n\nFinally, we sum the correlated samples across lobs, resulting in a vector of values representing the aggregate reserve distribution:\ntotalIBNR = apply(corrIBNR, MARGIN=1, sum)\n\nPercentiles of the aggregate IBNR distribution can be obtained by calling:\n&gt; quantile(totalIBNR, c(.01, .25, .50, .75, .99))\n       1%       25%       50%       75%       99% \n 962340.6 1107900.3 1171348.8 1241553.0 1428743.0 \n\n\n\nComparing different values of \\(\\Sigma\\)\nWe’ve re-run the procedure described in the previous section for 5 different correlation matrices, assuming 0, .25, .50, .75 and .99 off-diagonal correlation, and combined the results into a single data.table qqDF. I then estimated the 1st, 25th, 50th, 75th and 99th percentile of each aggregate reserve distribution and created an exhibit comparing the distribution of each as a function of percentile. The code used to create the exhibit is given below:\n# ------------------------------------------------------------------------\n# Assume qqDF contains  1st, 25th, 50th, 75th and 99th percentile of the\n# aggregate IBNR distribution for off-diagonal correlation values of \n# 0, .25, .50, .75 and .99. The first few records of qqDF look like:\n#\n#          rho    x         y\n#       1:  0% 0.00  871243.8\n#       2:  0% 0.25 1107900.3\n#       3:  0% 0.50 1171348.8\n#       4:  0% 0.75 1241553.0\n#       5:  0% 0.99 1428743.0\n# \n# ------------------------------------------------------------------------\nggplot(qqDF, aes(x=x, y=y)) + geom_line(aes(color=rho), size=.5) + \n    scale_y_continuous(\n        labels=function(x) format(x, big.mark=\",\", scientific=FALSE)\n        ) + \n    theme(\n        axis.title.y=element_blank(), axis.title.x=element_blank()\n        ) + xlim(0, 1) + \n    ggtitle(\"Aggregate reserve distribution by off-diagonal correlation\")\n\nWhich produces:\n\n\nBy setting xlim(.50, 1), we can zoom in on the right-hand side of the distribution:\n\n\nWe see that around .50 mark on the x-axis, there is essntially no difference between 0% and 25% off-diagonal correlation assumption. However, as we move right along the x-axis, there’s a greater and greater discrepancy. when x=.99, the difference in the estimated total needed reserve is ~50,000, which represents approximately a 5% difference.\nA few take-aways:\n\nIf the goal is to determine the expected value of the reserve run-off, the expected value for each lob can be computed separately then added together.\nIf the aim is to quantify a value other than the mean such as the 75th percentile, we cannot simply sum across the lines of business, as this is akin to assuming full correlation between lines of business, which is unlikely and will overstate the aggregate reserve need.\nOff-diagonal correlation values do not need to be the same, but the matrix does need to be symmetric (identical values at \\(m_{i,j}\\) and \\(m_{j,i}\\)).\nSee Correlations between insurance lines of business: An illusion or a real phenomenon? Some methodological considerations for further context."
  },
  {
    "objectID": "posts/cnn-interpretability/cnn-interpretability.html",
    "href": "posts/cnn-interpretability/cnn-interpretability.html",
    "title": "Techniques for Convolutional Neural Network Interpretability",
    "section": "",
    "text": "Saliency maps for Convolutional Neural Networks are a technique used to visualize and understand the behavior of CNN models, particularly in tasks such as image classification, object detection and segmentation. They highlight which parts of an input (like an image) are most important for a model’s decision. They help with understanding how a model is making its predictions by showing the areas that contributed most to its output. The term “saliency” refers to the prominence of certain features, and in the context of deep learning saliency maps can reveal which pixels in an image most influenced the model’s classification task.\nIn this post, we walkthrough the process of generating saliency maps for holdout images using the CIFAR-10 dataset. To begin, we’ll train a simple Convolutional Neural Network model using PyTorch. After the model is trained, we will use it to generate saliency maps for a random selection of holdout images from the dataset. These saliency maps will help us visualize which parts of the images were most influential in the model’s predictions.\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom skimage.transform import rescale, resize\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\nnp.set_printoptions(suppress=True, precision=8, linewidth=1000)\npd.options.mode.chained_assignment = None\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\n\n\n# Number of images to process in each batch. \nbatch_size = 32\n\n\n# Specify CIFAR-10 classes.\nclasses = [\n    \"plane\", \"car\", \"bird\", \"cat\", \"deer\", \"dog\", \n    \"frog\", \"horse\", \"ship\", \"truck\"\n    ]\n\n\n# ImageNet transforms to normalize images. \ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n     )\n\n\n# Download CIFAR-10 training and validation data.\ntrain_ds = torchvision.datasets.CIFAR10(\n    root=\"./data\", train=True, download=True, transform=transform\n    )\nvalid_ds = torchvision.datasets.CIFAR10(\n    root=\"./data\", train=False, download=True, transform=transform\n    )\n\n# Create training and validation DataLoader instances. This is what gets \n# iterated over during training. \ntrain_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\nvalid_loader = DataLoader(valid_ds, batch_size=batch_size, shuffle=False)\n\nprint(f\"\\nNumber of training batches of size {batch_size}: {len(train_loader)}\")\n\nFiles already downloaded and verified\nFiles already downloaded and verified\n\nNumber of training batches of size 32: 1563\n\n\n\nDefine network, training and validation loops.\n\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BasicCNN(nn.Module):\n    def __init__(self, dropout=0):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n        self.fc1 = nn.Linear(in_features=16 * 5 * 5, out_features=120)\n        self.fc2 = nn.Linear(in_features=120, out_features=84)\n        self.fc3 = nn.Linear(in_features=84, out_features=10)\n        self.drp = nn.Dropout(p=dropout)\n        \n    def forward(self, X):\n        output = self.pool(F.relu(self.conv1(X)))\n        output = self.pool(F.relu(self.conv2(output)))\n        output = torch.flatten(output, 1)\n        output = F.relu(self.drp(self.fc1(output)))\n        output = F.relu(self.drp(self.fc2(output)))\n        output = self.fc3(output)\n        return output\n\n\ndef epoch_trainer(epoch, data_loader, model, loss_fn, optimizer, device, verbose=True):\n    \"\"\"\n    Execute a single training epoch. Return last batch training loss\n    and accuracy. \n    \"\"\"\n    loss, checkpoint_loss, correct, samples = 0.0, 0.0, 0, 0\n\n    # Put model in train mode.\n    model.train()\n\n    # Iterate over batches in data_loader. \n    for ii, (X, yactual) in enumerate(data_loader):\n\n        # Send datasets to device. \n        X, yactual = X.to(device), yactual.to(device)\n\n        # Zero out parameter gradients.\n        optimizer.zero_grad()\n\n        # Get model predictions (forward pass). \n        ypred = model(X)\n\n        # Compute loss. \n        loss_ii = loss_fn(ypred, yactual)\n\n        # Backpropagation and optimizer step. \n        loss_ii.backward()\n        optimizer.step()\n\n        # Update running_loss.\n        loss+=loss_ii.item()\n        correct+=(ypred.argmax(dim=1)==yactual).type(torch.float).sum().item()\n        samples+=yactual.size(dim=0)\n\n        # Print running_loss for every 100 mini-batches.\n        if ii % 250 == 0:\n            checkpoint_acc = correct / samples\n            checkpoint_loss = loss / 250\n\n            if verbose:\n                print(f\"\\t+ [train][epoch={epoch}, batch={ii}] loss = {checkpoint_loss:,.5f}, acc = {checkpoint_acc:.5f}.\")\n            \n            loss, correct, samples = 0.0, 0, 0\n\n    return checkpoint_loss, checkpoint_acc\n        \n\n\ndef epoch_validator(data_loader, model, loss_fn, optimizer, device):\n    \"\"\"\n    Execute a single validation epoch. Return average validation loss\n    and accuracy.\n    \"\"\"\n    valid_loss, correct = 0.0, 0\n\n    # Put model in validation mode.\n    model.eval()\n\n    with torch.no_grad():\n\n        for ii, (X, yactual) in enumerate(data_loader, start=1):\n\n            # Send dataset and target to device. \n            X, yactual = X.to(device), yactual.to(device)\n\n            # Get model predictions. \n            ypred = model(X)\n\n            # Compute loss and update valid_loss.\n            valid_loss+=loss_fn(ypred, yactual).item()\n\n            # Count number of correct class predictions.\n            correct+=(ypred.argmax(dim=1)==yactual).type(torch.float).sum().item()\n\n    loss, acc = valid_loss / ii, correct / len(data_loader.dataset)\n\n    return loss, acc\n\n\nConfigure training parameters.\n\n\nimport torch.optim as optim\n\n\n# Configuration ----------------------------------------------------------------\n\n# Number of epochs.\nn_epochs = 25\n\n# Learning rate.\nlr = 0.001\n\n# Momentum.\nmomentum = .90\n\n# Dropout.\ndropout = 0.0\n\n# ------------------------------------------------------------------------------\n\n# Check if gpu is available. If not, use cpu. \ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Initialize instance of BasicCNN. Put on device for completeness.\nmdl = BasicCNN(dropout=dropout).to(device)\n\n# Specify loss function.\nloss_fn = nn.CrossEntropyLoss()\n\n# Specify optimizer. \noptimizer = optim.SGD(mdl.parameters(), lr=lr, momentum=momentum)\n\nprint(f\"device: {device}\")\n\n\ndevice: cpu\n\n\n\nTrain model for 25 epochs.\n\n\nresults = []\n\nfor epoch in range(1, n_epochs + 1):\n\n    tloss, tacc = epoch_trainer(\n        epoch=epoch, data_loader=train_loader, model=mdl, loss_fn=loss_fn, \n        optimizer=optimizer, device=device, verbose=False\n        )\n    \n    vloss, vacc = epoch_validator(\n        data_loader=valid_loader, model=mdl, loss_fn=loss_fn, \n        optimizer=optimizer, device=device\n        )\n    \n    print(f\"[epoch={epoch}]: tloss={tloss:.5f}, tacc={tacc:.5f}, vloss={vloss:.5f}, vacc={vacc:.5f}.\")\n\n    # Append metrics to results.\n    results.append((tloss, tacc, vloss, vacc))\n\n\n[epoch=1]: tloss=2.11117, tacc=0.23975, vloss=2.01603, vacc=0.27400.\n[epoch=2]: tloss=1.71003, tacc=0.36488, vloss=1.68098, vacc=0.37990.\n[epoch=3]: tloss=1.54233, tacc=0.44200, vloss=1.53085, vacc=0.44330.\n[epoch=4]: tloss=1.43812, tacc=0.48037, vloss=1.42335, vacc=0.47970.\n[epoch=5]: tloss=1.37371, tacc=0.50675, vloss=1.34363, vacc=0.51350.\n[epoch=6]: tloss=1.31023, tacc=0.53050, vloss=1.30940, vacc=0.53170.\n[epoch=7]: tloss=1.25473, tacc=0.55137, vloss=1.26415, vacc=0.54980.\n[epoch=8]: tloss=1.19880, tacc=0.58000, vloss=1.20769, vacc=0.57130.\n[epoch=9]: tloss=1.14536, tacc=0.60250, vloss=1.23778, vacc=0.56390.\n[epoch=10]: tloss=1.10834, tacc=0.60925, vloss=1.15941, vacc=0.58830.\n[epoch=11]: tloss=1.10043, tacc=0.61250, vloss=1.12090, vacc=0.60210.\n[epoch=12]: tloss=1.06026, tacc=0.62600, vloss=1.11189, vacc=0.60440.\n[epoch=13]: tloss=1.04040, tacc=0.63150, vloss=1.10926, vacc=0.60780.\n[epoch=14]: tloss=0.98636, tacc=0.65163, vloss=1.08105, vacc=0.62470.\n[epoch=15]: tloss=0.99510, tacc=0.65763, vloss=1.11601, vacc=0.60920.\n[epoch=16]: tloss=0.93263, tacc=0.67000, vloss=1.04570, vacc=0.63480.\n[epoch=17]: tloss=0.92362, tacc=0.67175, vloss=1.03489, vacc=0.63770.\n[epoch=18]: tloss=0.89687, tacc=0.68788, vloss=1.03604, vacc=0.64380.\n[epoch=19]: tloss=0.88411, tacc=0.69112, vloss=1.07083, vacc=0.62580.\n[epoch=20]: tloss=0.84812, tacc=0.69963, vloss=1.03803, vacc=0.64410.\n[epoch=21]: tloss=0.82565, tacc=0.70750, vloss=1.01656, vacc=0.64570.\n[epoch=22]: tloss=0.81153, tacc=0.71813, vloss=1.02534, vacc=0.65150.\n[epoch=23]: tloss=0.80340, tacc=0.72525, vloss=1.04477, vacc=0.64590.\n[epoch=24]: tloss=0.80007, tacc=0.72150, vloss=1.01501, vacc=0.65290.\n[epoch=25]: tloss=0.78900, tacc=0.72288, vloss=1.03552, vacc=0.64820.\n\n\n\nThe gradient of the model’s output class score (or a specific neuron’s activation) with respect to the input image pixels is computed. The gradients are computed using backpropagation through the model. This involves propagating the gradients backward from the output layer to the input layer while computing the partial derivatives of the output with respect to the input. The magnitude of the gradients indicates how sensitive the model’s output is to changes in each pixel of the input image. Pixels with higher magnitude gradients are considered more salient and are highlighted in the saliency map. Once the gradients are computed, they are typically normalized and then visualized as a saliency map, where brighter regions correspond to more salient parts of the image and darker regions correspond to less salient parts.\nBelow is a function which computes the saliency map for a give image in CIFAR-10, and returns the map with the original image. This can also be accomplished using libraries such has captum, but for this demonstration we’re keeping it simple:\n\n\n\ndef get_saliency(tensor, model):\n    \"\"\"\n    Return saliency map and original image.\n    \"\"\"\n    # Put original image pixels on [0, 1].\n    img = tensor.squeeze().permute(1, 2, 0).detach().numpy()\n\n    # img = cv2.resize(img, (224, 224))\n    # img = resize(img, (32, 32))\n    # img /= np.max(np.abs(img), axis=0)\n\n    model.eval()\n    tensor.requires_grad_()\n    scores = model(tensor)\n\n    # Get the index corresponding to the maximum score and the maximum score itself.\n    score_max_index = scores.argmax()\n    score_max = scores[0, score_max_index]  \n\n    # Backward method on score_max performs the backward pass in the computation \n    # graph and calculates the gradient of score_max with respect to nodes in the \n    # graph.\n    score_max.backward()\n\n    # Saliency would be the gradient with respect to the input image. But note that \n    # the input image has 3 channels, R, G and B. To derive a single class saliency \n    # value for each pixel (i, j),  we take the maximum magnitude across all color \n    # channels.\n    saliency, _ = torch.max(tensor.grad.data.abs(), dim=1)\n\n    # Return original image along with saliency map. \n    return img, saliency\n\n\n\ndef plot_saliency(img, saliency, actual_label, predicted_label):\n    \"\"\"\n    Display original image along with saliency map.\n    \"\"\"\n    fig, ax = plt.subplots(1, 2, figsize=(6, 3.), tight_layout=True)\n    ax[0].imshow(img)\n    ax[0].axis(\"off\")\n    ax[0].set_title(\"original\", fontsize=8)\n    ax[1].imshow(saliency[0], cmap=plt.cm.hot)\n    ax[1].axis(\"off\")\n    ax[1].set_title(\"saliency map\", fontsize=8)\n    plt.suptitle(f\"actual={actual_label}, predicted={predicted_label}\")\n    plt.show()\n    \n\n\nFor each image in the batch, display the original image and saliency map side-by-side, along with actual and predicted classes.\n\n\n# Get images and labels for first batch of validation set.\nimages, labels = next(iter(valid_loader))\nyactual = labels.detach().numpy()\nypred = mdl(images).argmax(axis=1).detach().numpy()\n\n\n# For each image in current batch, show original along with saliency map\n# with actual and predicted class labels.\nfor ii in range(len(yactual)):\n    tensor = images[[ii]]\n    img, saliency = get_saliency(tensor, mdl)\n    actual_label, predicted_label = classes[yactual[ii]], classes[ypred[ii]]\n    plot_saliency(img, saliency, actual_label, predicted_label)\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\n\nWe can observe which regions are highlighted in the saliency map for each image, and start to get an idea of which image characteristics result in a particular prediction. This is an informative diagnostic and a valuable technique to use when attempting to explain CNN model output."
  },
  {
    "objectID": "posts/c-libs-python/c-libs-python.html",
    "href": "posts/c-libs-python/c-libs-python.html",
    "title": "Accessing C Library Functions from Python",
    "section": "",
    "text": "ctypes is part of the Python standard library which provides C compatible data types and allows calling functions in shared libraries. It can be used to wrap libraries written in compiled languages from Python.\nIn this post, we’ll demonstrate how to call functions written in C from Python using ctypes. The functions take as input an array of normal variates with specified mean and variance, then compute the normal CDF for each input array element. The sample code also demonstrates how to link to the C math library during compilation."
  },
  {
    "objectID": "posts/c-libs-python/c-libs-python.html#calling-c-functions-from-python",
    "href": "posts/c-libs-python/c-libs-python.html#calling-c-functions-from-python",
    "title": "Accessing C Library Functions from Python",
    "section": "Calling C Functions from Python",
    "text": "Calling C Functions from Python\nPrior to calling our C functions from Python, we need to specify the parameter and return types of norm_cdf and cdf_array. In addition, we need to coerce any Python data types that are passed to the library functions into C-compatible data types. This is demonstrated below, with each section commented to make it easier to follow along. This is a Python file named as norm_main.py:\n#!/usr/bin/env python\n\"\"\"\nnorm_main.py\n\nCalls 2 functions from the compiled C library `norm.so`:\n\n    [+] double norm(double x, double mu, double sigma)\n\n    [+] void cdf_array(double mu, double sigma, int n,\n                     double* input_array, double* output_array)\n\n\"\"\"\nimport ctypes\nimport numpy as np\nfrom scipy.stats import norm\n\nnp.set_printoptions(suppress=True)\n\n# Provide full path to shared library.\nLIB_PATH = \"norm.so\"\n\n# Bind reference to shared library `norm.so`.\nnormlib = ctypes.cdll.LoadLibrary(LIB_PATH)\n\n# Specify argument datatypes for norm_cdf and cdf_array.\nnormlib.norm_cdf.argtypes  = [\n    ctypes.c_double, ctypes.c_double, ctypes.c_double\n    ]\n    \nnormlib.cdf_array.argtypes = [\n    ctypes.c_double, ctypes.c_double, ctypes.c_int,\n    ctypes.POINTER(ctypes.c_double), ctypes.POINTER(ctypes.c_double)\n    ]\n\n# Specify return datatypes for norm_cdf and cdf_array (cdf_array declared as void).\nnormlib.norm_cdf.restype  = ctypes.c_double\nnormlib.cdf_array.restype = None\n\n# Use scipy.stats to generate 10 standard normal random variates. This will \n# be `input_arr`. We also initialize `output_arr` to all zeros, and set the \n# random seed in numpy for reproducibility.\nnp.random.seed(516)\nmu, sigma, n = 0., 1., 10\ninput_arr = norm.rvs(loc=mu, scale=sigma, size=n)\noutput_arr = np.zeros(n, np.float_)\n\n\n# Initialize ctypes-compatible versions of mu, sigma, n, input_arr and output_arr.\nct_mu = ctypes.c_double(mu)\nct_sigma = ctypes.c_double(sigma)\nct_n = ctypes.c_int(n)\nct_input_arr = np.ctypeslib.as_ctypes(input_arr)\nct_output_arr = np.ctypeslib.as_ctypes(output_arr)\n\n\nprint(f\"\\nNormal variates w/ mean {mu} and standard deviation {sigma}:\\n{input_arr}\")\nprint(f\"\\nOutput_arr before passing to cdf_array:\\n{output_arr}\")\n\n# Call `normlib.cdf_array` from C library.\nnormlib.cdf_array(ct_mu, ct_sigma, ct_n, ct_input_arr, ct_output_arr)\n\nprint(f\"\\nOutput_arr after passing to cdf_array:\\n\\{output_arr}\")\n\n# Compare results returned by cdf_array to scipy's norm.cdf.\nspcdfs = norm.cdf(input_arr, loc=mu, scale=sigma)\nprint(f\"\\nscipy-evaluated CDFs:\\n\\{spcdfs}\")\nTo summarize, we read in norm.so, specify the parameter and return data types for the library functions, then call cdf_array. In the last few lines, we compare the output of cdf_array with norm.cdf from scipy.stats, and find the results to be identical.\nNote that we are not copying data, but simply passing pointers to the data from Python to C. In C, the data pointed to is operated on, which means we do not need to pass any data back. This explains why cdf_array’s return type is void.\nAlso note that calculating normal CDFs for a sequence of normal variates can be accomplished more efficiently using Scipy. This particular example was chosen to demonstrate non-trival ctypes extensibility, but the example itself should be considered a demonstration of the method, not an optimal approach for computing normal CDFs.\nThe following terminal capture verifies that CDFs calculated with cdf_array and scipy are the same:\n\nFinally, we compare CDFs for normal variates generated from a non-standard normal distribution. The only change we need to make in norm_main.py is to update mu and sigma. Setting mu=2.5 and sigma=5 yields:"
  },
  {
    "objectID": "posts/beta-function/beta-function.html",
    "href": "posts/beta-function/beta-function.html",
    "title": "The Beta Function and its Variants",
    "section": "",
    "text": "This post highlights variants of the beta function and includes an implementation that reproduces CDF output from scipy.stats.nbinom for a given parameterization.\nNo discussion of the beta function would be complete without first introducing the gamma function. The gamma function is an extension of the factorial function, with its argument shifted down by 1, to real and complex numbers:\n\\[\n\\Gamma(x) = \\int_{0}^{\\infty} t^{x-1} e^{-t} dt\n\\]\nIf \\(n\\) is a positive integer, the gamma function reduces to:\n\\[\n\\Gamma(n) = (n-1)!.\n\\]\nThe beta function is given by:\n\\[\nB(a, b) = \\int_{0}^{1} t^{a-1} (1-t)^{b-1} dt, \\quad a, b \\in \\mathbb{R}^{\\geq}.\n\\]\nThe beta function can be represented in terms of the gamma function as follows:\n\\[\nB(a, b) = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}\n\\]\nWhen \\(a, b \\in \\mathbb{Z}^{\\geq}\\), the expression simplifies to:\n\\[\nB(a, b) = \\frac{(a-1)!(b-1)!}{(a+b-1)!}.\n\\]\nIn the first beta function expression, the limits of integration were \\((0,1)\\). The incomplete beta function is a generalization of the beta function which allows the upper limit of integration to take on values within the range \\([0,x]\\). Symbolically, the incomplete beta function is represented as:\n\\[\nB(x; a, b) = \\int_{0}^{x} t^{a-1} (1-t)^{b-1} dt.\n\\]\nWhen \\(x=1\\), the incomplete beta function and the beta function are equivalent. Put another way, the beta function is the incomplete beta function evaluated at \\(x=1\\). Having described the beta function and the incomplete beta function, the regularized incomplete beta function is introduced, which is also referred to as the regularized beta function. It is defined as the ratio of the incomplete beta function to the beta function, evaluated at \\(x\\):\n\\[\nI_{x}(a,b) = \\frac{B(x; a,b)}{B(a,b)}\n\\]\nThe regularized beta function is the cumulative distribution function for the beta distribution, which can be used to calculate the CDF for both the negative binomial and binomial distributions. For a binomial random variable \\(X\\), to determine the probability of \\(k\\) or fewer successes in \\(n\\) independent trials, \\(k \\leq n\\), the CDF can be expressed by:\n\\[\nF(x) = \\sum_{i=0}^{n} \\binom{n}{i} p^{i} (1-p)^{n-i} = I_{1-p}(n-k,1+k).\n\\]\nTo demonstrate, assume \\(p=.25\\), \\(n=5\\). The probability of 3 or fewer successes is conventionally computed via binomial expansion as:\n\\[\nP[X \\leq 3] = \\binom{5}{0} .25^{0} .75^{5} + \\binom{5}{1} .25^{1} .75^{4} + \\binom{5}{2} .25^{2} .75^{3} + \\binom{5}{3} .25^{3} .75^{2} = \\mathbf{0.984375}.\n\\]\nEquivalently, leveraging the regularized incomplete beta function yields:\n\\[\nP[X \\leq 3] = I_{.75}(2, 4) = \\mathbf{0.984375}.\n\\]\nFor the negative binomial distribution, assuming the common \\(r,k\\) parameterization in which the PDF is given by\n\\[\nP(X=k) = \\binom{k+r-1}{k} p^{k} (1-p)^{r} \\quad \\text{for} k = 0, 1, 2, \\cdots.\n\\]\nThe negative binomial CDF can be computed using the regularized incomplete beta function:\n\\[\nP(X \\leq k) = 1 - I_{p}(k+1, r).\n\\]\n\nImplementation\nIn what follows, a function to compute the CDF of the negative binomial distribution using functions available in scipy.special is demonstrated. Results will be compared to scipy.stats.nbinom.cdf for a set of inputs to assess the correctness of the implementation.\n\n\"\"\"\nCalculation of negative binomial CDF using regularized incomplete \nbeta function. Note that the call signature for nbinom.cdf is:\n\n    nbinom.cdf(&lt;nbr_failures&gt;, &lt;nbr_successes&gt;, &lt;prob_success&gt;)\n\"\"\"\nfrom scipy.special import betainc\nfrom scipy.stats import nbinom\nimport numpy as np\n\n# Vectorization of nbinom.pmf.\nnb_pdfs = np.vectorize(nbinom.pmf)\n\n\ndef nb_cdf(k, n,  p):\n    \"\"\"\n    Negative binomial CDF using the regularized incomplete beta function.\n    \"\"\"\n    I = betainc(n + 1, k, p)\n    return(1 - I)\n\n\nWe test the implementation against the actual negative binomial CDF:\n\n\nnbr_successes, nbr_failures, prob_success = 10, 10, .5\nsum_pdfs = nb_pdfs(np.arange(nbr_failures + 1), nbr_successes, prob_success).sum()\nactual_cdf = nbinom.cdf(nbr_failures, nbr_successes, prob_success)\nnew_cdf = nb_cdf(nbr_failures, nbr_successes, prob_success)\n\nprint(f\"sum_pdfs: {sum_pdfs:.6f}\")\nprint(f\"actual_cdf: {actual_cdf:.6f}\")\nprint(f\"new_cdf: {new_cdf:.6f}\")\n\nsum_pdfs: 0.588099\nactual_cdf: 0.588099\nnew_cdf: 0.588099\n\n\nAs expected, all three approaches arrive at the same value for the negative binomial CDF."
  },
  {
    "objectID": "posts/backprop-python/backprop-python.html",
    "href": "posts/backprop-python/backprop-python.html",
    "title": "Backpropagation for Fully-Connected Neural Networks",
    "section": "",
    "text": "Backpropagation is a key algorithm used in training fully connected neural networks, also known as feed-forward neural networks. In this algorithm, the network’s output error is propagated backward, layer by layer, to adjust the weights of connections between neurons.\n\nThe process starts by comparing the network’s output to the desired output, calculating the error. Then, starting from the output layer and moving backward, the algorithm computes the gradients of the error with respect to each weight in the network using the chain rule of calculus. These gradients indicate how much each weight contributes to the error.\nNext, the weights are updated using gradient descent, where they are adjusted in the direction that minimizes the error. This adjustment is proportional to the gradient and a predefined learning rate, ensuring the network converges towards a solution. Backpropagation continues iteratively over the training data until the network’s performance reaches a satisfactory level or a predetermined number of iterations is reached.\nOverall, backpropagation efficiently adjusts the weights of a fully connected network, enabling it to learn complex relationships between input and output data through iterative optimization of the network’s parameters.\nIn what follows, we walkthrough the mathematics and pseudocode required to train a 2-layer fully connected network for a classification task.\n\nForward Pass\nIn the following, superscripts represent the layer associated with each variable:\n\n\\(X = A^{(0)}\\): Input data having dimension n-by-f, where n is the number of samples and f the number of features. For a batch of 32 MNIST samples, \\(X\\) would have dimension (32, 784).\n\\(y\\): Target variable. classifying a single digit from MINST, a vector populated with 0s and 1s indicating the ground truth label for the sample (8 or not 8). Has the same length as the first dimension of \\(X\\).\n\\(W^{(l)}\\): Trainable weights. Projects previous layer activations to lower dimensional representation. Again referring to the first set of weights for a batch of 32 MNIST samples, \\(W^{(1)}\\)’s first dimension will match the second dimension of the activations from the previous layer (784), and \\(W^{(1)}\\)’s second dimension will be some lower dimension, say 256. \\(W^{(1)}\\) will therefore have dimension (784, 256).\n\\(b^{(l)}\\): Bias term, a one-dimensional vector associated with each hidden layer having length equal to the second dimension of the hidden layer. \\(b^{(1)}\\) will have dimension (256,).\n\\(Z^{(l)} = A^{(l-1)} W^{(l)} + b^{(l)}\\): Output of layer \\(l\\), which is the matrix product of the previous layer activations \\(A^{(l-1)}\\) and current layer weights (plus bias term).\n\\(A^{(l)} = \\sigma(Z^{(l)})\\): Activations associated with layer \\(l\\). Passes \\(Z^{(l)}\\) through a non-linearity such as sigmoid or ReLU.\n\nMore concretely, assume a 2-layer fully-connected neural network with one hidden layer of size 256, through which a dataset of dimension 32-by-784 is passed to predict whether each of the 32 images is an 8 or not. The forward pass looks like:\n\nRandomly initialize \\(W^{(1)}\\) (784x256), \\(W^{(2)}\\) (256x1), \\(b^{(1)}\\) (256x1) and \\(b^{(2)}\\) (1x1)\n\\(X = A^{(0)}\\hspace{.75em}\\) (32x784)\n\\(Z^{(1)} = A^{(0)} W^{(1)} + b^{(1)}\\hspace{.75em}\\) (32x256)\n\\(A^{1} = \\sigma(Z^{(1)})\\hspace{.75em}\\) (32x256)\n\\(Z^{(2)} = A^{(1)} W^{(2)} + b^{(2)}\\hspace{.75em}\\) (32x1)\n\\(\\hat{y} = A^{(2)} = \\sigma(Z^{(2)})\\hspace{.75em}\\) (32x1)\n\nThe final output, \\(\\hat{y}\\), represents the probability that each sample is the number 8 or not.\nWith the actual labels \\(y\\) and our predicted probabilities \\(\\hat{y}\\), we can define our loss function, the cross-entropy loss for binary classification:\n\\[\n\\mathcal{L} = -\\frac{1}{n}\\big(y \\times \\mathrm{log}(\\hat{y}) - (1 - y)\\times \\mathrm{log}(1 - \\hat{y})\\big)\n\\]\n\n\nBackward Pass (Backpropagation)\nThe goal of backpropagation is to compute the partial derivatives of the loss function \\(\\mathcal{L}\\) with respect to any weight \\(W\\) or \\(b\\) in the network. In order to update our weights, we need to take derivatives of \\(\\mathcal{L}\\) w.r.t. \\(W\\) and \\(b\\), then update \\(W\\) and \\(b\\) using the derivatives. Backpropagation starts by taking the derivative of the loss function. We first compute the derivatives of the loss function w.r.t. \\(W^{(2)}\\) and \\(b^{(2)}\\). Here we make use of the chain rule:\n\\[\n\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial W^{(2)}} &= \\frac{\\partial \\mathcal{L}}{\\partial A^{(2)}} \\cdot \\frac{\\partial A^{(2)}}{\\partial Z^{(2)}} \\cdot \\frac{\\partial Z^{(2)}}{\\partial W^{(2)}}\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial b^{(2)}} &= \\frac{\\partial \\mathcal{L}}{\\partial A^{(2)}} \\cdot \\frac{\\partial A^{(2)}}{\\partial Z^{(2)}} \\cdot \\frac{\\partial Z^{(2)}}{\\partial b^{(2)}}\n\\end{align*}\n\\]\nOnce we have \\(\\frac{\\partial \\mathcal{L}}{\\partial W^{(2)}}\\) and \\(\\frac{\\partial \\mathcal{L}}{\\partial b^{(2)}}\\), \\(W^{(2)}\\) and \\(b^{(2)}\\) are updated as follows:\n\\[\n\\begin{align*}\nW^{(2)} &:= W^{(2)} - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W^{(2)}}\\\\\nb^{(2)} &:= b^{(2)} - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial b^{(2)}}\n\\end{align*}\n\\]\nfor some learning rate \\(\\alpha\\). This holds for all layers. For given layer \\(i\\), the update rule for \\(W^{(i)}\\) and \\(b^{(i)}\\) is:\n\\[\n\\begin{align*}\nW^{(i)} &:= W^{(i)} - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W^{(i)}}\\\\\nb^{(i)} &:= b^{(i)} - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial b^{(i)}}.\n\\end{align*}\n\\]\nLet’s start with unpacking \\(\\frac{\\partial \\mathcal{L}}{\\partial W^{(2)}}\\). The first entry on the r.h.s., \\(\\frac{\\partial \\mathcal{L}}{\\partial A^{(2)}}\\), represents the derivative of the loss function w.r.t. \\(A^{(2)} = \\hat{y}\\), which is\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial A^{(2)}} = -\\frac{y}{A^{(2)}} + \\frac{1 - y}{1 - A^{(2)}}.\n\\]\nThe second term on the r.h.s., \\(\\frac{\\partial A^{(2)}}{\\partial Z^{(2)}}\\), is the derivative of the sigmoid activation (\\(A^{(2)} = \\sigma(Z^{(2)})\\)). The derivative of the sigmoid function is given by\n\\[\n\\frac{d\\sigma}{dx} = \\sigma(x)(1-\\sigma(x)),\n\\]\ntherefore \\(\\frac{\\partial A^{(2)}}{\\partial Z^{(2)}}\\) is given by\n\\[\n\\frac{\\partial A^{(2)}}{\\partial Z^{(2)}} = A^{(2)}(1 - A^{(2)}).\n\\]\nFor the third term on the r.h.s., \\(\\frac{\\partial Z^{(2)}}{\\partial W^{(2)}}\\), recall that \\(Z^{(2)} = A^{(1)} W^{(2)} + b^{(2)}\\). Therefore\n\\[\n\\frac{\\partial Z^{(2)}}{\\partial W^{(2)}} = A^{(1)}.\n\\]\nFinally, we have\n\\[\n\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial W^{(2)}} &= \\frac{\\partial \\mathcal{L}}{\\partial A^{(2)}} \\cdot \\frac{\\partial A^{(2)}}{\\partial Z^{(2)}} \\cdot \\frac{\\partial Z^{(2)}}{\\partial W^{(2)}}\\\\\n&= \\Big(-\\frac{y}{A^{(2)}} + \\frac{1 - y}{1 - A^{(2)}}\\Big) \\cdot \\big(A^{(2)}(1 - A^{(2)})\\big) \\cdot \\big(A^{(1)}\\big)\\\\\n&= (A^{(2)} - y) \\cdot A^{(1)}.\n\\end{align*}\n\\]\nAs a notational convenience, we define \\(\\frac{\\partial \\mathcal{L}}{\\partial Z^{(2)}}\\):\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial Z^{(2)}} = \\frac{\\partial \\mathcal{L}}{\\partial A^{(2)}} \\cdot \\frac{\\partial A^{(2)}}{\\partial Z^{(2)}} = (A^{(2)} - y).\n\\]\nThis way, \\(\\frac{\\partial \\mathcal{L}}{\\partial W^{(2)}}\\) can be expressed as\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial W^{(2)}} = \\frac{\\partial \\mathcal{L}}{\\partial Z^{(2)}} \\cdot A^{(1)}.\n\\]\nWe proceed in a similar fashion for \\(\\frac{\\partial \\mathcal{L}}{\\partial b^{(2)}}\\):\n\\[\n\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial b^{(2)}} &= \\frac{\\partial \\mathcal{L}}{\\partial A^{(2)}} \\cdot \\frac{\\partial A^{(2)}}{\\partial Z^{(2)}} \\cdot \\frac{\\partial Z^{(2)}}{\\partial b^{(2)}}\\\\\n&= \\frac{\\partial \\mathcal{L}}{\\partial Z^{(2)}} \\cdot \\frac{\\partial Z^{(2)}}{\\partial b^{(2)}}\\\\\n&= (A^{(2)} - y),\n\\end{align*}\n\\]\nsince \\(\\frac{\\partial Z^{(2)}}{\\partial b^{(2)}} = 1\\).\nFor the first layer we re-use many of these calculations, but for new terms on the r.h.s., we employ the chain rule in the same way. For reference, restate the terms from the forward pass:\n\\[\n\\begin{align*}\nA^{(0)} &= X\\\\\nZ^{(1)} &= A^{(0)} W^{(1)} + b^{(1)}\\\\\nA^{1} &= \\sigma(Z^{(1)})\\\\\nZ^{(2)} &= A^{(1)} W^{(2)} + b^{(2)}\\\\\nA^{(2)} &= \\hat{y} = \\sigma(Z^{(2)})\n\\end{align*}\n\\]\nWe next consider \\(\\frac{\\partial \\mathcal{L}}{\\partial W^{(1)}}\\):\n\\[\n\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial W^{(1)}} &= \\frac{\\partial \\mathcal{L}}{\\partial A^{(2)}} \\cdot \\frac{\\partial A^{(2)}}{\\partial Z^{(2)}} \\cdot \\frac{\\partial Z^{(2)}}{\\partial A^{(1)}} \\cdot \\frac{\\partial A^{(1)}}{\\partial Z^{(1)}} \\cdot \\frac{\\partial Z^{(1)}}{\\partial W^{(1)}}\\\\\n&= \\frac{\\partial \\mathcal{L}}{\\partial Z^{(2)}} \\cdot \\frac{\\partial Z^{(2)}}{\\partial A^{(1)}} \\cdot \\frac{\\partial A^{(1)}}{\\partial Z^{(1)}} \\cdot \\frac{\\partial Z^{(1)}}{\\partial W^{(1)}}\n\\end{align*}\n\\]\nConsidering each term on the r.h.s:\n\\[\n\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial Z^{(2)}} &= (A^{(2)} - y)\\\\\n\\frac{\\partial Z^{(2)}}{\\partial A^{(1)}} &= W^{(2)}\\\\\n\\frac{\\partial A^{(1)}}{\\partial Z^{(1)}} &= \\sigma(Z^{(1)}) (1 - \\sigma(Z^{(1)})) = A^{(1)}(1 - A^{(1)})\\\\\n\\frac{\\partial Z^{(1)}}{\\partial W^{(1)}} &= A^{(0)} = X\n\\end{align*}\n\\]\nResulting in:\n\\[\n\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial W^{(1)}} &= (A^{(2)} - y) \\cdot W^{(2)} \\cdot \\big(A^{(1)}(1 - A^{(1)})\\big) \\cdot A^{(0)}\\\\\n&= \\frac{\\partial \\mathcal{L}}{\\partial Z^{(2)}}\\cdot W^{(2)} \\cdot \\big(A^{(1)}(1 - A^{(1)})\\big) \\cdot A^{(0)}\\\\\n\\end{align*}\n\\]\nAs before, we define \\(\\frac{\\partial \\mathcal{L}}{\\partial Z^{(1)}}\\) as\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial Z^{(1)}} = \\frac{\\partial \\mathcal{L}}{\\partial Z^{(2)}}\\cdot W^{(2)} \\cdot \\big(A^{(1)}(1 - A^{(1)})\\big),\n\\]\nwhich allows us to write \\(\\frac{\\partial \\mathcal{L}}{\\partial W^{(1)}}\\) as\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial W^{(1)}} = \\frac{\\partial \\mathcal{L}}{\\partial Z^{(1)}} \\cdot A^{(0)}.\n\\]\nSimilarly for \\(b^{(1)}\\):\n\\[\n\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial b^{(1)}} &= \\frac{\\partial \\mathcal{L}}{\\partial A^{(2)}} \\cdot \\frac{\\partial A^{(2)}}{\\partial Z^{(2)}} \\cdot \\frac{\\partial Z^{(2)}}{\\partial A^{(1)}} \\cdot \\frac{\\partial A^{(1)}}{\\partial Z^{(1)}} \\cdot \\frac{\\partial Z^{(1)}}{\\partial b^{(1)}}\\\\\n&= \\frac{\\partial \\mathcal{L}}{\\partial Z^{(2)}} \\cdot \\frac{\\partial Z^{(2)}}{\\partial A^{(1)}} \\cdot \\frac{\\partial A^{(1)}}{\\partial Z^{(1)}} \\cdot \\frac{\\partial Z^{(1)}}{\\partial b^{(1)}}.\n\\end{align*}\n\\]\nConsidering each term on the r.h.s:\n\\[\n\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial Z^{(2)}} &= (A^{(2)} - y)\\\\\n\\frac{\\partial Z^{(2)}}{\\partial A^{(1)}} &= W^{(2)}\\\\\n\\frac{\\partial A^{(1)}}{\\partial Z^{(1)}} &= \\sigma(Z^{(1)}) (1 - \\sigma(Z^{(1)})) = A^{(1)}(1 - A^{(1)})\\\\\n\\frac{\\partial Z^{(1)}}{\\partial b^{(1)}} &= 1\n\\end{align*}\n\\]\nTherefore\n\\[\n\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial b^{(1)}} &= (A^{(2)} - y) \\cdot W^{(2)} \\cdot \\big(A^{(1)}(1 - A^{(1)})\\big) \\cdot 1\\\\\n&= \\frac{\\partial \\mathcal{L}}{\\partial Z^{(2)}} \\cdot W^{(2)} \\cdot \\big(A^{(1)}(1 - A^{(1)})\\big)\\\\\n&= \\frac{\\partial \\mathcal{L}}{\\partial Z^{(1)}}.\n\\end{align*}\n\\]\nTo complete the backpropagation algorithm, it is necessary to define \\(\\frac{\\partial \\mathcal{L}}{\\partial A^{(1)}}\\):\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial A^{(1)}} = \\frac{\\partial \\mathcal{L}}{\\partial A^{(2)}} \\cdot \\frac{\\partial A^{(2)}}{\\partial Z^{(2)}} \\cdot \\frac{\\partial Z^{(2)}}{\\partial A^{(1)}} = \\frac{\\partial \\mathcal{L}}{\\partial Z^{(2)}} \\cdot W^{(2)}.\n\\]\nAssume \\(X\\) is a 32x784 batch of MNIST images, and our network has one hidden layer of size 256. Our task is to identify which digit 0-9 a sample most closely resembles. We first declare a number of functions, then implement the forward and backward passes along with weights update.\n\n\nimport numpy as np\n\n\ndef sigmoid(X):\n    \"\"\"\n    Compute the sigmoid activation for the input.\n    \"\"\"\n    return 1 / (1 + np.exp(-X))\n\n\ndef sigmoid_dev(X):\n    \"\"\"\n    The analytical derivative of sigmoid function at X.\n    \"\"\"\n    return sigmoid(X) * (1 - sigmoid(X))\n\n\ndef softmax(scores):\n    \"\"\"\n    Compute softmax scores given the raw output from the model.\n    Returns softmax probabilities (N, num_classes).\n    \"\"\"\n    numer = np.exp(scores - scores.max(axis=1, keepdims=True))\n    denom = numer.sum(axis=1, keepdims=True)\n    return np.divide(numer, denom)\n\n\ndef cross_entropy_loss(ypred, yactual):\n    \"\"\"\n    Compute Cross-Entropy Loss based on prediction of the network and labels\n    \"\"\"\n    yactual = np.asarray(yactual)\n    ypred = ypred[np.arange(len(yactual)), yactual]\n    return -np.mean(np.log(ypred))\n\n\ndef compute_accuracy(ypred, yactual):\n    \"\"\"\n    Compute the accuracy of current batch.\n    \"\"\"\n    yactual = np.asarray(yactual)\n    yhat = np.argmax(ypred, axis=1)\n    return (y == yhat).sum() / y.shape[0]\n\n\n\n# Stand in for batch of 32 MNIST images. \nX = np.random.randint(0, 256, size=(32, 784))\ny = np.random.randint(0, 10, size=32)\n\n# Reshape labels to 32 x 10. \nY = np.zeros((32, 10))\nY[np.arange(X.shape[0]), y] = 1  # (32, 10)\n\n# Learning rate.\nalpha = .05 \n\n# Initialize weights.\nb1 = np.zeros(256)\nb2 = np.zeros(10)\nW1 = 0.001 * np.random.randn(784, 256)\nW2 = 0.001 * np.random.randn(256, 10)\n\n# Forward pass.\nZ1 = X @ W1 + b1   # (32, 256)\nA1 = sigmoid(Z1)   # (32, 256)\nZ2 = A1 @ W2 + b2  # (32, 10)\nA2 = softmax(Z2)   # (32, 10)\n\n# Compute loss and accuracy.\nloss = cross_entropy_loss(A2, y)\naccuracy = compute_accuracy(A2, y)\n\n# Backward pass.\ndZ2 = A2 - Y                            # (32, 10)\ndW2 = (A1.T @ dZ2) / 32                 # (256, 10)\ndb2 = np.sum(dZ2, axis=0) / 32          # (10,)\ndA1 = dZ2 @ W2.T                        # (32, 256)\ndZ1 = np.multiply(dA1, sigmoid_dev(Z1)) # (32, 256)\ndW1 = (X.T @ dZ1) / 32                  # (784, 256)\ndb1 = np.sum(dZ1, axis=0) / 32          # (256,)\n\n# Update weights.\nW2 = W2 - alpha * dW2\nb2 = b2 - alpha * db2\nW1 = W1 - alpha * dW1\nb1 = b1 - alpha * db1\n\nThe code starting with the forward pass would be iterated over a set of batches for a pre-determined number of epochs. The final weights would then be used for inference."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hello, my name is James Triveri. I’m a Data Scientist at The Mutual Group, a modern, scalable solution to mutual insurance carriers intended to enhance their operational efficiency. My primary interests are Machine Learning/Deep Learning, Numerical Linear Algebra, Geospatial Data Science and Scientific Computing. I also serve as a Computer Science instructor at Des Moines Area Community College where I teach introductory programming and Data Science courses.\nI’m an ardent supporter of Free Software, as well as a long-time user and advocate of the Python programming language. I’m an active contributor to Open Source, and maintain a few projects of my own. You can check out my GitHub page here.\nI graduated from Augustana College in Rock Island, Illinois where I studied Mathematics and Physics and received an M.S. in Computer Science from the Georgia Institute of Technology specializing in Machine Learning.\nMy favorite animal is the sloth (the Bradypodidae variety). My favorite guitarists are Duane Allman, Mick Taylor, Jerry Garcia and Tony Rice.\nMy CV is available here.\nThanks for checking in!\nJT"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Pleasure of Finding Things Out",
    "section": "",
    "text": "Determining Distance to Coastline for Policy Locations Using GeoPandas\n\n\n\n\n\n\nPython\n\n\nGeospatial\n\n\n\nDetermining distance to coastline for policy locations using GeoPandas\n\n\n\n\n\nOct 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nA Data-Driven Approach to Ranking Teams in Uneven Paired Competition\n\n\n\n\n\n\nMachine Learning\n\n\nPython\n\n\n\nA data-driven approach to ranking teams in uneven paired competition\n\n\n\n\n\nOct 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTechniques for Convolutional Neural Network Interpretability\n\n\n\n\n\n\nMachine Learning\n\n\nPython\n\n\n\nTechniques for convolutional neural network interpretability\n\n\n\n\n\nOct 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGenerating Correlated Boostrap Reserve Estimates\n\n\n\n\n\n\nStatistical Modeling\n\n\nR\n\n\n\nGenerating correlated bootstrap reserve estimates with R\n\n\n\n\n\nSep 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Markov Chain Monte Carlo - The Metropolis-Hastings Algorithm\n\n\n\n\n\n\nStatistical Modeling\n\n\nPython\n\n\n\nIntroduction to markov chain monte carlo - The Metropolis-Hastings algorithm\n\n\n\n\n\nSep 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a Blog with Quarto and GitHub Pages\n\n\n\n\n\n\nOther\n\n\n\nCreating a blog with quarto and github pages\n\n\n\n\n\nSep 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Folium\n\n\n\n\n\n\nPython\n\n\nGeospatial\n\n\n\nIntroduction to Folium\n\n\n\n\n\nSep 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with Geospatial Vector Data\n\n\n\n\n\n\nPython\n\n\nGeospatial\n\n\n\nWorking with geospatial vector data\n\n\n\n\n\nAug 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nComposite Estimators in scikit-learn\n\n\n\n\n\n\nMachine Learning\n\n\nPython\n\n\n\nCombining preprocessing and classifier within a single pipeline\n\n\n\n\n\nAug 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nQuerying SQL Server from Pandas\n\n\n\n\n\n\nPython\n\n\n\nQuerying SQL Server from Pandas\n\n\n\n\n\nJun 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter Search and Classifier Threshold Selection\n\n\n\n\n\n\nMachine Learning\n\n\nPython\n\n\n\nHyperparameter search and classifier threshold selection\n\n\n\n\n\nApr 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUsing the Singular Value Decomposition for Image Compression\n\n\n\n\n\n\nMachine Learning\n\n\nPython\n\n\n\nUsing the singular value decomposition for image compression\n\n\n\n\n\nApr 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nThe Doomsday Argument\n\n\n\n\n\n\nOther\n\n\n\nWalking through the probabilities associated with the doomsday argument\n\n\n\n\n\nMar 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating Logistic Regression coefficients From Scratch in Python\n\n\n\n\n\n\nStatistical Modeling\n\n\nPython\n\n\n\nFitting logistic regression models with iterative reweighted least squares in Python\n\n\n\n\n\nMar 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAssessing Model Goodness-of-Fit in Python with Scipy\n\n\n\n\n\n\nStatistical Modeling\n\n\nPython\n\n\n\nAssessing model goodness-of-git in Python with Scipy\n\n\n\n\n\nMar 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating Logistic Regression coefficients From Scratch in R\n\n\n\n\n\n\nStatistical Modeling\n\n\nR\n\n\n\nFitting logistic regression models with iterative reweighted least squares in R\n\n\n\n\n\nMar 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBackpropagation for Fully-Connected Neural Networks\n\n\n\n\n\n\nMachine Learning\n\n\nPython\n\n\n\nBackpropagation for fully-connected neural networks\n\n\n\n\n\nFeb 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCreating scikit-learn Pipelines\n\n\n\n\n\n\nMachine Learning\n\n\nPython\n\n\n\nCreating scikit-learn pipelines\n\n\n\n\n\nFeb 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nShared Data Parallel Processing in Python\n\n\n\n\n\n\nPython\n\n\n\nSharing data between processes with Python’s multiprocessing library\n\n\n\n\n\nFeb 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nFinding Roots of Equations in R with uniroot\n\n\n\n\n\n\nStatistical Modeling\n\n\nR\n\n\n\nFinding roots of equations in R with uniroot\n\n\n\n\n\nFeb 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nLogLikelihood Estimation in R with optim\n\n\n\n\n\n\nStatistical Modeling\n\n\nR\n\n\n\nLogLikelihood estimation in R with optim\n\n\n\n\n\nFeb 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nA PDF Harvester in 25 Lines of Python\n\n\n\n\n\n\nPython\n\n\n\nA PDF harvester in 25 Lines of python\n\n\n\n\n\nFeb 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nOnline Mean and Variance Update without Full Recalculation\n\n\n\n\n\n\nStatistical Modeling\n\n\nPython\n\n\n\nUpdating sample mean and variance to account for new observations without full recalculation\n\n\n\n\n\nFeb 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPerforming Rolling Joins with data.table\n\n\n\n\n\n\nR\n\n\n\nPerforming rolling joins with data.table\n\n\n\n\n\nFeb 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSmoothing Data with Cubic Splines in R\n\n\n\n\n\n\nStatistical Modeling\n\n\nR\n\n\n\nSmoothing data with cubic splines in R\n\n\n\n\n\nFeb 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nThe Beta Function and its Variants\n\n\n\n\n\n\nStatistical Modeling\n\n\n\nAn investigation of the complete and incomplete beta functions with use cases\n\n\n\n\n\nFeb 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAccessing C Library Functions from Python\n\n\n\n\n\n\nPython\n\n\n\nAccessing C library functions from Python\n\n\n\n\n\nFeb 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Dijkstra’s Algorithm to Find All Shortest Paths in a Graph\n\n\n\n\n\n\nStatistical Modeling\n\n\nPython\n\n\n\nUsing Dijkstra’s algorithm to find all shortest paths in a graph\n\n\n\n\n\nFeb 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nExpectation Maximization from Scratch in R\n\n\n\n\n\n\nMachine Learning\n\n\nR\n\n\n\nExpectation maximization from scratch in R\n\n\n\n\n\nFeb 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the Graph Convolutional Network Propogation Model\n\n\n\n\n\n\nMachine Learning\n\n\nPython\n\n\n\nWalking through the forward pass described in the Kipf and Welling paper\n\n\n\n\n\nFeb 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Linear Regression via Gibbs Sampling\n\n\n\n\n\n\nStatistical Modeling\n\n\nPython\n\n\n\nGibbs sampling approaches for Bayesian linear regression\n\n\n\n\n\nFeb 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSpeeding Up R with Rcpp\n\n\n\n\n\n\nR\n\n\n\nSpeeding Up R Code with Rcpp\n\n\n\n\n\nFeb 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBessel’s Correction\n\n\n\n\n\n\nStatistical Modeling\n\n\nPython\n\n\n\nCorrecting the bias in estimators of population variance\n\n\n\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUser-Defined Binary Operators in R\n\n\n\n\n\n\nR\n\n\n\nImplementing custom binary operators in R\n\n\n\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGenerating Correlated Random Samples in Python\n\n\n\n\n\n\nStatistical Modeling\n\n\nPython\n\n\n\nGenerating correlated random samples in python\n\n\n\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAggregation with data.table\n\n\n\n\n\n\nR\n\n\n\nAggregation with data.table\n\n\n\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Special Symbols in data.table\n\n\n\n\n\n\nR\n\n\n\nIntroduction to special symbols in data.table\n\n\n\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nDerivation of the Normal Equations\n\n\n\n\n\n\nStatistical Modeling\n\n\n\nDerivation of the Normal Equations via least squares and maximum likelihood\n\n\n\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nComputing the Product of a Variable Number of Columns using data.table\n\n\n\n\n\n\nR\n\n\n\nComputing the product of a variable number of columns using data.table\n\n\n\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nDenoising Signals using the FFT\n\n\n\n\n\n\nMachine Learning\n\n\nPython\n\n\n\nDenoising signals using the FFT\n\n\n\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGeoHashing from Scratch in Python\n\n\n\n\n\n\nPython\n\n\nGeospatial\n\n\n\nGeoHashing from scratch in python\n\n\n\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUtilizing Zero-Width Assertions with grep\n\n\n\n\n\n\nLinux\n\n\n\nUtilizing zero-width assertions with grep\n\n\n\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGradient Descent for Logistic Regression\n\n\n\n\n\n\nMachine Learning\n\n\nPython\n\n\n\nImplementing gradient descent to estimate logistic regression coefficients\n\n\n\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nA Matrix Factorization Approach to Linear Regression\n\n\n\n\n\n\nStatistical Modeling\n\n\nPython\n\n\n\nA matrix factorization approach to linear regression\n\n\n\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nParameterized Decorators in Python\n\n\n\n\n\n\nPython\n\n\n\nParameterized decorators in Python\n\n\n\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nDerivation of the Poisson Distribution as a Limiting Case of the Binomial PDF\n\n\n\n\n\n\nStatistical Modeling\n\n\n\nDerivation of the Poisson distribution as a limiting case of the binomial PDF\n\n\n\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Example of RSA Encryption and Decryption\n\n\n\n\n\n\nPython\n\n\n\nAn illustrative example of RSA encryption and decryption using small primes\n\n\n\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nQuerying SQL Server with Pandas\n\n\n\n\n\n\nPython\n\n\n\nQuerying SQL server with Pandas\n\n\n\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with SQLite in R\n\n\n\n\n\n\nR\n\n\n\nWorking with SQLite in R\n\n\n\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nThe Buried Assumption in Constant Dispersion Tweedie Models\n\n\n\n\n\n\nStatistical Modeling\n\n\n\nThe buried assumption in constant dispersion tweedie models\n\n\n\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCreating R Packages with devtools and roxygen\n\n\n\n\n\n\nR\n\n\n\nCreating R packages with devtools and roxygen\n\n\n\n\n\nJan 31, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nThe Elbow Method for Determining the Number of Clusters in K-Means\n\n\n\n\n\n\nMachine Learning\n\n\nPython\n\n\n\nThe elbow method for determining the number of clusters in k-keans\n\n\n\n\n\nJan 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nDemonstration of Principal Component Analysis\n\n\n\n\n\n\nMachine Learning\n\n\nPython\n\n\n\nDemonstration of principal component analysis\n\n\n\n\n\nJan 28, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/bessels-correction/bessels-correction.html",
    "href": "posts/bessels-correction/bessels-correction.html",
    "title": "Bessel’s Correction",
    "section": "",
    "text": "Bessel’s correction is the use of \\(n-1\\) instead of \\(n\\) in the sample variance formula where \\(n\\) is the number of observations in a sample. This method corrects the bias in the estimation of the population variance.\nRecall that bias is defined as:\n\\[\n\\mathrm{Bias}(\\theta) = E[\\hat{\\theta}] - \\theta,\n\\]\nwhere \\(\\theta\\) represents the actual parameter value, and \\(E[\\hat{\\theta}]\\) is an estimator of the parameter \\(\\theta\\). A desirable property of an estimator is that its expected value equals the parameter being estimated, or \\(E[\\hat{\\theta}] = \\theta\\). When this occurs, the estimator is said to be unbiased. Let \\(\\sigma^{2}\\) represent the population variance, given by\n\\[\n\\hat{\\sigma}^{2} = \\frac{1}{n}\\sum_{i=1}^{n}(Y_{i} - \\bar{Y})^{2}.\n\\]\nTo show that \\(\\hat{\\sigma}^{2}\\) is a biased estimator for \\(\\sigma^{2}\\), let \\(Y_{1}, Y_{2}, \\cdots, Y_{n}\\) be a random sample with \\(E[Y_{i}] = \\mu\\) and \\(Var[Y_{i}] = \\sigma^{2}\\). First, note that\n\\[\n\\sum_{i=1}^{n}(Y_{i} - \\bar{Y})^{2} = \\sum_{i=1}^{n}Y_{i}^{2} - n\\bar{Y}^{2},\n\\]\nand as a result\n\\[\nE\\Big[\\sum_{i=1}^{n}(Y_{i} - \\bar{Y})^{2}\\Big] = E\\Big(\\sum_{i=1}^{n}Y_{i}^{2}\\Big) - nE(\\bar{Y}^{2}) = \\sum_{i=1}^{n}E(Y_{i}^{2}) - nE(\\bar{Y}^{2}).\n\\]\nRearranging the familiar expression for variance yields\n\\[\nE[Y^{2}] = Var[Y] + E[Y]^{2} = \\sigma^{2} + \\mu^{2},\n\\]\nand similarly,\n\\[\nVar[\\bar{Y}] + E[\\bar{Y}]^{2} = \\sigma^{2}/n + \\mu^{2}.\n\\]\nTherefore\n\\[\n\\begin{align*}\nE\\Big[\\sum_{i=1}^{n}(Y_{i}-\\bar{Y})^{2}\\Big] &= \\sum_{i=1}^{n}\\sigma^{2}+\\mu^{2}-n\\Big(\\frac{\\sigma^{2}}{n} + \\mu^{2}\\Big) \\\\\n&=n(\\sigma^{2} + \\mu^{2}) - n\\Big(\\frac{\\sigma^{2}}{n} + \\mu^{2}\\Big) \\\\\n&=n\\sigma^{2} - \\sigma^{2} = (n-1)\\sigma^{2}.\n\\end{align*}\n\\]\nThus,\n\\[\nE[\\hat{\\sigma}^{2}] = \\frac{1}{n}E\\Big[\\sum_{i=1}^{n}(Y_{i} - \\bar{Y})^{2}\\Big] = \\frac{1}{n}(n-1)\\sigma^{2} = \\Big(\\frac{n-1}{n}\\Big)\\sigma^{2},\n\\]\nand we conclude that \\(\\sigma^{2}\\) is biased since \\(E[\\hat{\\sigma}^{2}] \\ne \\sigma^{2}\\). We now consider the sample variance \\(S^{2}\\):\n\\[\nE[S^{2}] = \\frac{1}{n-1}E\\Big[\\sum_{i=1}^{n}(Y_{i} - \\bar{Y})^{2}\\Big] = \\frac{1}{n-1}(n-1)\\sigma^{2} = \\sigma^{2},\n\\]\nand since \\(E[S^{2}] = \\sigma^{2}\\), we conclude that \\(S^{2}\\) is an unbiased estimator for \\(\\sigma^{2}\\).\n\nDemonstration\nAn important property of an unbiased estimator of a population parameter is that if the sample statistic is evaluated for every possible sample and the average computed, the mean over all samples will exactly equal the population parameter. For a given population with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\), if the sample variance (division by \\((n−1)\\)) is computed for all possible permutations of the dataset, the average of the sample variances will exactly equal \\(\\sigma^{2}\\). This also demonstrates (indirectly) that division by \\(n\\) would consistently underestimate the population variance.\nWe now attempt to verify this property on the following dataset:\n\\[\n7, 9, 10, 12, 15\n\\]\nThe Python itertools module exposes a collection of efficient iterators that stream values on-demand based on various starting and/or stopping conditions. For example, the permutations implementation takes as arguments an iterable and the length of the permutation r. It returns all r-length permutations of elements from the iterable (itertools also exposes a combinations function that does the same for all r-length combinations). The product function generates the cartesian product of the specified iterables, and takes an optional repeat argument. From the documentation:\n\nTo compute the product of an iterable with itself, specify the number of repetitions with the optional repeat keyword argument. For example, product(A, repeat=4) means the same as product(A, A, A, A).\n\nproduct is used to compute the average sample variance for all 2, 3 and 4-element permutations from \\([7, 9, 10, 12, 15]\\), and the result is compared to the population variance. Before we begin, lets calculate the population mean and variance:\n\\[\n\\begin{align*}\n\\bar{Y} &= 10.6 \\\\\n\\sigma^{2} &= \\sum_{i=1}^{5}\\frac{Y_{i}^{2}}{n} - \\bar{Y}^{2} \\\\\n&= 119.8 - 10.6^{2} \\\\\n&= 7.44\n\\end{align*}\n\\]\nWe now compute the average of the sample variance for all \\(k\\)-element permutations from \\([7, 9, 10, 12, 15]\\) for \\(2 \\le k \\le 5\\):\n\n\"\"\"\nDemonstrating that the sample variance is an unbiased estimator \nof the population variance. \n\nGenerate all possible 2, 3, 4 and 5-element permutations from \n[7, 9, 10, 12, 15], and determine the sample variance of each \nsample. The average of the sample variances will exactly equate \nto the population variance if the sample variance is an unbiased \nestimator of the population variance.\n\"\"\"\nimport itertools\nimport numpy as np\n\nv = [7, 9, 10, 12, 15]\n\n\n# Verify that the average of the sample variance\n# for all 2-element samples equates to 7.44.\ns2 = list(itertools.product(v, repeat=2))\nresult2 = np.mean([np.var(ii, ddof=1) for ii in s2])\n\n# Verify that the average of the sample variance\n# for all 3-element samples equates to 7.44.\ns3 = list(itertools.product(v, repeat=3))\nresult3 = np.mean([np.var(ii, ddof=1) for ii in s3])\n\n# Verify that the average of the sample variance\n# for all 4-element samples equates to 7.44.\ns4 = list(itertools.product(v, repeat=4))\nresult4 = np.mean([np.var(ii, ddof=1) for ii in s4])\n\n# Verify that the average of the sample variance\n# for all 5-element samples equates to 7.44.\ns5 = list(itertools.product(v, repeat=5))\nresult5 = np.mean([np.var(ii, ddof=1) for ii in s5])\n\nprint(f\"result2: {result2}\")\nprint(f\"result3: {result3}\")\nprint(f\"result4: {result4}\")\nprint(f\"result5: {result5}\")\n\nresult2: 7.44\nresult3: 7.4399999999999995\nresult4: 7.44\nresult5: 7.44\n\n\nSince the sample variance is an unbiased estimator of the population variance, these results should come as no surprise, but it is an interesting demonstration nonetheless."
  },
  {
    "objectID": "posts/binary-operators-r/binary-operators-r.html",
    "href": "posts/binary-operators-r/binary-operators-r.html",
    "title": "User-Defined Binary Operators in R",
    "section": "",
    "text": "One of the things I miss most (when working in R) is Python’s builtin string methods and string manipulation functions. The two methods I miss the most are startswith and endswith. Here’s and example of how they work in Python:\nIn [1]: \"apple\".endswith(\"e\")\nTrue\nIn [2]: \"rubbersoul\".startswith(\"rubber\")\nTrue\nIn [3]: \"billiondollar\".endswith(\"babies\")\nFalse\nEverything in Python is an object, and all Python string objects expose these two methods (and many others). I wanted to make the same functionality available in R while maintaining the simplicity of the Python approach. I found a way to accomplish this using user-defined binary operators in R.\n\nBinary Operators\nUser-defined binary operators in R consist of a string of characters between two % characters. Some frequently used builtin binary operators include %/% for integer division and %%, which represents the modulus operator. Declaring a binary operator is identical to declaring any other function, except for the name. Here’s an implementation of %startswith% and %endswith%:\n# Example of declaring user-defined binary operators in R.\n\n`%startswith%` = function(teststr, testchars) {\n    #   `teststr`: The target string.\n    # `testchars`: The character(s) to test for in `teststr`. \n    return(grepl(paste0(\"^\", testchars), teststr))\n}\n\n`%endswith%` = function(teststr, testchars) {\n    #   `teststr`: The target string.\n    # `testchars`: The character(s) to test for in `teststr`. \n    return(grepl(paste0(testchars, \"$\"), teststr))\n}\nOnce read in to the current session, both individual strings and vectors of strings can be passed to either operator to test for the specified leading or trailing character(s). For example, if I had the following vector:\nmonths = c(\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \n           \"August\", \"September\", \"October\", \"November\", \"December\")\nAnd wanted to test whether or not the elements of months start with “J”,\n%startswith% could be used as follows:\n&gt; months %startswith% \"J\"\n[1]  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\nSimilarly, to check whether elements of months end with “ber”, run:\n&gt; months %endswith% \"ber\"\n[1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\nTo obtain the indicies of the elements of months ending with “ber”, we can use %endswith% in conjunction with which:\n&gt; months %endswith% \"ber\"\n[1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n&gt; which(months %endswith% \"ber\")\n[1]  9 10 11 12\n&gt; months[which(months %endswith% \"ber\")]\n[1] \"September\" \"October\"   \"November\"  \"December\""
  },
  {
    "objectID": "posts/classifier-thresholding/classifier-thresholding.html",
    "href": "posts/classifier-thresholding/classifier-thresholding.html",
    "title": "Hyperparameter Search and Classifier Threshold Selection",
    "section": "",
    "text": "The following notebook demonstrates how to use GridSearchCV to identify optimal hyperparameters for a given model and metric, and alternatives for selecting a classifier threshold in scikit-learn.\nFirst we load the breast cancer dataset. We will forgo any pre-processing, but create separate train and validation sets:\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\n\nnp.set_printoptions(suppress=True, precision=8, linewidth=1000)\npd.options.mode.chained_assignment = None\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\n\ndata = load_breast_cancer()\nX = data[\"data\"]\ny = data[\"target\"]\n\n\n# Create train, validation and test splits. \nXtrain, Xvalid, ytrain, yvalid = train_test_split(X, y, test_size=.20, random_state=516)\n\nprint(f\"Xtrain.shape: {Xtrain.shape}\")\nprint(f\"Xvalid.shape: {Xvalid.shape}\")\n\nXtrain.shape: (455, 30)\nXvalid.shape: (114, 30)"
  },
  {
    "objectID": "posts/classifier-thresholding/classifier-thresholding.html#hyperparameter-search",
    "href": "posts/classifier-thresholding/classifier-thresholding.html#hyperparameter-search",
    "title": "Hyperparameter Search and Classifier Threshold Selection",
    "section": "Hyperparameter Search",
    "text": "Hyperparameter Search\nA random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting (see documentation here).\nThe RandomForestClassifier takes a number of hyperparameters. It can be difficult to determine which values to set these to manually, so instead we can perform a cross-validated grid search over a number of candidate values to determine which hyperparmeter combination is best for our data and specified metric. GridSearchCVis part of scikit-learn, and is a method used to find the best possible configuration of hyperparameters for optimal performance. It works as follows:\n\nDefine a parameter grid: The grid is a dictionary that maps parameter names to the values that should be tested. These parameters are specific to the model you are working to optimize.\nSpecify a model: Choose a model that you want to optimize using GridSearchCV. This model is not trained yet; it’s just passed in with it’s default parameters.\nCross-validation setup: GridSearchCV uses cross-validation to evaluate each combination of parameter values provided in the grid. You need to specify the number of folds (splits) for the cross-validation process (this is the cv parameter). Common choices are 5 or 10 folds, depending on the size of your dataset and how thorough you want the search to be.\nSearch Execution: With the parameter grid, model, and cross-validation setup, GridSearchCV systematically works through multiple combinations of parameter sets, cross-validating as it goes to determine which configuration gives the best performance based on a score function. The performance is often measured using metrics like accuracy, precision or recall for classification problems or mean squared error for regression problems.\nResults: Finally, GridSearchCV provides the best parameters, allowing you to understand which parameters work best for your model. Additionally, it can provide other results like the score for each parameter combination, allowing for deeper analysis of how different parameter values impact model performance.\n\n\nThe documentation for GridSearchCV is available here.\n\nIn the next cell, we assess the following RandomForestClassifier hyperparameters:\n\nn_estimators: [100, 150, 250]\nmin_samples_leaf: [2, 3, 4]\nccp_alpha: [0, .1, .2, .3]\n\nFor the metric, recall is used since the cost of a false negative is high (not detecting breast cancer). This means the hyperparameter combination with the maximum average recall over the k-folds will be selected as the best parameter set.\n\n\"\"\"\nExample using GridSearchCV to identify optimal hyperparameters w.r.t. recall.\nNote that within GridSearchCV, cv represents the number of folds for \nk-Fold cross validation.\n\"\"\"\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n\n# Create parameter grid as dictionary.\nparam_grid = {\n    \"n_estimators\": [100, 150, 250],\n    \"min_samples_leaf\": [2, 3, 4],\n    \"ccp_alpha\": [0, .1, .2, .3]\n    }\n\n\n# Pass model and param_grid into GridSearchCV.\nmdl = GridSearchCV(\n    RandomForestClassifier(random_state=516), \n    param_grid, \n    scoring=\"recall\", \n    cv=5\n    )\n    \n# Fit model on training set. This can take a while depending on the number of \n# hyperparameter combinations in param_grid.\nmdl.fit(Xtrain, ytrain)\n\n\n# Print optimal parameters.\nprint(f\"best parameters: {mdl.best_params_}\")\n\n\nbest parameters: {'ccp_alpha': 0, 'min_samples_leaf': 4, 'n_estimators': 100}\n\n\n\nFor random forests, boosting models and other tree-based ensemble methods, we can obtain a summary of the relative importance of each of the input features. This is available in the mdl.best_estimator_.feature_importances_ attribute. We can plot feature importances in decreasing order as follows:\n\n\nimp = mdl.best_estimator_.feature_importances_\n\nrf_imp = pd.Series(imp, index=data[\"feature_names\"]).sort_values(ascending=False)\n\nfig, ax = plt.subplots(figsize=(9, 5), tight_layout=True)\nrf_imp.plot.bar(ax=ax)\nax.set_title(\"RandomForestClassifier feature importances\")\nax.set_ylabel(\"mean decrease in impurity\")\nplt.show()\n\n\n\n\n\n\n\n\n\nIn terms of mean decrease in impurity, the top 7 features are assigned the highest importance, with the remaining features deemed not as relevant. For more information on how feature importance is calculated, see here.\n\nThe resulting mdl object can be used to make predictions on the validation set (mdl exposes the RandomForestClassifier with optimal hyperparameters set). We use mdl.predict_proba to get probabilities on [0, 1], with values closer to 1 representing positive predicted instances of breast cancer on the validation set:\n\n\nypred = mdl.predict_proba(Xvalid)[:,1]\n\nypred\n\narray([0.005     , 0.82743637, 0.97088095, 0.        , 0.        , 1.        , 0.98020202, 0.67380556, 0.        , 0.99333333, 0.9975    , 0.30048576, 0.9528113 , 0.99666667, 0.04102381, 0.99444444, 1.        , 0.828226  , 0.        , 0.        , 0.97916667, 1.        , 0.99607143, 0.90425163, 0.        , 0.02844156, 0.99333333, 0.98183333, 0.9975    , 0.08869769, 0.97369841, 0.        , 1.        , 0.71100866, 0.96022727, 0.        , 0.71200885, 0.06103175, 0.005     , 0.99490476, 0.1644127 , 0.        , 0.23646934, 1.        , 0.57680164, 0.64901715, 0.9975    , 0.61790818, 0.95509668, 0.99383333, 0.04570455, 0.97575758, 1.        , 0.47115815, 0.92422619, 0.77371415, 0.        , 1.        , 0.26198657, 0.        , 0.28206638, 0.95216162, 0.98761905, 0.99464286, 0.98704762, 0.85579351, 0.10036905, 0.00222222, 0.98011905, 0.99857143, 0.92285967, 0.95180556, 0.97546947, 0.84433189, 0.005     , 0.99833333, 0.83616339, 1.        , 0.9955    , 1.        , 0.99833333, 1.        ,\n       0.86399315, 0.9807381 , 0.        , 0.99833333, 0.9975    , 0.        , 0.98733333, 0.96822727, 0.23980827, 0.7914127 , 0.        , 0.98133333, 1.        , 1.        , 0.89251019, 0.9498226 , 0.18943254, 0.83494391, 0.9975    , 1.        , 0.77079113, 0.99722222, 0.30208297, 1.        , 0.92111977, 0.99428571, 0.91936508, 0.47118074, 0.98467172, 0.006     , 0.05750305, 0.96954978])\n\n\nNote that scikit-learn predict_proba outputs an nx2 dimensional array, where the first column represents the probability of class 0 and the second column the probability of class 1 (has breast cancer). Each row will sum to 1. We will work with the probabilities of the class we’re interested in analyzing, so we extract only the values from the positive class (the second column), that’s why we call mdl.predict_proba(Xvalid)[:,1].\n\n\nThreshold Selection\nIn order to master machine learning, it is necessary to learn a variety of minor concepts that underpin these systems. One such concept is setting the optimal classification threshold.\nBy default, for probabilistic classifiers scikit-learn uses a threshold of .50 to distinguish between positive and negative class instances. The predicted classes are obtained by calling mdl.predict. Here’s a side by side comparison of the model predicted probabilities and predicted classes:\n\n\n# Predicted probabilities.\nypred = mdl.predict_proba(Xvalid)[:,1].reshape(-1, 1)\n\n\n# Predicted classes.\nyhat = mdl.predict(Xvalid).reshape(-1, 1)\n\n\n# Combine probabilities and predicted class labels.\npreds = np.concatenate([ypred, yhat], axis=1)\n\npreds\n\narray([[0.005     , 0.        ],\n       [0.82743637, 1.        ],\n       [0.97088095, 1.        ],\n       [0.        , 0.        ],\n       [0.        , 0.        ],\n       [1.        , 1.        ],\n       [0.98020202, 1.        ],\n       [0.67380556, 1.        ],\n       [0.        , 0.        ],\n       [0.99333333, 1.        ],\n       [0.9975    , 1.        ],\n       [0.30048576, 0.        ],\n       [0.9528113 , 1.        ],\n       [0.99666667, 1.        ],\n       [0.04102381, 0.        ],\n       [0.99444444, 1.        ],\n       [1.        , 1.        ],\n       [0.828226  , 1.        ],\n       [0.        , 0.        ],\n       [0.        , 0.        ],\n       [0.97916667, 1.        ],\n       [1.        , 1.        ],\n       [0.99607143, 1.        ],\n       [0.90425163, 1.        ],\n       [0.        , 0.        ],\n       [0.02844156, 0.        ],\n       [0.99333333, 1.        ],\n       [0.98183333, 1.        ],\n       [0.9975    , 1.        ],\n       [0.08869769, 0.        ],\n       [0.97369841, 1.        ],\n       [0.        , 0.        ],\n       [1.        , 1.        ],\n       [0.71100866, 1.        ],\n       [0.96022727, 1.        ],\n       [0.        , 0.        ],\n       [0.71200885, 1.        ],\n       [0.06103175, 0.        ],\n       [0.005     , 0.        ],\n       [0.99490476, 1.        ],\n       [0.1644127 , 0.        ],\n       [0.        , 0.        ],\n       [0.23646934, 0.        ],\n       [1.        , 1.        ],\n       [0.57680164, 1.        ],\n       [0.64901715, 1.        ],\n       [0.9975    , 1.        ],\n       [0.61790818, 1.        ],\n       [0.95509668, 1.        ],\n       [0.99383333, 1.        ],\n       [0.04570455, 0.        ],\n       [0.97575758, 1.        ],\n       [1.        , 1.        ],\n       [0.47115815, 0.        ],\n       [0.92422619, 1.        ],\n       [0.77371415, 1.        ],\n       [0.        , 0.        ],\n       [1.        , 1.        ],\n       [0.26198657, 0.        ],\n       [0.        , 0.        ],\n       [0.28206638, 0.        ],\n       [0.95216162, 1.        ],\n       [0.98761905, 1.        ],\n       [0.99464286, 1.        ],\n       [0.98704762, 1.        ],\n       [0.85579351, 1.        ],\n       [0.10036905, 0.        ],\n       [0.00222222, 0.        ],\n       [0.98011905, 1.        ],\n       [0.99857143, 1.        ],\n       [0.92285967, 1.        ],\n       [0.95180556, 1.        ],\n       [0.97546947, 1.        ],\n       [0.84433189, 1.        ],\n       [0.005     , 0.        ],\n       [0.99833333, 1.        ],\n       [0.83616339, 1.        ],\n       [1.        , 1.        ],\n       [0.9955    , 1.        ],\n       [1.        , 1.        ],\n       [0.99833333, 1.        ],\n       [1.        , 1.        ],\n       [0.86399315, 1.        ],\n       [0.9807381 , 1.        ],\n       [0.        , 0.        ],\n       [0.99833333, 1.        ],\n       [0.9975    , 1.        ],\n       [0.        , 0.        ],\n       [0.98733333, 1.        ],\n       [0.96822727, 1.        ],\n       [0.23980827, 0.        ],\n       [0.7914127 , 1.        ],\n       [0.        , 0.        ],\n       [0.98133333, 1.        ],\n       [1.        , 1.        ],\n       [1.        , 1.        ],\n       [0.89251019, 1.        ],\n       [0.9498226 , 1.        ],\n       [0.18943254, 0.        ],\n       [0.83494391, 1.        ],\n       [0.9975    , 1.        ],\n       [1.        , 1.        ],\n       [0.77079113, 1.        ],\n       [0.99722222, 1.        ],\n       [0.30208297, 0.        ],\n       [1.        , 1.        ],\n       [0.92111977, 1.        ],\n       [0.99428571, 1.        ],\n       [0.91936508, 1.        ],\n       [0.47118074, 0.        ],\n       [0.98467172, 1.        ],\n       [0.006     , 0.        ],\n       [0.05750305, 0.        ],\n       [0.96954978, 1.        ]])\n\n\nNotice that when the probability is less that 0.50, the predicted class is 0. When the predicted probability is greater than 0.50, the predicted class is 1. For certain applications, the 0.50 threshold might make sense, for example when your target is balanced or close to balanced (when the number of 0s and 1s in the training set is approximately equal). But for unbalanced datasets, using the default threshold can give misleading results. In what follows, we walkthrough a few approaches that can be used to assess the optimal discrimination theshold for a classifier.\n\n\n1. Use 0.50\nThe first approach is the most straightforward: Just use the default scikit-learn threshold of .50. This makes sense when your classes are balanced, but will give misleading results when classes are imbalanced.\n\n\n\n2. Use 1 - the proportion of positive instances in the training data\nIf we look at the number of positives (1s) vs. total samples in our training set, we have:\n\n\nprint(f\"Proportion of positives in training set: {ytrain.sum() / ytrain.shape[0]:.2f}\")\n\nProportion of positives in training set: 0.62\n\n\nWe see that 62% of the samples belong to class 1. This is usually not the case. In many classification scenarios, we’re dealing with 10%, 5% or even less than 1% of samples belonging to the positive class.\nTo illustrate the approach, since 62% percent of the observations belong to the positive class, we would use a threshold of 1 - .62 = .38. The predicted class labels are then created using the following code:\n\n\n# Creating predicted classes based on adjusted classifier threshold. \nthresh = .38\n\nyhat = np.where(ypred &lt;= thresh, 0, 1)\n\nNow any sample with a predicted probability less than or equal to .38 will be assigned to class 0, and samples with predicted probability greater than .38 are assigned to the positive class.\nIf we’re dealing with a highly imbalanced dataset with only 1% positive instances, we would use 1 - .01 = .99 as the threshold using this method.\n\n\n\n3. Use best f1-score\nThe f1-score is the geometric average of precision and recall. We can compute precision and recall for a number of different thresholds then select the threshold that maximizes the f1-score. This is a suitable approach if your classification task weighs precision and recall equally. Although this isn’t the case for our breast cancer classifier (we want to maximize recall since the cost of a false negative is high), the approach is demonstrated in the next cell:\n\n\nfrom sklearn.metrics import precision_recall_curve\n\n# Get precision and recall for various thresholds.\np, r, thresh = precision_recall_curve(yvalid, ypred)\n\n# Compute f1-score for each threshold.\nf1 = 2 * (p * r) / (p + r)\n\n# Identify threshold that maximizes f1-score.\nbest_thresh = thresh[np.argmax(f1)]\n\nprint(f\"Threshold using optimal f1-score: {best_thresh:,.3f}.\")\n\nThreshold using optimal f1-score: 0.471.\n\n\nUsing this method, we would set the discrimination threshold to .471, and would obtain the predicted class labels the same way as before:\n\n\nthresh = .471\n\nyhat = np.where(ypred &lt;= thresh, 0, 1)\n\n\n\n\n4. Inspection of ROC curve\nThe Receiver Operating Characteristic (ROC) curve plots the true positive rate (TPR) against the false positive rate (FPR) at various classification thresholds. Typically we’re interested in using a threshold that maximizes TPR while minimizing FPR, which is the point (0, 1). The curve starts with a threshold of 1 at the far left and decreases towards 0 as the x-axis increases.\nWe can plot the ROC curve in scikit-learn using the code below. Note that ypred are predicted probabilities and yvalid are class labels (1s or 0s).\n\n\nfrom sklearn.metrics import RocCurveDisplay\n\nroc_disp = RocCurveDisplay.from_predictions(\n    yvalid, ypred, name=\"RandomForestClassifier\", color=\"#191964\"\n    )\nroc_disp.ax_.set_title(\"ROC curve\", fontsize=9)\nroc_disp.ax_.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nUsing approach 4, the optimal threshold would be somewhere between .70-.80, which is much higher than what is indicated using the other methods so far. Ultimately it is up to you to determine which threshold makes the most sense, but intuitively, a threshold of .70-.80 seems too high when the prevalence of the positive class in the training data is 62%.\n\n\n\n5. Inspection of the precision-recall curve\nThe precision-recall curve is a graphical representation used in binary classification to evaluate the performance of a classification model at different probability thresholds. This curve shows the trade-off between precision and recall for a number of different thresholds. The curve plots recall on the x-axis and precision on the y-axis.\nThe curve starts from the rightmost part of the graph. As the threshold for classifying positive instances decreases, recall increases, and precision can either increase or decrease, but typically it decreases because the model starts to classify more instances as positive, including both true positives and false positives.\nThe top-right corner of the graph (high precision, high recall) represents the ideal point, where the classifier perfectly identifies all positive cases with no false positives. Generally, we’d like to select a threshold that corresponds to a point closest to top-right corner of the graph.\nWe can plot the precision-recall curve in scikit-learn using the code below. Note that ypred are predicted probabilities and yvalid are class labels (1s or 0s).\n\n\nfrom sklearn.metrics import PrecisionRecallDisplay\n\npr_disp = PrecisionRecallDisplay.from_predictions(\n    yvalid, ypred, name=\"RandomForestClassifier\", color=\"#CD0066\"\n    )\npr_disp.ax_.set_title(\"Precision-Recall curve\", fontsize=9)\npr_disp.ax_.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nBased on the plot, would want to select the threshold that corresponds to a recall of about .95, since this is close to the point (1, 1). This can be determined using the following code:\n\n\nfrom sklearn.metrics import precision_recall_curve\n\np, r, thresh = precision_recall_curve(yvalid, ypred)\n\nbest_thresh = thresh[np.where(r &gt;= .95)[-1][-1]]\n\nprint(f\"Selected threshold using precision-recall curve: {best_thresh:,.3f}.\")\n\nSelected threshold using precision-recall curve: 0.674.\n\n\n\n\n\n6. Inspection of precision and recall as a function of threshold\nIt is also possible to plot precision and recall as two separate series against threshold on the x-axis. The goal is to identify a point where precision and recall intersect. Using this approach may be suitable in some scenarios.\n\n\nfrom sklearn.metrics import precision_recall_curve\n\np, r, thresh = precision_recall_curve(yvalid, ypred)\np, r = p[:-1], r[:-1]\n\nfig, ax =  plt.subplots(1, 1, figsize=(6.5, 4), tight_layout=True)\nax.set_title(\"precision & recall vs. threshold\", fontsize=10)\nax.plot(thresh, p, color=\"red\", linewidth=1.25, label=\"precision\")\nax.plot(thresh, r, color=\"blue\", linewidth=1.25, label=\"recall\")\nax.set_xlabel(\"threshold\", fontsize=8)\n# ax.set_xticks(np.arange(tmax+1))\nax.tick_params(axis=\"x\", which=\"major\", direction=\"in\", labelsize=8)\nax.tick_params(axis=\"y\", which=\"major\", direction=\"in\", labelsize=8)\nax.xaxis.set_ticks_position(\"none\")\nax.yaxis.set_ticks_position(\"none\")\nax.grid(True)\nax.legend(loc=\"upper right\", fancybox=True, framealpha=1, fontsize=\"medium\")\n\nplt.show()\n\n\n\n\n\n\n\n\nThe precision and recall series intersect right after .60, therefore method #5 would set the threshold to roughly .60.\n\n\n\nAssessing a Classifier\nOnce a threshold has been selected, the predictive power of the classifier can be assessed. To do this, we will look at the confusion matrix as well as the sklearn.metrics.classification_report. Note that both diagnostics require actual and predicted labels. Once we’ve settled on a threshold, model assessment is performed comparing actual vs. predicted labels. In what follows, the 0.471 threshold obtained from method #3 will be used as the classification threshold.\nTechnically, once we’ve decided on a threshold, we should then assess the performance of the model on a separate test set. However, for the purposes of demonstration, we are going to re-use the validation set.\nWe start by creating the confusion matrix:\n\n\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\n# Determine predicted classes using the .471 threshold.\nthresh = .471\nyhat = np.where(ypred &lt;= thresh, 0, 1)\n\ncm_disp = ConfusionMatrixDisplay.from_predictions(yvalid, yhat, colorbar=False)\ncm_disp.ax_.set_title(f\"mm confusion matrix (thresh={thresh:.3})\", fontsize=9)\nplt.show()\n\n\n\n\n\n\n\n\nThe output indicates:\n\nThere are 76 True Positives (TP).\nThere are 34 True Negatives (TN).\nThere are 4 False Positives (FP).\nThere are 0 False Negatives (FN).\n\n\nNext we inspect the classification report. This also takes actual and predicted labels, and returns a summary of common classifier metrics:\n\n\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(yvalid, yhat))\n\n              precision    recall  f1-score   support\n\n           0       1.00      0.89      0.94        38\n           1       0.95      1.00      0.97        76\n\n    accuracy                           0.96       114\n   macro avg       0.97      0.95      0.96       114\nweighted avg       0.97      0.96      0.96       114\n\n\n\nOverall this is very good performance."
  },
  {
    "objectID": "posts/composite-estimators/composite-estimators.html",
    "href": "posts/composite-estimators/composite-estimators.html",
    "title": "Composite Estimators in scikit-learn",
    "section": "",
    "text": "To build a composite estimator in scikit-learn, transformers are usually combined with other transformers and/or predictors (such as classifiers or regressors). The most common tool used for composing estimators is a Pipeline. The Pipeline is often used in combination with ColumnTransformer or FeatureUnion which concatenate the output of transformers into a composite feature space.\nIn this notebook, I demonstrate how to create a composite estimator based on a synthetic dataset.\n\n\"\"\"\nCreate synthetic dataset for composite estimator demo.\n\"\"\"\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nnp.set_printoptions(suppress=True, precision=8)\npd.options.mode.chained_assignment = None\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\n\nrng = np.random.default_rng(516)\n\nn = 1000\n\ndf = pd.DataFrame({\n    \"A\": rng.gamma(shape=2, scale=50000, size=n),\n    \"B\": rng.normal(loc=1000, scale=250, size=n),\n    \"C\": rng.choice([\"red\", \"green\", \"blue\"], p=[.7, .2, .1], size=n),\n    \"D\": rng.choice([\"left\", \"right\", None], p=[.475, .475, .05], size=n),\n    \"E\": rng.poisson(17, size=n),\n    \"target\": rng.choice([0., 1.], p=[.8, .2], size=n)\n})\n\n# Set a selected samples to NaN in A, B and C. \ndf.loc[rng.choice(n, size=10),\"A\"] = np.NaN\ndf.loc[rng.choice(n, size=17),\"B\"] = np.NaN\ndf.loc[rng.choice(n, size=5),\"E\"] = np.NaN\n\n# Create train-validation split. \ny = df[\"target\"]\ndftrain, dfvalid, ytrain, yvalid = train_test_split(df, y, test_size=.05, stratify=y)\n\nprint(f\"dftrain.shape: {dftrain.shape}\")\nprint(f\"dfvalid.shape: {dfvalid.shape}\")\nprint(f\"prop. ytrain : {ytrain.sum() / dftrain.shape[0]:.4f}\")\nprint(f\"prop. yvalid : {yvalid.sum() / dfvalid.shape[0]:.4f}\")\n\ndftrain.shape: (950, 6)\ndfvalid.shape: (50, 6)\nprop. ytrain : 0.2389\nprop. yvalid : 0.2400\n\n\n\nFor this dataset, we’ll use ColumnTransformer to create separate pre-processing pipelines for continuous and categorical features. For continuous features, we impute missing values and standardize each to be on the same scale. For categorical features, we impute missing values and one-hot encode, creating k-1 features for a variable with k distinct levels. As the last step a LogisticRegression classifier is included with elastic net penatly. The code to accomplish this is given below:\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\n\n# Data pre-processing for LogisticRegression model.\nlr = LogisticRegression(\n    penalty=\"elasticnet\", solver=\"saga\", max_iter=5000\n    )\n\n# Identify continuous and catergorical features. \ncontinuous = [\"A\", \"B\", \"E\"]\ncategorical = [\"C\", \"D\"]\n\ncontinuous_transformer = Pipeline(steps=[\n    (\"imputer\", IterativeImputer()),\n    (\"scaler\" , StandardScaler())\n    ])\ncategorical_transformer = Pipeline(steps=[\n    (\"onehot\", OneHotEncoder(drop=\"first\", sparse_output=False, handle_unknown=\"error\"))\n    ])\n\npreprocessor = ColumnTransformer(transformers=[\n    (\"continuous\" , continuous_transformer, continuous),  \n    (\"categorical\", categorical_transformer, categorical)\n    ], remainder=\"drop\"\n    )\n\npipeline = Pipeline(steps=[\n    (\"preprocessor\", preprocessor),\n    (\"classifier\", lr)\n    ]).set_output(transform=\"pandas\")\n\npipeline\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('continuous',\n                                                  Pipeline(steps=[('imputer',\n                                                                   IterativeImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['A', 'B', 'E']),\n                                                 ('categorical',\n                                                  Pipeline(steps=[('onehot',\n                                                                   OneHotEncoder(drop='first',\n                                                                                 sparse_output=False))]),\n                                                  ['C', 'D'])])),\n                ('classifier',\n                 LogisticRegression(max_iter=5000, penalty='elasticnet',\n                                    solver='saga'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('continuous',\n                                                  Pipeline(steps=[('imputer',\n                                                                   IterativeImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['A', 'B', 'E']),\n                                                 ('categorical',\n                                                  Pipeline(steps=[('onehot',\n                                                                   OneHotEncoder(drop='first',\n                                                                                 sparse_output=False))]),\n                                                  ['C', 'D'])])),\n                ('classifier',\n                 LogisticRegression(max_iter=5000, penalty='elasticnet',\n                                    solver='saga'))])preprocessor: ColumnTransformerColumnTransformer(transformers=[('continuous',\n                                 Pipeline(steps=[('imputer',\n                                                  IterativeImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['A', 'B', 'E']),\n                                ('categorical',\n                                 Pipeline(steps=[('onehot',\n                                                  OneHotEncoder(drop='first',\n                                                                sparse_output=False))]),\n                                 ['C', 'D'])])continuous['A', 'B', 'E']IterativeImputerIterativeImputer()StandardScalerStandardScaler()categorical['C', 'D']OneHotEncoderOneHotEncoder(drop='first', sparse_output=False)LogisticRegressionLogisticRegression(max_iter=5000, penalty='elasticnet', solver='saga')\n\n\n\nIn the next cell, RandomizedSearchCV is run agasinst two hyperparameters: l1_ratio and C. Notice that we only call mdl.fit on the pipeline, as the data transform will be applied to each of the k-datasets separately based on the samples in each fold.\n\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform\n\n# Hyperparameters to search over. \nparam_grid = {\n    \"classifier__l1_ratio\": uniform(loc=0, scale=1), \n    \"classifier__C\": uniform(loc=0, scale=10)\n    }\n\nmdl = RandomizedSearchCV(\n    pipeline, param_grid, scoring=\"accuracy\", cv=5, verbose=2, \n    n_iter=3, random_state=516\n    )\n\nmdl.fit(dftrain.drop(\"target\", axis=1), ytrain)\n\nprint(f\"\\nbest parameters: {mdl.best_params_}\")\n\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END classifier__C=8.115660497752215, classifier__l1_ratio=0.7084090612742915; total time=   0.0s\n[CV] END classifier__C=8.115660497752215, classifier__l1_ratio=0.7084090612742915; total time=   0.0s\n[CV] END classifier__C=8.115660497752215, classifier__l1_ratio=0.7084090612742915; total time=   0.0s\n[CV] END classifier__C=8.115660497752215, classifier__l1_ratio=0.7084090612742915; total time=   0.0s\n[CV] END classifier__C=8.115660497752215, classifier__l1_ratio=0.7084090612742915; total time=   0.0s\n[CV] END classifier__C=1.115284252761577, classifier__l1_ratio=0.5667878644753359; total time=   0.0s\n[CV] END classifier__C=1.115284252761577, classifier__l1_ratio=0.5667878644753359; total time=   0.0s\n[CV] END classifier__C=1.115284252761577, classifier__l1_ratio=0.5667878644753359; total time=   0.0s\n[CV] END classifier__C=1.115284252761577, classifier__l1_ratio=0.5667878644753359; total time=   0.0s\n[CV] END classifier__C=1.115284252761577, classifier__l1_ratio=0.5667878644753359; total time=   0.0s\n[CV] END classifier__C=7.927782545875722, classifier__l1_ratio=0.8376069301429002; total time=   0.0s\n[CV] END classifier__C=7.927782545875722, classifier__l1_ratio=0.8376069301429002; total time=   0.0s\n[CV] END classifier__C=7.927782545875722, classifier__l1_ratio=0.8376069301429002; total time=   0.0s\n[CV] END classifier__C=7.927782545875722, classifier__l1_ratio=0.8376069301429002; total time=   0.0s\n[CV] END classifier__C=7.927782545875722, classifier__l1_ratio=0.8376069301429002; total time=   0.0s\n\nbest parameters: {'classifier__C': 8.115660497752215, 'classifier__l1_ratio': 0.7084090612742915}\n\n\n\nWhen an estimator is included within a scikit-learn pipeline and a grid search performed using RandomizedGridSearchCV, the estimator is automatically set to the best parameters found during the search. The best_estimator_ attribute of the RandomizedGridSearchCV object will reflect the best parameters for the estimator within the pipeline in terms of the scoring measure:\n\n\nmdl.best_estimator_\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('continuous',\n                                                  Pipeline(steps=[('imputer',\n                                                                   IterativeImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['A', 'B', 'E']),\n                                                 ('categorical',\n                                                  Pipeline(steps=[('onehot',\n                                                                   OneHotEncoder(drop='first',\n                                                                                 sparse_output=False))]),\n                                                  ['C', 'D'])])),\n                ('classifier',\n                 LogisticRegression(C=8.115660497752215,\n                                    l1_ratio=0.7084090612742915, max_iter=5000,\n                                    penalty='elasticnet', solver='saga'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('continuous',\n                                                  Pipeline(steps=[('imputer',\n                                                                   IterativeImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['A', 'B', 'E']),\n                                                 ('categorical',\n                                                  Pipeline(steps=[('onehot',\n                                                                   OneHotEncoder(drop='first',\n                                                                                 sparse_output=False))]),\n                                                  ['C', 'D'])])),\n                ('classifier',\n                 LogisticRegression(C=8.115660497752215,\n                                    l1_ratio=0.7084090612742915, max_iter=5000,\n                                    penalty='elasticnet', solver='saga'))])preprocessor: ColumnTransformerColumnTransformer(transformers=[('continuous',\n                                 Pipeline(steps=[('imputer',\n                                                  IterativeImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['A', 'B', 'E']),\n                                ('categorical',\n                                 Pipeline(steps=[('onehot',\n                                                  OneHotEncoder(drop='first',\n                                                                sparse_output=False))]),\n                                 ['C', 'D'])])continuous['A', 'B', 'E']IterativeImputerIterativeImputer()StandardScalerStandardScaler()categorical['C', 'D']OneHotEncoderOneHotEncoder(drop='first', sparse_output=False)LogisticRegressionLogisticRegression(C=8.115660497752215, l1_ratio=0.7084090612742915,\n                   max_iter=5000, penalty='elasticnet', solver='saga')\n\n\n\nOnce the optimal model has been determined, we can pass our validation/test data into the pipeline to generate predicted probabilities for unseen data:\n\n\n# Assessing model performance on unseen data.\nypred = mdl.predict_proba(dfvalid)[:,1]\n\nypred\n\narray([0.23803061, 0.23987571, 0.22497394, 0.2360284 , 0.21692351,\n       0.24979123, 0.22930123, 0.23805811, 0.18848299, 0.2269307 ,\n       0.18739627, 0.21963412, 0.24601412, 0.24592807, 0.26313459,\n       0.19509853, 0.22403892, 0.2644474 , 0.25217899, 0.25114582,\n       0.25275472, 0.25602435, 0.23526247, 0.22682578, 0.21364797,\n       0.31097165, 0.25706994, 0.26917858, 0.21912074, 0.14953379,\n       0.2521859 , 0.19803027, 0.23446292, 0.20239688, 0.22329016,\n       0.23452063, 0.19225738, 0.1971433 , 0.32557197, 0.2366244 ,\n       0.21352434, 0.27294373, 0.25589429, 0.23278834, 0.24858346,\n       0.2058699 , 0.17559173, 0.24556249, 0.22534097, 0.22728177])\n\n\nIn some cases, we may want to pickle our model to share with a third-party for some downstream task. This is straightforward:\n\nimport pickle\n\nwith open(\"my-model.pkl\", \"wb\") as fpkl:\n    pickle.dump(mdl, fpkl, protocol=pickle.HIGHEST_PROTOCOL)"
  },
  {
    "objectID": "posts/correlated-samples/correlated-samples.html",
    "href": "posts/correlated-samples/correlated-samples.html",
    "title": "Generating Correlated Random Samples in Python",
    "section": "",
    "text": "An analysis may require the ability to generate correlated random samples. For example, imagine we have monthly returns for three financial indicators over a 20 year period. We are interested in modeling these returns using parametric distributions for some downstream analysis, perhaps to estimate tail behavior over a large number of samples. In this post, I’ll demonstrate that assuming independence of truly correlated random variables falls short, and how to correctly model the correlation for sample generation. The financial indicator data is available here.\n\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import stats\nimport pandas as pd\n\nindicator_url = \"https://gist.githubusercontent.com/jtrive84/9955433e344ec773e5766657f961fde5/raw/b2e2c99db1e05aeb69186550b9c78cc9412df911/sample_returns.csv\"\n\ndf = pd.read_csv(indicator_url)\n\ndf.head()\n\n\n\n\n\n\n\n\ndate\nus_credit\nus_market\nglobal_market\n\n\n\n\n0\n1/31/2001\n0.035525\n0.0301\n0.022123\n\n\n1\n2/28/2001\n-0.001583\n-0.0950\n-0.091336\n\n\n2\n3/31/2001\n0.002638\n-0.0675\n-0.075002\n\n\n3\n4/30/2001\n0.010607\n0.0738\n0.075063\n\n\n4\n5/31/2001\n0.010448\n0.0035\n-0.010473\n\n\n\n\n\n\n\nThe table contains monthly returns for us_credit, us_market and global_market from 2001-01-31 up to 2023-06-30, but our approach can be extended to any number of financial indicators. Our goal is to find an appropriate parametric distribution for each indicator to use for sample generation. We start by plotting histograms of each indicator to get an idea of the distributional form (symmetric, skewed, etc.). This will dictate which distribution we use to find the best fitting parameters via maximum likelihood. Since there are positive and negative values for each indicator, using a normal distribution is probably a safe bet. If values of the indicators were strictly positive, we would use a distribution with support on \\((0, \\infty)\\) such as gamma, lognormal or weibull.\n\n# Plot histogram for each indicator.\ndf = df.drop(\"date\", axis=1)\n\nindicators = df.columns\n\nfig, ax = plt.subplots(nrows=1, ncols=3, figsize=(9, 3.5), tight_layout=True) \n\nfor ii, ind_name in enumerate(indicators):\n    \n    ind_mean, ind_std = df[ind_name].mean(), df[ind_name].std()\n    label0 = r\"$\\bar x = $\" + f\"{ind_mean:,.4f}\"\n    label1 = r\"$s = $\" + f\"{ind_std:,.4f}\"\n    ax[ii].set_title(ind_name, color=\"#000000\", loc=\"center\", fontsize=9)\n    ax[ii].hist(\n        df[ind_name].values, 18, density=True, alpha=1, color=\"#ff7595\", \n        edgecolor=\"#FFFFFF\", linewidth=1.0\n        )\n    \n    ax[ii].axvline(ind_mean, color=\"#000000\", linewidth=1.25, linestyle=\"--\", label=r\"$\\hat \\mu$\")\n    ax[ii].set_yticklabels([])\n    ax[ii].set_xlabel(\"\")\n    ax[ii].set_ylabel(\"\")\n    ax[ii].tick_params(axis=\"x\", which=\"major\", direction='in', labelsize=6)\n    ax[ii].tick_params(axis=\"x\", which=\"minor\", direction='in', labelsize=6)\n    ax[ii].tick_params(axis=\"y\", which=\"major\", direction='in', labelsize=6)\n    ax[ii].tick_params(axis=\"y\", which=\"minor\", direction='in', labelsize=6)\n    ax[ii].xaxis.set_ticks_position(\"none\")\n    ax[ii].yaxis.set_ticks_position(\"none\")\n    ax[ii].grid(True)   \n    ax[ii].set_axisbelow(True) \n    \n    ax[ii].annotate(\n        label0, xy=(.05, .90), xycoords=\"axes fraction\", ha=\"left\", va=\"bottom\", \n        fontsize=9, rotation=0, weight=\"normal\", color=\"#000000\"\n        ) \n    ax[ii].annotate(\n        label1, xy=(.05, .85), xycoords=\"axes fraction\", ha=\"left\", va=\"bottom\", \n        fontsize=9, rotation=0, weight=\"normal\", color=\"#000000\"\n        ) \n    ax[ii].legend(loc=\"upper right\", fancybox=True, framealpha=1, fontsize=\"x-small\")\n    \nplt.show();\n\n\n\n\nDistribution of returns for each indicator\n\n\n\n\nThe distribution of each indicator appears relatively normal. Given that the parametric form has been identified, we can use Scipy to determine the optimal parameters to fit three separate normal distributions (one per indicator) via maximum likelihood.\n\n\nfrom scipy.stats import norm\n\n# Get normal parameter estimates (mean & standard deviation) via maximum likelihood.\nmu0, std0 = norm.fit(df[\"us_credit\"], method=\"MLE\")\nmu1, std1 = norm.fit(df[\"us_market\"], method=\"MLE\")\nmu2, std2 = norm.fit(df[\"global_market\"], method=\"MLE\")\n\ndparams = {\n    \"us_credit\": {\"mean\": mu0, \"std\": std0},\n    \"us_market\": {\"mean\": mu1, \"std\": std1},\n    \"global_market\": {\"mean\": mu2, \"std\": std2},\n    }\n\nprint(f\"\\n- us_credit    : mean={mu0:,.5f} std=={std0:,.5f}\")\nprint(f\"- us_market    : mean={mu1:,.5f} std=={std1:,.5f}\")\nprint(f\"- global_market: mean={mu2:,.5f} std=={std2:,.5f}\\n\")\n\n\n- us_credit    : mean=0.00015 std==0.02138\n- us_market    : mean=0.00595 std==0.04442\n- global_market: mean=0.00517 std==0.04605\n\n\n\nThe parameter estimates match very closely with the empirical mean and standard deviation overlaid on each histogram. This is because the MLE estimates for the normal distribution are equal to the sample mean and the unadjusted sample variance. Next we overlay the best fitting parametric distribution with each indicator histogram in order to assess the quality of fit.\n\n# Plot histogram for each indicator along with parameterized normal distribution.\nhist_color = \"#ff7595\"\ndist_color = \"#0000FF\"\n\nfig, ax = plt.subplots(nrows=1, ncols=3, figsize=(9.5, 3.5), tight_layout=True) \n\nfor ii, ind_name in enumerate(indicators):\n\n    vals = df[ind_name]\n    mle_mean, mle_std = dparams[ind_name][\"mean\"], dparams[ind_name][\"std\"]\n\n    # Get PDF values associated with distribution.\n    ndist = norm(mle_mean, mle_std)\n    xvals = np.linspace(vals.min() * .90, vals.max() * 1.10, 1000)\n    yvals = ndist.pdf(xvals)\n\n    label0 = r\"$\\hat \\mu = $\" + f\"{mle_mean:,.4f}\"\n    label1 = r\"$\\hat \\sigma = $\" + f\"{mle_std:,.4f}\"\n    \n    ax[ii].set_title(ind_name, color=\"#000000\", loc=\"center\", fontsize=9)\n    ax[ii].hist(\n        df[ind_name].values, 18, density=True, alpha=1, color=hist_color, \n        edgecolor=\"#FFFFFF\", linewidth=1.0\n        )\n\n    # Plot normal distribution.\n    ax[ii].plot(xvals, yvals, linewidth=1.5, color=dist_color, linestyle=\"--\")\n    ax[ii].set_yticklabels([])\n    ax[ii].set_xlabel(\"\")\n    ax[ii].set_ylabel(\"\")\n    ax[ii].tick_params(axis=\"x\", which=\"major\", direction='in', labelsize=6)\n    ax[ii].tick_params(axis=\"x\", which=\"minor\", direction='in', labelsize=6)\n    ax[ii].tick_params(axis=\"y\", which=\"major\", direction='in', labelsize=6)\n    ax[ii].tick_params(axis=\"y\", which=\"minor\", direction='in', labelsize=6)\n    ax[ii].xaxis.set_ticks_position(\"none\")\n    ax[ii].yaxis.set_ticks_position(\"none\")\n    ax[ii].grid(True)   \n    ax[ii].set_axisbelow(True) \n    \n    ax[ii].annotate(\n        label0, xy=(.05, .90), xycoords=\"axes fraction\", ha=\"left\", va=\"bottom\", \n        fontsize=9, rotation=0, weight=\"normal\", color=\"#000000\"\n        ) \n    ax[ii].annotate(\n        label1, xy=(.05, .85), xycoords=\"axes fraction\", ha=\"left\", va=\"bottom\", \n        fontsize=9, rotation=0, weight=\"normal\", color=\"#000000\"\n        ) \n    #ax[ii].legend(loc=\"upper right\", fancybox=True, framealpha=1, fontsize=\"x-small\")\nplt.suptitle(\"Indicator histograms w/ parametric overlay\", fontsize=10)\nplt.show();\n\n\n\n\n\nIndicator histograms with parametric overlay\n\n\n\n\nThe distributions in each case enclose the original histograms pretty well, with decent tail coverage in each instance. To demonstrate the approach, we next generate independent random samples from each indicator.\n\n\n# Specify number of samples to generate.\nnbr_sims = 500\n\n# Copying dparams from previous cell.\ndparams = {\n    \"us_credit\": {\"mean\": mu0, \"std\": std0},\n    \"us_market\": {\"mean\": mu1, \"std\": std1},\n    \"global_market\": {\"mean\": mu2, \"std\": std2},\n    }\n\n# us_credit.\nmean0, std0 = dparams[\"us_credit\"][\"mean\"], dparams[\"us_credit\"][\"std\"]\nrv0 = norm(mean0, std0)\n\n# us_market.\nmean1, std1 = dparams[\"us_market\"][\"mean\"], dparams[\"us_market\"][\"std\"]\nrv1 = norm(mean1, std1)\n\n# global_market.\nmean2, std2 = dparams[\"global_market\"][\"mean\"], dparams[\"global_market\"][\"std\"]\nrv2 = norm(mean2, std2)\n\n# Create DataFrame to hold simulated indicators. \ndfsims1 = pd.DataFrame(\n    np.vstack([rv0.rvs(nbr_sims), rv1.rvs(nbr_sims), rv2.rvs(nbr_sims)]).T,\n    columns=indicators\n    )\n\ndfsims1.describe()\n\n\n\n\n\n\n\n\nus_credit\nus_market\nglobal_market\n\n\n\n\ncount\n500.000000\n500.000000\n500.000000\n\n\nmean\n0.000558\n0.012024\n0.006257\n\n\nstd\n0.022043\n0.042455\n0.045772\n\n\nmin\n-0.072494\n-0.115241\n-0.148560\n\n\n25%\n-0.014534\n-0.017264\n-0.024564\n\n\n50%\n-0.000501\n0.014443\n0.007020\n\n\n75%\n0.014217\n0.042519\n0.038046\n\n\nmax\n0.069504\n0.149524\n0.132896\n\n\n\n\n\n\n\nHere we assume the value that an indicator takes on is independent of all other indicators. This is almost surely not the case. Let’s check how correlated the 3 selected indicators are within the original sample:\n\n\ndf.corr()\n\n\n\n\n\n\n\n\nus_credit\nus_market\nglobal_market\n\n\n\n\nus_credit\n1.000000\n-0.099877\n-0.037652\n\n\nus_market\n-0.099877\n1.000000\n0.955514\n\n\nglobal_market\n-0.037652\n0.955514\n1.000000\n\n\n\n\n\n\n\nDo the independent samples exhibit the same correlation?\n\n\ndfsims1.corr()\n\n\n\n\n\n\n\n\nus_credit\nus_market\nglobal_market\n\n\n\n\nus_credit\n1.000000\n-0.072533\n-0.066023\n\n\nus_market\n-0.072533\n1.000000\n-0.044359\n\n\nglobal_market\n-0.066023\n-0.044359\n1.000000\n\n\n\n\n\n\n\nNot surprisingly, us_market and global_market are over 95% correlated in the original data. This is not exhibited within the set of non-correlated samples. With such high correlation, it is not reasonable to assume independence across indicators. We need instead to find a way to generate correlated random samples. This can be accomplished using Numpy’s multivariate normal distribution.\nAs a brief aside, the significance of a correlation between two random variables depends on the sample size. To test if the correlation between two variables demonstrates a linear relationship at the 5% significance level, we compute:\n\\[\n|\\rho| \\gt 2 / \\sqrt{n},\n\\]\nIn our case, there are 270 samples, therefore \\(n = \\sqrt{270} \\approx 16.4\\). Thus, in order for the correlation between two indicators to be significant at the 5% level, it needs to be greater than \\(2 / 16.4 \\approx .12\\), which is clearly the case for global_market-us_market.\n\n\n# Generate correlated random samples.\n# Specify number of simulations to generate.\nnbr_sims = 500\n\n# Create vector of means.\nmeans = [mean0, mean1, mean2]\n\n# Bind reference to covariance matrix.\nV = df.cov().values\n\ndfsims2 = pd.DataFrame(\n    np.random.multivariate_normal(means, V, nbr_sims),\n    columns=indicators\n    )\n\ndfsims2.describe()\n\n\n\n\n\n\n\n\nus_credit\nus_market\nglobal_market\n\n\n\n\ncount\n500.000000\n500.000000\n500.000000\n\n\nmean\n0.000547\n0.005966\n0.005387\n\n\nstd\n0.021886\n0.042421\n0.044833\n\n\nmin\n-0.062250\n-0.097373\n-0.137454\n\n\n25%\n-0.013780\n-0.024495\n-0.027463\n\n\n50%\n0.000788\n0.007268\n0.006098\n\n\n75%\n0.015203\n0.033833\n0.035412\n\n\nmax\n0.067324\n0.136830\n0.148773\n\n\n\n\n\n\n\nThe min and max values for each indicator seem to align with what was observed in the original data. Let’s verify that the samples in dfsims2 are correlated.\n\n\ndfsims2.corr()\n\n\n\n\n\n\n\n\nus_credit\nus_market\nglobal_market\n\n\n\n\nus_credit\n1.000000\n-0.084854\n-0.038219\n\n\nus_market\n-0.084854\n1.000000\n0.944791\n\n\nglobal_market\n-0.038219\n0.944791\n1.000000\n\n\n\n\n\n\n\nThis matches closely with the original correlation profile. Let’s visualize the correlation in the generated random samples by comparing indicator samples from the independent simulated dataset vs. the dependent simulated dataset.\n\n# Generate pair-wise scatter plots for each indicator for independent and dependent draws. \n\ndindices = {\n    (0, 0): {\"data\": \"independent\", \"x\": \"us_credit\", \"y\": \"us_market\"},\n    (0, 1): {\"data\": \"independent\", \"x\": \"global_market\", \"y\": \"us_credit\"},\n    (0, 2): {\"data\": \"independent\", \"x\": \"us_market\", \"y\": \"global_market\"},\n    (1, 0): {\"data\": \"dependent\", \"x\": \"us_credit\", \"y\": \"us_market\"},\n    (1, 1): {\"data\": \"dependent\", \"x\": \"global_market\", \"y\": \"us_credit\"},\n    (1, 2): {\"data\": \"dependent\", \"x\": \"us_market\", \"y\": \"global_market\"},\n    }\n\n\nfig, ax = plt.subplots(nrows=2, ncols=3, figsize=(10, 6.5), tight_layout=True) \n\nfor kk, vv in dindices.items():\n    \n    ii, jj = kk\n    data_desc, x_desc, y_desc = vv[\"data\"], vv[\"x\"], vv[\"y\"]\n\n    if data_desc == \"independent\":\n        xx = dfsims1[x_desc].values\n        yy = dfsims1[y_desc].values\n    else:\n        xx = dfsims2[x_desc].values\n        yy = dfsims2[y_desc].values\n        \n    titlestr = f\"{data_desc}: {x_desc} vs. {y_desc}\"\n    ax[ii, jj].set_title(titlestr, color=\"#000000\", loc=\"center\", weight=\"bold\", fontsize=7)\n    ax[ii, jj].scatter(\n        xx, yy, s=30, c=\"#FFFFFF\", alpha=.85, edgecolor=\"#000000\", linewidth=.75)\n\n    # ax[ii].set_yticklabels([])\n    ax[ii, jj].set_xlabel(x_desc, fontsize=7)\n    ax[ii, jj].set_ylabel(y_desc, fontsize=7)\n    ax[ii, jj].tick_params(axis=\"x\", which=\"major\", direction='in', labelsize=6)\n    ax[ii, jj].tick_params(axis=\"x\", which=\"minor\", direction='in', labelsize=6)\n    ax[ii, jj].tick_params(axis=\"y\", which=\"major\", direction='in', labelsize=6)\n    ax[ii, jj].tick_params(axis=\"y\", which=\"minor\", direction='in', labelsize=6)\n    ax[ii, jj].xaxis.set_ticks_position(\"none\")\n    ax[ii, jj].yaxis.set_ticks_position(\"none\")\n    ax[ii, jj].grid(True)   \n    ax[ii, jj].set_axisbelow(True) \n\nplt.show();\n\n\n\n\nScatterplots by indicator pair for correlated and uncorrelated samples.\n\n\n\n\nIn the case of global_market-us_market (bottom right), the dependent plot captures the correlation inherent in the original data."
  },
  {
    "objectID": "posts/data-table-aggregation/data-table-aggregation.html",
    "href": "posts/data-table-aggregation/data-table-aggregation.html",
    "title": "Aggregation with data.table",
    "section": "",
    "text": "data.table is a highly optimizedlibrary which improves upon R’s standard data.frame via the data.table object. data.tables can be used in any scenario that accepts a data.frame, and thus can be used as a replacement for the standard data.frame is all scenarios.\nDue to the highly optimized nature of data.table, aggregate operations are much faster compared with standard R builtins like aggregate. However, the syntax takes a little getting used to. In this article, we’ll demonstrate how to perform various common aggregate operations using the data.table library.\n\nData\nThroughout this article, we’ll work with a simulated loss frequency dataset:\nlibrary(\"data.table\")\nset.seed(516)\n\nDT0 = CJ(\n    district=c(1,2 ,3 ,4), group=c(\"A\", \"B\", \"C\", \"D\"),  \n    age=c(\"&lt;25\", \"25-34\", \"35-49\", \"&gt;50\")\n    )\nDT0[,`:=`(\n    claims=rpois(nrow(DT), 100), \n    holders=sample(1000:5000, size=nrow(DT), replace=FALSE)\n    )]\n\n# Table of adjustment factors. \nDT1 = data.table(\n    age=c(\"&lt;25\", \"25-34\", \"35-49\", \"&gt;50\"),\n    adj=c(1.50, 1.075, .85, .975),\n    stringsAsFactors=FALSE\n    )\nDT = DT1[DT0, on=\"age\"]\nDT[,claims:=as.integer(claims * adj)]\nDT[,adj:=NULL]\nTaking a look at the first few records yields:\n     age district group claims holders\n1: 25-34        1     A     95    4359\n2: 35-49        1     A     92    3764\n3:   &lt;25        1     A    157    4613\n4:   &gt;50        1     A    111    2529\n5: 25-34        1     B     90    2105\n6: 35-49        1     B     97    3379\n\n\nAggregate Operations\nBefore proceeding, we should highlight data.table’s general calling convention, that when understood, can be used to translate data.table expressions of arbitrary complexity into 3 steps:\n\nDT[i, j, by]\n\nWhich translates to:\n\nTake DT, subset rows using i, then calculate j grouped by by.\n\nThis can be represented in a diagram as follows:\nDT[ i, j, by ] # + extra arguments\n    |  |  |\n    |  |   -------&gt; grouped by what?\n    |   -------&gt; what to do?\n     ---&gt; on which rows?\nThis may not be immediately clear, but should start to make sense as we look at examples.\n\n\nReturning a Single Value from a Single Column\nReferring to DT from above, the total number of policyholders can be determined as:\n&gt; DT[,sum(holders)]\n[1] 190885\nTo compute the total number of policyholders and claims, run:\n&gt; DT[,.(holders=sum(holders), claims=sum(claims))]\n   holders claims\n1:  190885   7018\nThis returns a 1-row data.table with the total number of holders and claims. To change fieldnames in the resulting table, change the name on the left-hand side for the desired field within the sum expression:\n&gt; DT[,.(total_holders=sum(holders), total_claims=sum(claims))]\n   total_holders total_claims\n1:        190885         7018 \n\n\nAssigning an Aggregate to Each Row of an Existing Table\nIt may be necessary to assign an aggregate value from a table to each record in the same table. This can be accomplished using the := operator. Note that when computing aggregate operations that utilize :=, the number of rows in the resulting table will not change. Next we assign the total number of claims to each row in DT in a column identified as total_claims:\n&gt; DT[,total_claims:=sum(claims)]\n&gt; head(DT)\n     age district group claims holders total_claims\n1: 25-34        1     A     95    4359         7018\n2: 35-49        1     A     92    3764         7018\n3:   &lt;25        1     A    157    4613         7018\n4:   &gt;50        1     A    111    2529         7018\n5: 25-34        1     B     90    2105         7018\n6: 35-49        1     B     97    3379         7018\nIf we want to aggregate two or more columns, it is necessary to use a slight variation of the := operator. Next we include total_claims and total_holders:\n&gt; DT[, \":=\" (total_claims=sum(claims), total_holders=sum(holders))]\n&gt; head(DT)\n     age district group claims holders total_claims total_holders\n1: 25-34        1     A     95    4359         7018        190885\n2: 35-49        1     A     92    3764         7018        190885\n3:   &lt;25        1     A    157    4613         7018        190885\n4:   &gt;50        1     A    111    2529         7018        190885\n5: 25-34        1     B     90    2105         7018        190885\n6: 35-49        1     B     97    3379         7018        190885\n&gt; \nThis differs from the standard syntax in two ways: First, the := operator is specified upfront, and is surrounded by quotes (\":=\"). Second, when specifying the columns to aggregate, we use =, not :=, since this is already specified upfront. It is also possible to use the modified syntax for adding a single column. The following expressions are identical:\nDT[,total_claims:=sum(claims)]\n\nDT[,\":=\" (total_claims=sum(claims))]\n\n\nAggregate Operations by Group\nTo perform aggregate operations by group, the syntax introduced in the previous examples remains unchanged: The only difference is the addition of the by keyword.\nIn DT, there are four distinct levels in each of age, district and group:\n&gt; table(DT$age)\n\n  &lt;25   &gt;50 25-34 35-49 \n   16    16    16    16 \n&gt; table(DT$district)\n\n 1  2  3  4 \n16 16 16 16 \n&gt; table(DT$district)\n\n A  B  C  D \n16 16 16 16  \nTo calculate the total number of claims by each level in age, run:\n&gt; DT[,sum(claims), by=age]\n     age   V1\n1: 25-34 1643\n2: 35-49 1363\n3:   &lt;25 2410\n4:   &gt;50 1602\nThis returns total number of claims in a single unnamed column (identified as V1 by default). In order to preserve column names, we use the syntax from before, updated with by. In what follows, we compute the total number of claims and policyholders by age while preserving column names:\n&gt; DT[,.(claims=sum(claims), holders=sum(holders)), by=\"age\"]\n     age claims holders\n1: 25-34   1643   42109\n2: 35-49   1363   52062\n3:   &lt;25   2410   48932\n4:   &gt;50   1602   47782\nWhen it is necessary to aggregate by two or more columns, we supply a vector of column names over which to aggregate. To determine the total number of claims and policyholders by age and district, do:\n&gt; DT[,.(claims=sum(claims), holders=sum(holders)), by=c(\"age\", \"district\")]\n      age district claims holders\n 1: 25-34        1    384   10080\n 2: 35-49        1    345   13576\n 3:   &lt;25        1    649   13665\n 4:   &gt;50        1    423   10747\n 5: 25-34        2    422   12495\n 6: 35-49        2    343   13462\n 7:   &lt;25        2    587   11348\n 8:   &gt;50        2    424   11888\n 9: 25-34        3    420    8101\n10: 35-49        3    330   14933\n11:   &lt;25        3    582   13167\n12:   &gt;50        3    385   13730\n13: 25-34        4    417   11433\n14: 35-49        4    345   10091\n15:   &lt;25        4    592   10752\n16:   &gt;50        4    370   11417\n\n\nAssigning Aggregate Values by Group to Each Row in Existing Table\nAs before, aggrergate operations by group can be assigned to an existing table. Next we assign the average number of claims and policyholders by age to each record in the original dataset (note that I’ve removed total_claims and total_holders to simplify viewing the tables. It is fine to keep them in if you’re following along):\n&gt; DT[,\":=\"(avg_nbr_claims=mean(claims), avg_nbr_holders=mean(holders)), by=\"age\"]\n&gt; head(DT)\n     age district group claims holders avg_nbr_claims avg_nbr_holders\n1: 25-34        1     A     95    4359       102.6875        2631.812\n2: 35-49        1     A     92    3764        85.1875        3253.875\n3:   &lt;25        1     A    157    4613       150.6250        3058.250\n4:   &gt;50        1     A    111    2529       100.1250        2986.375\n5: 25-34        1     B     90    2105       102.6875        2631.812\n6: 35-49        1     B     97    3379        85.1875        3253.875\nThe average number of claims and policyholders by district and group can be computed as:\n&gt; head(DT, 25)\n      age district group claims holders avg_nbr_claims avg_nbr_holders\n 1: 25-34        1     A     95    4359         113.75         3816.25\n 2: 35-49        1     A     92    3764         113.75         3816.25\n 3:   &lt;25        1     A    157    4613         113.75         3816.25\n 4:   &gt;50        1     A    111    2529         113.75         3816.25\n 5: 25-34        1     B     90    2105         112.75         2277.50\n 6: 35-49        1     B     97    3379         112.75         2277.50\n 7:   &lt;25        1     B    160    1359         112.75         2277.50\n 8:   &gt;50        1     B    104    2267         112.75         2277.50\n 9: 25-34        1     C     92    1942         111.75         2598.25\n10: 35-49        1     C     68    2765         111.75         2598.25\n11:   &lt;25        1     C    178    4626         111.75         2598.25\n12:   &gt;50        1     C    109    1060         111.75         2598.25\n13: 25-34        1     D    107    1674         112.00         3325.00\n14: 35-49        1     D     88    3668         112.00         3325.00\n15:   &lt;25        1     D    154    3067         112.00         3325.00\n16:   &gt;50        1     D     99    4891         112.00         3325.00\n17: 25-34        2     A     96    4765         111.75         2748.75\n18: 35-49        2     A     90    1526         111.75         2748.75\n19:   &lt;25        2     A    144    2877         111.75         2748.75\n20:   &gt;50        2     A    117    1827         111.75         2748.75\n21: 25-34        2     B    118    2059         117.75         2778.00\n22: 35-49        2     B     89    3323         117.75         2778.00\n23:   &lt;25        2     B    148    2687         117.75         2778.00\n24:   &gt;50        2     B    116    3043         117.75         2778.00\n25: 25-34        2     C    105    3863         110.50         3931.00\n      age district group claims holders avg_nbr_claims avg_nbr_holders\n\n\nAlternative Aggregation Specification\nI will briefly discuss an alternative aggregation approach that greatly simplifies performing aggregate operations on large tables.\nIt’s hard to visualize with our example, but imagine having a table with 10’s or 100’s of columns. Performing aggregate operations as demonstrated would quickly become untenable. To address this, we can take advantage of R’s special symbols. We use .SD as a placeholder for the columns to aggregate, then pass a vector of these column names to .SDcols. .SD by default gets assigned all columns except the columns mentioned in by=. For example, aggregating claims and holders by age and district can be accomplished as follows:\n&gt; aggColumns = c(\"claims\", \"holders\")\n&gt; keyColumns = c(\"age\", \"district\")\n&gt; DT[,lapply(.SD, sum, na.rm=TRUE), by=keyColumns, .SDcols=aggColumns]\n      age district claims holders\n 1: 25-34        1    384   10080\n 2: 35-49        1    345   13576\n 3:   &lt;25        1    649   13665\n 4:   &gt;50        1    423   10747\n 5: 25-34        2    422   12495\n 6: 35-49        2    343   13462\n 7:   &lt;25        2    587   11348\n 8:   &gt;50        2    424   11888\n 9: 25-34        3    420    8101\n10: 35-49        3    330   14933\n11:   &lt;25        3    582   13167\n12:   &gt;50        3    385   13730\n13: 25-34        4    417   11433\n14: 35-49        4    345   10091\n15:   &lt;25        4    592   10752\n16:   &gt;50        4    370   11417\nUsing this approach, we can write programs that assign which columns to aggregate dynamically at the point of execution.\n\nSummary\nA few things to keep in mind when performing aggregate operations in data.table:\n\n.SD by default gets assigned all columns except the columns mentioned in by=.\n.SDcols represents the fields that should be aggregated (columns must be numeric).\nby=is similiar to SQL GROUP BY. Specifies the keys over which data should be aggregated.\nIf you keep the := operator, the results will not be aggregated. Instead the aggregate amount of the target column will be appended to each record in the original data.table."
  },
  {
    "objectID": "posts/deriv-normal-equations/deriv-normal-equations.html",
    "href": "posts/deriv-normal-equations/deriv-normal-equations.html",
    "title": "Derivation of the Normal Equations",
    "section": "",
    "text": "The Normal Equations, represented in matrix form as\n\\[\n(X^{T}X)\\hat{\\beta} = X^{T}y\n\\]\nare utilized in determining coefficient estimates associated with regression models. The matrix form is a compact representation of the model specification commonly represented as\n\\[\ny = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\cdots + \\beta_{k}x_{k} + \\varepsilon\n\\]\nwhere \\(\\epsilon\\) represents the error term, and\n\\[\n\\sum_{i=1}^{n} \\varepsilon_{i} = 0.\n\\]\nFor a dataset with \\(n\\) records by \\(k\\) explanatory variables per record, the components of the Normal Equations are:\n\n\\(\\hat{\\beta} = (\\hat{\\beta}_{0},\\hat{\\beta}_{1},\\cdots,\\hat{\\beta}_{k})^{T}\\), a vector of \\((k+1)\\) coefficents (one for each of the k explanatory variables plus one for the intercept term)\n\n\\(X\\) , an \\(n\\) by \\((k+1)\\)-dimensional matrix of explanatory variables, with the first column consisting entirely of 1’s\n\n\\({y} = (y_{1}, y_{2},...,y_{n})\\), the response\n\nThe task is to solve for the \\((k+1)\\) \\(\\beta_{j}\\)’s such that \\(\\hat{\\beta}_{0}, \\hat{\\beta}_{1},...,\\hat{\\beta}_{k}\\) minimize\n\\[\n\\sum_{i=1}^{n} \\hat{\\varepsilon}^{2}_{i} = \\sum_{i=1}^{n} (y_{i} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{i1} - \\hat{\\beta}_{2}x_{i2} - \\cdots - \\hat{\\beta}_{k}x_{ik})^2.\n\\]\nThe Normal Equations can be derived using Least-Squares and Maximum likelihood Estimation.\n\nLeast-Squares Derivation\nUnlike Maximum Likelihood derivation, the Least-Squares approach requires no distributional assumption. For \\(\\hat{\\beta}_{0}, \\hat{\\beta}_{1}, \\cdots ,\\hat{\\beta}_{k}\\), we seek estimators that minimize the sum of squared deviations between the \\(n\\) response values and the predicted values, \\(\\hat{y}\\). The objective is to minimize\n\\[\n\\sum_{i=1}^{n} \\hat{\\varepsilon}^{2}_{i} = \\sum_{i=1}^{n} (y_{i} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{i1} - \\hat{\\beta}_{2}x_{i2} - \\cdots - \\hat{\\beta}_{k}x_{ik})^2.\n\\]\nUsing matrix notation, our model can be represented as \\(y = X^{T}\\beta + \\varepsilon\\). Isolating and squaring the error term yields\n\\[\n\\hat \\varepsilon^T \\hat \\varepsilon =  \\sum_{i=1}^{n} (y - X\\hat{\\beta})^{T}(y - X\\hat{\\beta}).\n\\]\nExpanding the right-hand side and combining terms results in\n\\[\n\\hat \\varepsilon^T \\hat \\varepsilon = y^{T}y - 2y^{T}X\\hat{\\beta} + \\hat{\\beta}X^{T}X\\hat{\\beta}\n\\]\nTo find the value of \\(\\hat{\\beta}\\) that minimizes \\(\\hat \\varepsilon^T \\hat \\varepsilon\\), we differentiate \\(\\hat \\varepsilon^T \\hat \\varepsilon\\) with respect to \\(\\hat{\\beta}\\), and set the result to zero:\n\\[\n\\frac{\\partial \\hat{\\varepsilon}^{T}\\hat{\\varepsilon}}{\\partial \\hat{\\beta}} = -2X^{T}y + 2X^{T}X\\hat{\\beta} = 0\n\\]\nWhich can then be solved for \\(\\hat{\\beta}\\):\n\\[\n\\hat{\\beta} = {(X^{T}X)}^{-1}{X}^{T}y\n\\]\nSince \\(\\hat{\\beta}\\) minimizes the sum of squares, \\(\\hat{\\beta}\\) is called the Least-Squares Estimator.\n\n\nMaximum Likelihood Derivation\nFor the Maximum Likelihood derivation, \\(X\\), \\(y\\) and \\(\\hat{\\beta}\\) are the same as described in the Least-Squares derivation, and the model still follows the form\n\\[\ny = X^{T}\\beta + \\varepsilon\n\\]\nbut now we assume the \\(\\varepsilon_{i}\\) are \\(iid\\) and follow a zero-mean normal distribution:\n\\[\nN(\\varepsilon_{i}; 0, \\sigma^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} e^{- \\frac{(y_{i}-X^{T}\\hat{\\beta})^{2}}{2\\sigma^{2}}}.\n\\]\nIn addition, the responses, \\(y_{i}\\), are each assumed to follow a normal distribution. For \\(n\\) observations, the likelihood function is\n\\[\nL(\\beta) = \\Big(\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\Big)^{n} e^{-(y-X\\beta)^{T}(y-X\\beta)/2\\sigma^{2}}.\n\\]\nThe Log-Likelihood is then\n\\[\n\\mathrm{Ln}(L(\\beta)) = -\\frac{n}{2}\\mathrm{Ln}(2\\pi) -\\frac{n}{2}\\mathrm{Ln}(\\sigma^{2})-\\frac{1}{2\\sigma^{2}}(y-X\\beta)^{T}(y-X\\beta).\n\\]\nTaking derivatives with respect to \\(\\beta\\) and setting the result equal to zero yields\n\\[\n\\frac{\\partial \\mathrm{Ln}(L(\\beta))}{\\partial \\beta} = -2X^{T}y -2X^{T}X\\beta = 0.\n\\]\nRearranging and solving for \\(\\beta\\) we obtain\n\\[\n\\hat{\\beta} = {(X^{T}X)}^{-1}{X}^{T}y,\n\\]\nwhich is the same result obtained via Least Squares."
  },
  {
    "objectID": "posts/direct-method-eigenrankings/direct-method-eigenrankings.html",
    "href": "posts/direct-method-eigenrankings/direct-method-eigenrankings.html",
    "title": "A Data-Driven Approach to Ranking Teams in Uneven Paired Competition",
    "section": "",
    "text": "I recently came across The Perron-Frobenius and the Ranking of Football Teams, an interesting paper in which the author describes four different methods to rank teams in uneven paired competition. He goes on to show how each of these methods depends in some way on the Perron-Frobenius theorem. The Perron-Frobenius theorem provides key insights into the structure of non-negative matrices, especially in terms of their largest eigenvalue and associated eigenvector. For irreducible non-negative matrices, the theorem guarantees the existence of a dominant eigenvalue that is real, simple, and larger than all others in magnitude, with a corresponding non-negative eigenvector.\nAn uneven paired competition is one in which the outcome of competition between pairs of teams is known, but the pairings are not evenly matched, meaning the competition is not a round robin in which each team is paired with every other team an equal number of times. A good example is regular season football in-conference play for any of the major NCAA Division I conferences: For the 2023 season, the Big 12 had 14 teams, but each team had only 9 conference games.\nHere we focus on the first ranking method, which the author refers to as the “direct method”. The direct method formulates the ranking approach as a linear eigenvalue problem which makes direct use of the Perron-Frobenius theorem. For each team under consideration, the goal is to assign a score to each team based on its interactions with other teams, with the goal that the assigned score reflect both the interactions as well as the strength of opponents. We will then compare our data-driven ranking approach with the final regular season standings and assess how they line up. A similar exercise will be performed focusing on the 2021 MLB regular season. \n\nCreating the Adjacency Matrix\nIt is first necessary to construct the adjacency matrix in order to encode interactions between teams. Big 12 2023 regular season football results were obtained here. Within the matrix, the value in cell \\(a_{ij}\\) is set to 1 if team \\(i\\) defeated team \\(j\\), and 0 otherwise. For games that resulted in tie, \\(a_{ij} = \\frac{1}{2}\\), but there were no such cases in 2023 Big 12 regular season conference play.\nThe regular season rankings and adjacency matrix can be downloaded from GitHub (links available in the next cell):\n\n%load_ext watermark\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\nfrom numpy.linalg import eig\n\nnp.set_printoptions(suppress=True, precision=5)\npd.options.mode.chained_assignment = None\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\n\n\nbig12_matrix_url = \"https://gist.githubusercontent.com/jtrive84/b9b4ff8620f90045a0377f27ec4eb50f/raw/e6923583530edadbe9da1a1f4821e415d8a7e6f2/2023-big-12-adjacency.csv\"\nbig12_rankings_url = \"https://gist.githubusercontent.com/jtrive84/0207b8fd18a05e096a89498290b08d4a/raw/462d2b1bef52d96ae20e077f55501bfa23951ae4/2023-big-12-rankings.csv\"\n\n# ------------------------------------------------------------------------------\n\n%watermark --python --conda --hostname --machine --iversions\n\n\nPython implementation: CPython\nPython version       : 3.11.10\nIPython version      : 8.28.0\n\nconda environment: py311\n\nCompiler    : MSC v.1941 64 bit (AMD64)\nOS          : Windows\nRelease     : 10\nMachine     : AMD64\nProcessor   : Intel64 Family 6 Model 170 Stepping 4, GenuineIntel\nCPU cores   : 22\nArchitecture: 64bit\n\nHostname: JTRIZPC11\n\nnumpy     : 2.1.0\nmatplotlib: 3.9.2\npandas    : 2.2.2\nnetworkx  : 3.3\n\n\n\n\nBig 12 regular season rankings for 2023:\n\n\nranks_big12 = pd.read_csv(big12_rankings_url)\n\nranks_big12.head(15)\n\n\n\n\n\n\n\n\nteam\nconf_wins\nconf_losses\noverall_wins\noverall_losses\nconf_win_pct\noverall_win_pct\n\n\n\n\n0\nTexas\n8\n1\n12\n2\n0.889\n0.857\n\n\n1\nOklahoma State\n7\n2\n10\n4\n0.778\n0.714\n\n\n2\nOklahoma\n7\n2\n10\n3\n0.778\n0.769\n\n\n3\nIowa State\n6\n3\n7\n6\n0.667\n0.538\n\n\n4\nKansas State\n6\n3\n9\n4\n0.667\n0.692\n\n\n5\nWest Virginia\n6\n3\n9\n4\n0.667\n0.692\n\n\n6\nTexas Tech\n5\n4\n7\n6\n0.556\n0.538\n\n\n7\nKansas\n5\n4\n9\n4\n0.556\n0.692\n\n\n8\nUCF\n3\n6\n6\n7\n0.333\n0.462\n\n\n9\nTCU\n3\n6\n5\n7\n0.333\n0.417\n\n\n10\nHouston\n2\n7\n4\n8\n0.222\n0.333\n\n\n11\nBYU\n2\n7\n5\n7\n0.222\n0.417\n\n\n12\nBaylor\n2\n7\n3\n9\n0.222\n0.250\n\n\n13\nCincinnati\n1\n8\n3\n9\n0.111\n0.250\n\n\n\n\n\n\n\n\nThe adjacency matrix considers only conference play (non-conference games excluded):\n\n\nadj_big12 = pd.read_csv(big12_matrix_url)\n\nadj_big12.head(15)\n\n\n\n\n\n\n\n\nUnnamed: 0\nBaylor\nBYU\nCincinnati\nHouston\nIowa State\nKansas\nKansas State\nOklahoma\nOklahoma State\nTCU\nTexas\nTexas Tech\nUCF\nWest Virginia\n\n\n\n\n0\nBaylor\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n1\nBYU\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n2\nCincinnati\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\nHouston\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\nIowa State\n1\n1\n1\n0\n0\n0\n1\n0\n1\n1\n0\n0\n0\n0\n\n\n5\nKansas\n0\n1\n1\n0\n1\n0\n0\n1\n0\n0\n0\n0\n1\n0\n\n\n6\nKansas State\n1\n0\n0\n1\n0\n1\n0\n0\n0\n1\n0\n1\n1\n0\n\n\n7\nOklahoma\n0\n1\n1\n0\n1\n0\n0\n0\n0\n1\n1\n0\n1\n1\n\n\n8\nOklahoma State\n0\n1\n1\n1\n0\n1\n1\n1\n0\n0\n0\n0\n0\n1\n\n\n9\nTCU\n1\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n10\nTexas\n1\n1\n0\n1\n1\n1\n1\n0\n0\n1\n0\n1\n0\n0\n\n\n11\nTexas Tech\n1\n0\n0\n1\n0\n1\n0\n0\n0\n1\n0\n0\n1\n0\n\n\n12\nUCF\n0\n0\n1\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\n13\nWest Virginia\n1\n1\n1\n0\n0\n0\n0\n0\n0\n1\n0\n1\n1\n0\n\n\n\n\n\n\n\n\nFor each row in adj_big12, 1 indicates that team at row \\(i\\) defeated the team in column \\(j\\). For example, Oklahoma defeated BYU 31-24 in 2023, so the value at the intersection of row Oklahoma and column BYU is 1. The value at the intersection of row BYU and column Oklahoma is 0, since BYU did not defeat Oklahoma in 2023, and they only faced each other in one contest.\nThe sum of each row in the adjacency matrix represents the number of regular season wins in conference play for a given team. Texas was 8-1 in 2023 regular season conference play, therefore the sum of the Texas row is 8. The columnar sum represents the number of losses for a given team (for Texas, this is 1).\nWe can use NetworkX to visualize the relationships encoded in the adjacency matrix (each node label corresponds to the alphabetical enumeration of teams: 0=Baylor, 1=BYU, … 13=West Virginia). Edges indicate whether team \\(i\\) and team \\(j\\) faced each other in a regular season contest:\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Create adjacency matrix as Numpy array. \n# team_names = dfadj[\"Unnamed: 0\"].values\nA = adj_big12.drop(\"Unnamed: 0\", axis=1).values\nG = nx.from_numpy_array(A)\n\nfig, ax = plt.subplots(1, 1, figsize=(7.5, 5), tight_layout=True)\nax.set_title(\n    \"2023 Big-12 Regular Season Football Matchups\", \n    color=\"#000000\", loc=\"center\", weight=\"normal\", fontsize=9\n)\nnx.draw_networkx(\n    G, node_color=\"#E02C70\", node_size=350, ax=ax, with_labels=True, \n    edge_color=\"grey\", width=.25, pos=nx.spring_layout(G, seed=516)\n)\n\n\n\n\n\n\n\n\n\nThe adjacency matrix, \\(A\\):\n\n\nA\n\narray([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0],\n       [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0],\n       [1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0],\n       [0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1],\n       [0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1],\n       [1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0],\n       [1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0],\n       [0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0]])\n\n\n\nIf we create an initial ranking vector \\(r_{0}\\) with all values set to 1, the the \\(i^{th}\\) component of \\(Ar_{0}\\) is the winning percentage for team \\(i\\) (after dividing by the number of games played):\n\n\nr0 = np.ones(14)\nwin_pcts = A @ r0 / 9\n\npairs = zip(adj_big12.columns[1:], win_pcts.tolist())\n\nfor tt in pairs:\n    print(tt)\n\n('Baylor', 0.2222222222222222)\n('BYU', 0.2222222222222222)\n('Cincinnati', 0.1111111111111111)\n('Houston', 0.2222222222222222)\n('Iowa State', 0.6666666666666666)\n('Kansas', 0.5555555555555556)\n('Kansas State', 0.6666666666666666)\n('Oklahoma', 0.7777777777777778)\n('Oklahoma State', 0.7777777777777778)\n('TCU', 0.3333333333333333)\n('Texas', 0.8888888888888888)\n('Texas Tech', 0.5555555555555556)\n('UCF', 0.3333333333333333)\n('West Virginia', 0.6666666666666666)\n\n\n\nThis aligns with values in the conf_win_pct column from ranks_big12.\nThe \\(i^{th}\\) component of \\(A^{2}r_{0}\\) represents the average winning percentage of the teams that team \\(i\\) defeated. As the author highlights, \\(A^{2}r_{0}\\) can be considered a proxy for strength of schedule. In the limit as \\(n\\) goes to infinity, \\(A^{n}r_{0}/|A^{n}r_{0}|\\) converges to the unique positive eigenvector of \\(A\\), and the magnitude of the entries of this eigenvector gives a ranking of teams.\nIf \\(A\\) has nonnegative entries (which will always be the case given out definition of \\(A\\)), then it has an eigenvector \\(\\vec{r}\\) with non-negative entries associated with a positive eigenvalue \\(\\lambda\\). If \\(A\\) is irreducible, then \\(\\vec{r}\\) has strictly positive entries and the corresponding eigenvalue is the one of largest absolute value. Note however that if \\(\\vec{r}\\) is an eigenvector of \\(A\\), so is \\(-\\vec{r}\\). In practice, we simply take the absolute value of the eigenvector associated with the largest eigenvalue. Note that for \\(A\\) to be irreducible, there can be no winless teams.\nThe steps for deriving our rankings are outlined below:\n\nConstruct the adjacency matrix \\(A\\), in which entry \\(a_{ij}\\) represents the number of times team \\(i\\) defeated team \\(j\\).\nPerform the eigendecomposition of \\(A\\), factoring the matrix into its eigenvalues and eigenvectors.\nIdentify the index of the largest eigenvalue.\nExtract the eigenvector at the index identified in step 3. If using Numpy and the maximum eigenvalue is found at index \\(j\\), the corresponding eigenvector \\(\\vec{r}\\) will be located in column \\(j\\).\nTake the absolute value of \\(\\vec{r}\\). The value at index \\(i\\) in \\(\\vec{r}\\) represents the score for the team at the same index (for the Big 12 example, index 0 = Baylor, index 1 = BYU, …).\nSort the eigenvector scores in decreasing order; higher performing teams will have a larger value, poorer performing teams will have a smaller value.\n\n\nKeep in mind that using a binary encoding scheme in a football setting, where each team may only compete once per season, overlooks information that could enhance the encoding. As it stands, a victory by 80 points for team A over team B is treated the same as a victory in triple overtime. In sports where teams face each other multiple times in a season, \\(a_{ij}\\) serves as a better indicator of the relative strength between the two teams. We’ll explore regular season Major League Baseball results later.\nThe next cell demonstrates how to implement the ranking procedure using Numpy.\n\n\nfrom numpy.linalg import eig\n\n# Adjacency matrix as Numpy array.\nA = adj_big12.drop(\"Unnamed: 0\", axis=1).values.astype(float)\n\n# Perform eigendecomposition of A. \ne_vals, e_vecs = eig(A)\n\n# Identify index of largest eigenvalue. \ne_val1_indx = np.argmax(e_vals)\n\n# Extract real part of eigenvector at index e_val1_indx. \ne_vec1 = np.abs(e_vecs[:, e_val1_indx])\n\n# Get indices associated with each team.\nindices = np.argsort(e_vec1)[::-1]\n\n# Associate ranks with teams. \nteams = adj_big12.columns[1:]\nranked_teams = teams[indices]\n\nfor team in ranked_teams:\n    print(team)\n\nTexas\nOklahoma State\nOklahoma\nKansas\nIowa State\nKansas State\nTexas Tech\nWest Virginia\nUCF\nHouston\nBYU\nTCU\nBaylor\nCincinnati\n\n\n\nWe can compare actual vs. predicted rankings to see how well the direct method performed:\n\n\nfor jj, team in enumerate(ranked_teams):\n    actual_rank = ranks_big12[ranks_big12.team==team].index.item()\n    print(f\"{team}: actual/predicted : {actual_rank}/{jj}\")\n\n\nTexas: actual/predicted : 0/0\nOklahoma State: actual/predicted : 1/1\nOklahoma: actual/predicted : 2/2\nKansas: actual/predicted : 7/3\nIowa State: actual/predicted : 3/4\nKansas State: actual/predicted : 4/5\nTexas Tech: actual/predicted : 6/6\nWest Virginia: actual/predicted : 5/7\nUCF: actual/predicted : 8/8\nHouston: actual/predicted : 10/9\nBYU: actual/predicted : 11/10\nTCU: actual/predicted : 9/11\nBaylor: actual/predicted : 12/12\nCincinnati: actual/predicted : 13/13\n\n\nThere are a few discrepancies, but the ranks are largely consistent. An interesting discrepancy is Kansas, having an actual rank of 7 vs. a predicted rank of 3. It’s difficult to say why Kansas is given such a high rank, but it may have to do with strength of schedule.\nFor the bottom five teams, the direct method does a good job. Three of the five teams have the same in-conference winning percentage, therefore slight out-of-orderings aren’t of concern.\nNext let’s look at a more substantial example: All games for the 2021 MLB regular season.\n\n\n\nMLB Example\nA Major League Baseball dataset with game results from 2016-2021 is available on Kaggle. The games.csv dataset has information about each contest that can be used to build an adjacency matrix. We load the dataset and inspect the first few records:\n\n\nimport numpy as np\nimport pandas as pd\n\nnp.set_printoptions(suppress=True, precision=5)\npd.options.mode.chained_assignment = None\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\n\ndf = pd.read_csv(\"C:/Users/jtriv/datasets/MLB/games.csv\")\n\nprint(f\"df.shape: {df.shape}\")\n\ndf.head(3)\n\ndf.shape: (13439, 43)\n\n\n\n\n\n\n\n\n\nGame\naway\naway-record\nawayaway-record\nhome\nhome-record\nhomehome-record\naway-score\nhome-score\npostseason info\nWalks Issued - Away\nWalks Issued - Home\nStolen Bases - Away\nStolen Bases - Home\nStrikeouts Thrown - Away\nStrikeouts Thrown - Home\nTotal Bases - Away\nTotal Bases - Home\nStadium\nDate\nLocation\nOdds\nO/U\nAttendance\nCapacity\nDuration\nUmpires\nWIN - Pitcher - Stats\nWIN - Pitcher - Id\nWIN - Pitcher - Name\nWIN - Pitcher - AbbrName\nWIN - Pitcher - Record\nLOSS - Pitcher - Stats\nLOSS - Pitcher - Id\nLOSS - Pitcher - Name\nLOSS - Pitcher - AbbrName\nLOSS - Pitcher - Record\nSAVE - Pitcher - Stats\nSAVE - Pitcher - Id\nSAVE - Pitcher - Name\nSAVE - Pitcher - AbbrName\nSAVE - Pitcher - Record\nExtra Innings\n\n\n\n\n0\n360403123\nSTL\n0-1\n0-1 Away\nPIT\nJan-00\n1-0 Home\n1.0\n4.0\nNaN\n5.0\n5.0\n0.0\n0.0\n5.0\n14.0\n5.0\n13.0\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\tPNC Park\\n\\t\\t\\t\\t\\t\\t...\n2016-04-03T17:00Z\nPittsburgh, Pennsylvania\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t...\nNaN\nNaN\n39,500\n38,362\n3:02\nHome Plate Umpire - Jerry Layne, First Base Um...\n6.0 IP, 0 ER, 10 K, 5 BB\n6211.0\nFrancisco Liriano\nF. Liriano\n(1-0)\n6.0 IP, 3 ER, 3 K, 3 BB\n5403.0\nAdam Wainwright\nA. Wainwright\n(0-1)\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n360403130\nTOR\nJan-00\n1-0 Away\nTB\n0-1\n0-1 Home\n5.0\n3.0\nNaN\n1.0\n3.0\n0.0\n0.0\n7.0\n16.0\n11.0\n11.0\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\tTropicana Field\\n\\t\\t\\...\n2016-04-03T20:00Z\nSt. Petersburg, Florida\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\...\nNaN\nNaN\n31,042\n31,042\n2:51\nHome Plate Umpire - Mike Everitt, First Base U...\n8.0 IP, 3 ER, 5 K, 1 BB\n32815.0\nMarcus Stroman\nM. Stroman\n(1-0)\n5.0 IP, 2 ER, 12 K, 3 BB\n31003.0\nChris Archer\nC. Archer\n(0-1)\n1.0 IP, 0 ER, 2 K, 0 BB\n32693.0\nRoberto Osuna\nR. Osuna\n-1.0\nNaN\n\n\n2\n360403107\nNYM\n0-1\n0-1 Away\nKC\nJan-00\n1-0 Home\n3.0\n4.0\nNaN\n2.0\n6.0\n0.0\n1.0\n3.0\n9.0\n8.0\n9.0\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\tKauffman Stadium\\n\\t\\t...\n2016-04-04T00:30Z\nKansas City, Missouri\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\...\nNaN\nNaN\n40,030\n37,903\n3:13\nHome Plate Umpire - Gerry Davis, First Base Um...\n6.0 IP, 0 ER, 5 K, 3 BB\n6401.0\nEdinson Volquez\nE. Volquez\n(1-0)\n5.2 IP, 3 ER, 2 K, 2 BB\n31214.0\nMatt Harvey\nM. Harvey\n(0-1)\n1.0 IP, 0 ER, 2 K, 1 BB\n28957.0\nWade Davis\nW. Davis\n-1.0\nNaN\n\n\n\n\n\n\n\n\nIt is first necessary to filter down to 2021 regular season games. If “postseason info” column is null, we assume the game is a regular season matchup. The “Date” column is used to extract the year.\nIn order to create the adjacency matrix only “away”, “home”, “away-score” and “home-score” need be retained. All other columns are removed:\n\n\ndf[\"yyyy\"] = pd.to_datetime(df[\"Date\"]).dt.year\n\ndf21 = (\n    df[(pd.isnull(df[\"postseason info\"])) & (df[\"yyyy\"]==2021)]\n    .dropna(subset=[\"away\", \"home\", \"away-score\", \"home-score\"])\n    .rename({\"away-score\": \"away_score\", \"home-score\": \"home_score\"}, axis=1)\n    [[\"away\", \"home\", \"away_score\", \"home_score\"]]\n    .reset_index(drop=True)\n)\n\nprint(f\"df21.shape: {df21.shape}\")\n\ndf21.head(15)\n\ndf21.shape: (2310, 4)\n\n\n\n\n\n\n\n\n\naway\nhome\naway_score\nhome_score\n\n\n\n\n0\nTOR\nNYY\n3.0\n2.0\n\n\n1\nCLE\nDET\n2.0\n3.0\n\n\n2\nMIN\nMIL\n5.0\n6.0\n\n\n3\nPIT\nCHC\n5.0\n3.0\n\n\n4\nATL\nPHI\n2.0\n3.0\n\n\n5\nARI\nSD\n7.0\n8.0\n\n\n6\nLAD\nCOL\n5.0\n8.0\n\n\n7\nSTL\nCIN\n11.0\n6.0\n\n\n8\nTB\nMIA\n1.0\n0.0\n\n\n9\nTEX\nKC\n10.0\n14.0\n\n\n10\nCHW\nLAA\n3.0\n4.0\n\n\n11\nHOU\nOAK\n8.0\n1.0\n\n\n12\nSF\nSEA\n7.0\n8.0\n\n\n13\nBAL\nBOS\n3.0\n0.0\n\n\n14\nTB\nMIA\n6.0\n4.0\n\n\n\n\n\n\n\n\nAll 30 MLB teams are represented in the “home” and away” columns. An empty DataFrame is created with columns and rows indexed using the 30 teams ordered alphabetically.\n\n\n# Create empty DataFrame with rows and columns indexed by the 30 MLB teams.\nmlb_teams = sorted(df21[\"home\"].unique().tolist())\ndfadj = pd.DataFrame(index=mlb_teams, columns=mlb_teams)\ndfadj.loc[:,:] = 0\n\ndfadj\n\n\n\n\n\n\n\n\nARI\nATL\nBAL\nBOS\nCHC\nCHW\nCIN\nCLE\nCOL\nDET\nHOU\nKC\nLAA\nLAD\nMIA\nMIL\nMIN\nNYM\nNYY\nOAK\nPHI\nPIT\nSD\nSEA\nSF\nSTL\nTB\nTEX\nTOR\nWSH\n\n\n\n\nARI\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nATL\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nBAL\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nBOS\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nCHC\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nCHW\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nCIN\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nCLE\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nCOL\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nDET\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nHOU\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nKC\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nLAA\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nLAD\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nMIA\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nMIL\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nMIN\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nNYM\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nNYY\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nOAK\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nPHI\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nPIT\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nSD\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nSEA\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nSF\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nSTL\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nTB\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nTEX\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nTOR\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nWSH\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\nWe iterate over df21, updating values in dfadj according to the following rules:\n\nIf team in row \\(i\\) defeated team in column \\(j\\), \\(a_{ij}\\) is incremented by 1.\nIf team in row \\(i\\) lost to team in column \\(j\\), \\(a_{ji}\\) is incremented by 1.\nIf the contest resulted in a tie, \\(a_{ij}\\) and \\(a_{ji}\\) are incremented by \\(\\frac{1}{2}\\), but there are no ties in df21.\n\nA dictionary dresults tracking wins and losses for each team is also created, in order to use regular season winning percentage as a proxy to compare against our direct method rankings.\n\n\ndresults = {kk: {\"wins\": 0, \"losses\": 0} for kk in dfadj.columns}\n\nfor rr in df21.itertuples(index=False):\n\n    ii, jj, ii_score, jj_score = rr.away, rr.home, rr.away_score, rr.home_score\n\n    if ii_score == jj_score:\n        dfadj.at[ii, jj]+=.5\n        dfadj.at[jj, ii]+=.5\n\n    elif ii_score &gt; jj_score:\n        dfadj.at[ii, jj]+=1\n        dresults[ii][\"wins\"]+=1\n        dresults[jj][\"losses\"]+=1\n\n    else:\n        dfadj.at[jj, ii]+=1\n        dresults[jj][\"wins\"]+=1\n        dresults[ii][\"losses\"]+=1\n\ndfadj\n\n\n\n\n\n\n\n\nARI\nATL\nBAL\nBOS\nCHC\nCHW\nCIN\nCLE\nCOL\nDET\nHOU\nKC\nLAA\nLAD\nMIA\nMIL\nMIN\nNYM\nNYY\nOAK\nPHI\nPIT\nSD\nSEA\nSF\nSTL\nTB\nTEX\nTOR\nWSH\n\n\n\n\nARI\n0\n1\n0\n0\n2\n0\n6\n0\n9\n0\n1\n0\n0\n3\n2\n1\n0\n1\n0\n0\n4\n4\n8\n2\n2\n1\n0\n1\n0\n3\n\n\nATL\n4\n0\n3\n1\n5\n0\n4\n0\n2\n0\n0\n0\n0\n2\n11\n3\n0\n9\n1\n0\n10\n4\n4\n0\n3\n5\n1\n0\n0\n12\n\n\nBAL\n0\n0\n0\n6\n0\n0\n0\n2\n0\n2\n3\n4\n2\n0\n2\n0\n2\n1\n8\n3\n1\n0\n0\n2\n0\n0\n1\n4\n5\n3\n\n\nBOS\n0\n3\n13\n0\n0\n3\n0\n4\n0\n3\n2\n5\n3\n0\n2\n0\n3\n4\n10\n3\n3\n0\n0\n4\n0\n0\n8\n2\n8\n3\n\n\nCHC\n4\n2\n0\n0\n0\n1\n8\n1\n2\n2\n0\n0\n0\n2\n1\n4\n2\n4\n0\n0\n2\n14\n5\n0\n1\n9\n0\n0\n0\n4\n\n\nCHW\n0\n0\n5\n2\n5\n0\n3\n9\n0\n9\n2\n8\n2\n0\n0\n1\n12\n0\n0\n4\n0\n3\n0\n3\n0\n2\n3\n5\n4\n0\n\n\nCIN\n1\n3\n0\n0\n11\n1\n0\n3\n5\n1\n0\n2\n0\n3\n5\n9\n2\n3\n0\n0\n3\n12\n1\n0\n1\n9\n0\n0\n0\n5\n\n\nCLE\n0\n0\n5\n2\n3\n7\n2\n0\n0\n12\n1\n13\n5\n0\n0\n0\n7\n0\n3\n2\n0\n1\n0\n3\n0\n2\n1\n4\n1\n0\n\n\nCOL\n10\n4\n0\n0\n2\n0\n2\n0\n0\n0\n2\n0\n1\n6\n4\n2\n0\n1\n0\n1\n5\n4\n10\n2\n3\n3\n0\n4\n0\n4\n\n\nDET\n0\n0\n5\n3\n1\n7\n2\n5\n0\n0\n4\n8\n1\n0\n0\n3\n6\n0\n3\n1\n0\n1\n0\n5\n0\n3\n4\n6\n3\n0\n\n\nHOU\n2\n0\n3\n5\n0\n5\n0\n6\n2\n1\n0\n3\n13\n2\n0\n0\n3\n0\n2\n11\n0\n0\n2\n11\n1\n0\n4\n14\n4\n0\n\n\nKC\n0\n0\n3\n2\n3\n9\n1\n3\n0\n11\n4\n0\n2\n0\n0\n4\n10\n0\n2\n2\n0\n3\n0\n4\n0\n1\n2\n2\n2\n0\n\n\nLAA\n3\n0\n4\n3\n0\n5\n0\n1\n2\n6\n6\n4\n0\n3\n0\n0\n4\n0\n4\n4\n0\n0\n2\n8\n1\n0\n1\n11\n3\n0\n\n\nLAD\n16\n4\n0\n0\n3\n0\n3\n0\n13\n0\n2\n0\n3\n0\n3\n4\n0\n6\n0\n2\n4\n6\n12\n3\n9\n4\n0\n2\n0\n7\n\n\nMIA\n5\n8\n2\n0\n5\n0\n2\n0\n2\n0\n0\n0\n0\n4\n0\n3\n0\n9\n0\n0\n9\n2\n3\n0\n3\n0\n1\n0\n0\n8\n\n\nMIL\n6\n3\n0\n0\n13\n2\n10\n3\n5\n1\n0\n0\n0\n3\n3\n0\n2\n3\n0\n0\n2\n13\n5\n0\n4\n8\n0\n0\n0\n3\n\n\nMIN\n0\n0\n4\n2\n2\n5\n2\n10\n0\n10\n4\n9\n1\n0\n0\n4\n0\n0\n1\n1\n0\n1\n0\n2\n0\n1\n3\n4\n3\n0\n\n\nNYM\n5\n7\n3\n0\n3\n0\n3\n0\n2\n0\n0\n0\n0\n1\n9\n1\n0\n0\n3\n0\n6\n2\n4\n0\n1\n1\n0\n0\n2\n8\n\n\nNYY\n0\n3\n11\n7\n0\n5\n0\n4\n0\n3\n4\n4\n2\n0\n3\n0\n5\n1\n0\n4\n2\n0\n0\n5\n0\n0\n8\n6\n7\n2\n\n\nOAK\n4\n0\n3\n3\n0\n3\n0\n4\n2\n6\n8\n5\n15\n1\n0\n0\n3\n0\n3\n0\n0\n0\n2\n4\n2\n0\n4\n10\n2\n0\n\n\nPHI\n3\n9\n2\n3\n5\n0\n2\n0\n2\n0\n0\n0\n0\n2\n9\n5\n0\n9\n2\n0\n0\n4\n4\n0\n2\n4\n0\n0\n1\n12\n\n\nPIT\n2\n3\n0\n0\n5\n1\n6\n2\n0\n3\n0\n1\n0\n0\n5\n4\n2\n3\n0\n0\n3\n0\n3\n0\n4\n7\n0\n0\n0\n2\n\n\nSD\n11\n0\n0\n0\n1\n0\n6\n0\n7\n0\n4\n0\n2\n7\n4\n2\n0\n3\n0\n2\n2\n4\n0\n3\n8\n3\n0\n3\n0\n5\n\n\nSEA\n4\n0\n1\n3\n0\n4\n0\n4\n2\n1\n8\n3\n11\n1\n0\n0\n4\n0\n2\n15\n0\n0\n0\n0\n2\n0\n6\n13\n4\n0\n\n\nSF\n17\n3\n0\n0\n6\n0\n6\n0\n14\n0\n2\n0\n3\n10\n4\n3\n0\n5\n0\n4\n4\n3\n11\n1\n0\n2\n0\n3\n0\n4\n\n\nSTL\n6\n0\n0\n0\n8\n1\n8\n2\n4\n1\n0\n5\n0\n3\n6\n11\n2\n4\n0\n0\n3\n12\n3\n0\n4\n0\n0\n0\n0\n2\n\n\nTB\n0\n2\n18\n11\n0\n3\n0\n4\n0\n3\n2\n4\n6\n0\n5\n0\n3\n3\n11\n3\n4\n0\n0\n1\n0\n0\n0\n3\n11\n1\n\n\nTEX\n3\n0\n3\n4\n0\n1\n0\n2\n2\n1\n5\n4\n8\n1\n0\n0\n3\n0\n1\n9\n0\n0\n0\n6\n1\n0\n4\n0\n2\n0\n\n\nTOR\n0\n6\n12\n7\n0\n3\n0\n4\n0\n3\n2\n3\n2\n0\n4\n0\n4\n1\n10\n5\n2\n0\n0\n2\n0\n0\n8\n3\n0\n1\n\n\nWSH\n4\n5\n3\n0\n3\n0\n3\n0\n2\n0\n0\n0\n0\n0\n11\n1\n0\n6\n1\n0\n5\n4\n3\n0\n1\n4\n3\n0\n3\n0\n\n\n\n\n\n\n\n\nConvert dresults into a DataFrame and add win_pct column:\n\n\ndfresults = (\n    pd.DataFrame().from_dict(dresults, orient=\"index\")\n    .reset_index(drop=False, names=\"team\")\n)\n\n# Compute winning percentage. \ndfresults[\"win_pct\"] = dfresults[\"wins\"] / (dfresults[\"wins\"] + dfresults[\"losses\"])\n\n# Sort values by win_pct. \ndfresults = (\n    dfresults.sort_values(\"win_pct\", ascending=False)\n    .reset_index(drop=True)\n)\n\n# Add win_pct rank column.\ndfresults[\"rank0\"] = dfresults.index + 1\n\ndfresults.head(30)\n\n\n\n\n\n\n\n\nteam\nwins\nlosses\nwin_pct\nrank0\n\n\n\n\n0\nSF\n105\n53\n0.664557\n1\n\n\n1\nLAD\n106\n54\n0.662500\n2\n\n\n2\nTB\n98\n62\n0.612500\n3\n\n\n3\nHOU\n94\n66\n0.587500\n4\n\n\n4\nMIL\n89\n65\n0.577922\n5\n\n\n5\nBOS\n86\n64\n0.573333\n6\n\n\n6\nNYY\n86\n67\n0.562092\n7\n\n\n7\nATL\n84\n66\n0.560000\n8\n\n\n8\nTOR\n82\n65\n0.557823\n9\n\n\n9\nCHW\n82\n66\n0.554054\n10\n\n\n10\nSEA\n88\n71\n0.553459\n11\n\n\n11\nSTL\n85\n69\n0.551948\n12\n\n\n12\nOAK\n84\n76\n0.525000\n13\n\n\n13\nPHI\n80\n74\n0.519481\n14\n\n\n14\nCLE\n74\n73\n0.503401\n15\n\n\n15\nCIN\n80\n79\n0.503145\n16\n\n\n16\nSD\n77\n82\n0.484277\n17\n\n\n17\nLAA\n75\n82\n0.477707\n18\n\n\n18\nDET\n71\n79\n0.473333\n19\n\n\n19\nCOL\n70\n79\n0.469799\n20\n\n\n20\nMIN\n69\n79\n0.466216\n21\n\n\n21\nKC\n70\n85\n0.451613\n22\n\n\n22\nNYM\n61\n76\n0.445255\n23\n\n\n23\nCHC\n68\n86\n0.441558\n24\n\n\n24\nMIA\n66\n93\n0.415094\n25\n\n\n25\nWSH\n62\n89\n0.410596\n26\n\n\n26\nTEX\n60\n100\n0.375000\n27\n\n\n27\nPIT\n56\n97\n0.366013\n28\n\n\n28\nBAL\n51\n103\n0.331169\n29\n\n\n29\nARI\n51\n110\n0.316770\n30\n\n\n\n\n\n\n\n\nrank0 will be used to compare our results against. Let’s visualize the regular season matchup network:\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Create adjacency matrix as Numpy array. \nA = dfadj.values.astype(float)\nG = nx.from_numpy_array(A)\n\nfig, ax = plt.subplots(1, 1, figsize=(8.5, 6), tight_layout=True)\n\nax.set_title(\n    \"2021 MLB Regular Season Matchups\", \n    color=\"#000000\", loc=\"center\", weight=\"normal\", fontsize=9\n)\nnx.draw_networkx(\n    G, node_color=\"#32cd32\", node_size=200, ax=ax, with_labels=True, \n    edge_color=\"blue\", width=.15, pos=nx.spring_layout(G, seed=516)\n)\n\n\n\n\n\n\n\n\n\nNext we perform the same steps carried out for the Big 12 analysis:\n\n\nfrom numpy.linalg import eig\n\n\n# Adjacency matrix as Numpy array.\nA = dfadj.values.astype(float)\n\n# Perform eigendecomposition of A. \ne_vals, e_vecs = eig(A)\n\n# Identify index of largest eigenvalue. \ne_val1_indx = np.argmax(e_vals)\n\n# Extract real part of eigenvector at index e_val1_indx. \ne_vec1 = np.abs(e_vecs[:, e_val1_indx])\n\n# Get indices associated with each team.\nindices = np.argsort(e_vec1)[::-1]\n\n# Associate ranks with teams. \nteams = dfadj.columns\nranked_teams = teams[indices]\n\nfor team in ranked_teams:\n    print(team)\n\nHOU\nTB\nLAD\nSF\nSEA\nNYY\nOAK\nBOS\nTOR\nCHW\nMIL\nLAA\nSTL\nSD\nDET\nCLE\nATL\nKC\nCIN\nMIN\nPHI\nCOL\nTEX\nCHC\nMIA\nWSH\nBAL\nNYM\nPIT\nARI\n\n\n\nAgain comparing actual vs. predicted ranks, using regular season winning percentage as a proxy for actual rank:\n\n\nfor jj, team in enumerate(ranked_teams, start=1):\n    actual_rank = dfresults[dfresults.team==team][\"rank0\"].item()\n    print(f\"{team}: actual/predicted : {actual_rank}/{jj}\")\n\nHOU: actual/predicted : 4/1\nTB: actual/predicted : 3/2\nLAD: actual/predicted : 2/3\nSF: actual/predicted : 1/4\nSEA: actual/predicted : 11/5\nNYY: actual/predicted : 7/6\nOAK: actual/predicted : 13/7\nBOS: actual/predicted : 6/8\nTOR: actual/predicted : 9/9\nCHW: actual/predicted : 10/10\nMIL: actual/predicted : 5/11\nLAA: actual/predicted : 18/12\nSTL: actual/predicted : 12/13\nSD: actual/predicted : 17/14\nDET: actual/predicted : 19/15\nCLE: actual/predicted : 15/16\nATL: actual/predicted : 8/17\nKC: actual/predicted : 22/18\nCIN: actual/predicted : 16/19\nMIN: actual/predicted : 21/20\nPHI: actual/predicted : 14/21\nCOL: actual/predicted : 20/22\nTEX: actual/predicted : 27/23\nCHC: actual/predicted : 24/24\nMIA: actual/predicted : 25/25\nWSH: actual/predicted : 26/26\nBAL: actual/predicted : 29/27\nNYM: actual/predicted : 23/28\nPIT: actual/predicted : 28/29\nARI: actual/predicted : 30/30\n\n\n\nThe Houston Astros are considered the best team based on the direct method, which is encouraging as they ultimately reached the 2021 World Series. One of the biggest discrepancies is with the Atlanta Braves, who were 8th in terms of regular season winning percentage but 17th in terms of the direct method. They went on to win the 2021 World Series. Nonetheless, the modeled ranking are reasonable, and it is clear that the direct might be able to provide further insight into how teams rank looking beyond winning percentage."
  },
  {
    "objectID": "posts/doomsday-scenario/doomsday-scenario.html",
    "href": "posts/doomsday-scenario/doomsday-scenario.html",
    "title": "The Doomsday Argument",
    "section": "",
    "text": "A few years ago, Lex Fridman had Nick Bostrom as a guest on his podcast. They discussed many topics, but of particular interest for me was the Doomsday Argument, a controversial hypothesis regarding the estimation of the remaining lifespan of the human species. It proposes that based on statistical reasoning and the assumption that humans are not special in the grand scheme of things, one can estimate the likelihood of humanity’s extinction. It is a probabilistic argument that claims to predict the number of future members of the human species given an estimate of the total number of humans born so far. Supposing all humans are born in a random order, chances are that any one human is born roughly in the middle. The argument has three main points:\n\nAssuming that humans are a random sample from all humans that will ever exist, the probability that any individual is born at a particular time in human history is proportional to the population size at that time.\nBased on historical population data, one can estimate the total number of humans who have ever lived up to the present.\nUnder the assumption that humans will continue to reproduce at a roughly constant rate until extinction, the argument suggests that since you are observing humanity at a random point in its history, it is statistically more likely that you are living closer to the midpoint of human existence rather than at the beginning or end.\n\nIn the podcast, Bostrom suggests that we have systematically underestimated the probability that humanity will go extinct soon, and uses sampling from two urns and observing the results as an analogy:\nImagine we have two urns: The first (urn A) has 10 balls numbered 1 thru 10, and the second (urn B) has 1,000,000 balls numbered 1-1,000,000. Someone puts one urn in front of you, and asks you what is the probability that it is the 10 ball urn? With no other information, as a rational participant you might say 50%.\nBut you are then allowed to reach in and pick a ball at random from the urn. Suppose you draw a ball at random, and find that you’ve drawn a ball with 7 on it. Drawing a 7 is strong evidence for the 10 ball urn, since the probability of drawing a 7 from the 10 ball urn is 10%, while the probability of drawing a 7 from the 1,000,000 ball urn is .00001%.\nYou then perform a Bayesian update: If your prior was 50/50 for the 10 ball urn, you become virtually certain after finding a randomly sampled 7 that it only has 10 balls in it.\nLet:\n\n\\(P[A]\\) = Prior probability that the unknown urn is urn A = .50.\n\\(P[B]\\) = Prior probability that the unknown urn is urn B = .50.\n\\(P[U|X]\\) = Given a randomly drawn ball \\(X\\), the probability that it came from urn \\(U\\).\n\\(P[X|U]\\) = Given that a random draw originates from urn \\(U\\), the probability of observing the drawn \\(X\\).\n\nThe expression for the Bayesian update is then:\n\\[\nP[U|X] = \\frac{P[X|U]P[U]}{P[X]}\n\\]\nWe know \\(P[X=7|A] = .10\\) and \\(P[X=7|B] = .000001\\), since there are 10 balls in urn \\(A\\) and 1,000,000 balls in urn \\(B\\), and each has an equally likely chance of being drawn.\nAfter observing a 7, For urn A we have:\n\\[\nP[A|X=7] = \\frac{P[X=7|A]P[A]}{P[X=7|A]P[A] + P[X=7|B]P[B]} = \\frac{.10 \\times .50}{.10 \\times .50 + .000001 \\times .50} \\approx 0.99999.\n\\]\nAfter observing a 7, For urn B we have:\n\\[\nP[B|X=7] = \\frac{P[X=7|B]P[B]}{P[X=7|A]P[A] + P[X=7|B]P[B]} = \\frac{.000001 \\times .50}{.10 \\times .50 + .000001 \\times .50} \\approx 9.9999 \\times 10^{-6}.\n\\]\nTherefore, there is a greater than 99.99% probability that the unknown urn is urn \\(A\\) given the observed 7.\nThe Doomsday argument asks how many humans will have lived by the time the species goes extinct.\nSuppose we consider 2 hypotheses:\n\nThere will be 200,000,000,000 humans (200 billion) in total.\nThere will be 200,000,000,000,000 humans (200 trillion) in total.\n\nTake your own birth rank as a random sample (your position in the sequence of all humans who have ever lived), and reason that you are a random sample from the set of all humans who have ever existed. It turns out you are roughly human 100 billion. If there are only going to be a total of 200 billion humans that ever live, 100 billion is a perfectly unremarkable number, and your actual birth rank lies somewhere in the middle.\nBut if there are going to be 200 trillion humans, then a birth rank of 100 billion is remarkably early (100,000,000,000 / 200,000,000,000,000 = 0.0005).\nWhen considering these two hypotheses, you should update in favor of the human species having a lower total number of members (what Bostrom calls “doom soon”).\nHe goes on to summarize that there has to be something “fishy” with this argument, because from very weak premises, it gets a very striking implication, that we have almost no chance of reaching 200T humans in the future. How can we get there by simply reflecting on when we were born?"
  },
  {
    "objectID": "posts/elbow-method/elbow-method.html",
    "href": "posts/elbow-method/elbow-method.html",
    "title": "The Elbow Method for Determining the Number of Clusters in K-Means",
    "section": "",
    "text": "The elbow method is a heuristic used in determining the optimal number of clusters in a k-means clustering algorithm. It is called the elbow method because it involves plotting the explained variation as a function of the number of clusters, and picking the elbow of the curve as the number of clusters to use. This method is widely used because of its simplicity and effectiveness in practice.\nThe steps to create the elbow plot are provided below:\n\nRun the k-means algorithm using your data for different k (say, 1-25).\nFor each k, calculate the sum of squared distances from each point to its assigned centroid. This metric is also known as the inertia of the k-means.\nPlot SSE as a function of the number of clusters. The plot typically shows a rapid decrease in SSE as k increases initially because adding more clusters will naturally improve the fit.\nAs k continues to increase, the improvements in SSE will start to diminish. At some point, the gain in SSE reduction becomes marginal, creating an “elbow” in the plot. This point is considered to be a good trade-off between the complexity of the model (number of clusters) and the fit of the model. The idea is that adding more clusters beyond this point doesn’t provide substantial improvement in fitting the model to the data.\n\nThe elbow method provides a simple and intuitive visualization to choose the best k, and it often works well in practice to find a reasonable number of clusters if the data has a clear grouping.\nHowever, there are some limitations. The exact elbow point can sometimes be ambiguous or subjective, especially if the curve is gradual. In some datasets, especially those with clusters of varying density or size, the elbow method might not yield a clear point of inflection. This is pretty common in real-world datasets. Also, minimizing SSE might not be the best approach for choosing k. Other techniques should be evaluated in parallel, such as silhouette analysis.\n In the following cells, the elbow method is demonstrated on dataset representing the sample mean and standard deviation of unemployment data for each of the 50 states:\n\n\nimport pandas as pd\n\ndf = pd.read_csv(\"https://gist.githubusercontent.com/jtrive84/ea0f275a9ef010415392189e64c71fc3/raw/4bb56a71c5b597c16513c48183dde799ddf9ec51/unemp.csv\")\n\ndf.head(10)\n\n\n\n\n\n\n\n\nstate\nmean\nstddev\n\n\n\n\n0\nAL\n6.644952\n2.527530\n\n\n1\nAK\n8.033173\n1.464966\n\n\n2\nAZ\n6.120673\n1.743672\n\n\n3\nAR\n6.474038\n1.488008\n\n\n4\nCA\n7.204087\n1.833834\n\n\n5\nCO\n5.383654\n1.394852\n\n\n6\nCT\n5.181731\n1.630996\n\n\n7\nDE\n5.185817\n1.868010\n\n\n8\nFL\n6.227163\n1.863281\n\n\n9\nGA\n5.685337\n1.466855\n\n\n\n\n\n\n\n\n\nimport warnings\nfrom sklearn.cluster import KMeans\n\nwarnings.filterwarnings(\"ignore\")\n\n# Use mean and stddev columns for clustering.\nX = df[[\"mean\", \"stddev\"]].values\n\n# Compute SSE for each k. \nn_clust = range(1, 26)\n\nresults = []\n\nfor k in n_clust:\n\n    kmeans = KMeans(n_clusters=k, random_state=516, n_init=\"auto\")\n    grps = kmeans.fit_predict(X)\n    results.append((k, kmeans.inertia_))\n\nresults\n\n[(1, 71.48395739420056),\n (2, 27.89336983415545),\n (3, 20.24188288137045),\n (4, 12.626365462154586),\n (5, 8.68912965898914),\n (6, 6.459331517502614),\n (7, 5.553744900882928),\n (8, 4.267127391718113),\n (9, 3.4187536239865084),\n (10, 3.066194952112686),\n (11, 2.695955805444958),\n (12, 2.3218932453872867),\n (13, 2.1421618770145803),\n (14, 2.0290844649980366),\n (15, 1.8714834948645003),\n (16, 1.7848062391719266),\n (17, 1.588261705328375),\n (18, 1.4404492654991405),\n (19, 1.2530374848818748),\n (20, 1.1906452777121082),\n (21, 0.9542369194221477),\n (22, 0.8924547058739023),\n (23, 0.7465084872875154),\n (24, 0.6827132125717246),\n (25, 0.5324000984155649)]\n\n\n\nWe can then plot SSE as a function of the number of clusters:\n\n\n# Generate elbow plot.\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nxx, yy = zip(*results)\n\nfig, ax = plt.subplots(figsize=(8, 5), tight_layout=True)\nax.set_title(\"k-Means SSE vs. number of clusters\", fontsize=10)\nax.plot(xx, yy, color=\"#E02C70\", linestyle=\"-\", linewidth=1.25, markersize=5, marker=\"o\")\nax.axvline(x=4., linewidth=1.25, color=\"#222222\", linestyle=\"--\")\nax.set_xlabel(\"k\", fontsize=10)\nax.set_ylabel(\"SSE\", fontsize=10)\nax.set_ylim(bottom=0)\nax.set_xlim(left=0)\nax.tick_params(axis=\"x\", which=\"major\", direction=\"in\", labelsize=8)\nax.tick_params(axis=\"y\", which=\"major\", direction=\"in\", labelsize=8)\nax.get_yaxis().set_major_formatter(mpl.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\nax.xaxis.set_ticks_position(\"none\")\nax.yaxis.set_ticks_position(\"none\")\nax.grid(True)\n\nplt.show()\n\n\n\n\n\n\n\n\nAlthough subjective, selecting k=4 is reasonable (dashed vertical line). k=5 could also be justified, but there is little benefit using a k beyond 5 as SSE only marginally decreases.\nFinally, we can plot the cluster assignments using the optimal k:\n\n\nkmeans = KMeans(n_clusters=4, random_state=516,  n_init=\"auto\")\nkmeans.fit(X)\ncentroids = kmeans.cluster_centers_\nkclust = kmeans.labels_\n\n\nfig, ax = plt.subplots(figsize=(7, 5), tight_layout=True)\nax.set_title(\"k-means cluster assignment with centroids, k=4\", fontsize=10, weight=\"normal\")\nax.scatter(X[:, 0], X[:, 1], c=kclust, s=55, alpha=1, marker=\"o\", cmap=\"gist_rainbow\")\n\n# Plot centroids as crosses.\nax.scatter(centroids[:, 0], centroids[:, 1], s=300, marker=\"+\", c=\"#000000\")\nax.set_xticks([])\nax.set_yticks([])\nax.set_xticklabels([])\nax.set_yticklabels([])\n\nplt.show()"
  },
  {
    "objectID": "posts/fft-denoise/fft-denoise.html",
    "href": "posts/fft-denoise/fft-denoise.html",
    "title": "Denoising Signals using the FFT",
    "section": "",
    "text": "The Discrete Fourier Transform (DFT) turns a data vector into a sum of sine/cosine components. The DFT is a Fourier series on data instead of analytic functions. Why do we perform the DFT? Because the features typically of interest aren’t always obvious in the time domain. The Fast Fourier Transform (FFT) is an efficient method used to calculate the DFT. The naive implementation of DFT scales as \\(\\mathcal{O}(n^2)\\), whereas FFT scales as \\(\\mathcal{O}(n \\mathrm{log}(n))\\).\nOur original data is sampled at uniform intervals represented as \\([f_{0}, f_{1}, \\dots, f_{n}]^{T}\\), and we want to obtain a vector of Fourier coefficients \\([\\hat{f}_{0}, \\hat{f}_{1}, \\dots, \\hat{f}_{n}]^{T}\\). In the second vector, \\(\\hat{f}_{0}\\) represents how much of the lowest frequency is in the data, \\(\\hat{f}_{1}\\) represents how much of the second lowest frequency, etc. The formula used to go from data to Fourier coefficients is given by:\n\\[\n\\hat{f}_{k} = \\sum_{j=0}^{n-1} f_{j}e^{i 2\\pi jk/n}.\n\\]\nIn words, the \\(k^{th}\\) Fourier coefficient is obtained by taking the sum over all \\(j\\) data points at the \\(j^{th}\\) frequency times the \\(k^{th}\\) frequency divided by \\(n\\). We can see that this expression represents a sum of sin and cosine terms by recalling \\(e^{in} = \\mathrm{cos}(n) + i\\mathrm{sin}(n)\\). The \\(\\hat{f}_{k}\\) are complex numbers which contain phase and magnitude information. The magnitude of \\(\\hat{f}_{k}\\) tells us how much of the \\(k^{th}\\) mode is in the original data.\nTo go from Fourier coefficients back to data, the formula is\n\\[\nf_{k} = \\frac{1}{n}\\Big(\\sum_{j=0}^{n-1} \\hat{f}_{j}e^{i 2\\pi jk/n}\\Big),\n\\]\nwhich represents the inverse FFT.\nThe DFT matrix can be represented in terms of the fundemental frequency for an interval with \\(n\\) data points, defined as \\(\\omega_{n} = e^{-2\\pi i / n}\\):\n$$ M =\n\\[\\begin{pmatrix}\n1 & 1 & 1 & \\dots & 1 \\\\\n1 & \\omega_{n} & \\omega_{n}^{2} & \\dots & \\omega_{n}^{n-1} \\\\\n1 & \\omega_{n}^{2} & \\omega_{n}^{4} & \\dots & \\omega_{n}^{2(n-1)} \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n1 & \\omega_{n}^{n-1} & \\omega_{n}^{2(n-1)} & \\dots & \\omega_{n}^{(n-1)^2}\n\\end{pmatrix}\\]\n$$\n\nTo obtain the Fourier coefficients, we multiply \\(M\\) by \\(\\boldsymbol{f}\\), which yields a vector of complex coefficients, \\(\\boldsymbol{\\hat{f}}\\). Next we show how the FFT can be used to de-noise a signal.\n\nDenoising Signals\nThis example, as well as much of the background, is taken from Steve Brunton’s FFT videos available here. Assume you receive a dataset representing a discrete signal, and our goal is to identify the characteristic frequencies above a certain threshold. We then execute the following steps:\n\nFourier transform the data.\nIdentify the peaks in the frequency domain, and zero out everything below some threshold.\nRun the inverse Fourier transform to recover the denoised signal.\n\nWe create the original signal and add noise as follows:\n\nfrom math import pi\nimport numpy as np\nnp.random.seed(516)\n\ndt = .001\nt = np.arange(0, 1, dt)\nn = len(t)\nforig = np.sin(2 * pi * 50 * t) + np.sin(2 * pi * 120 * t)\nf = forig + 2.5 * np.random.randn(len(t))\n\n# Run FFT on f. Capture power spectrum.\nfhat = np.fft.fft(f, n)\npsd = np.abs(fhat)**2 / n\n\n# x-axis for plotting.\nfreq = (1 / (dt * n)) * np.arange(n)      \n\n# Filter out noise using psd.\nthresh = 100\n\n# Find all freqs with power &gt; thresh. Zero out small Fourier coeffs.\nindices = psd &gt; thresh\n\n# Inverse fft for filtered time signal.\nffilt = np.fft.ifft(indices * fhat) \n\nNext we create a 3-facet plot: The top facet represents forig overlaid on the noisy signal f. The second facet represents the power spectrum, and the third the reconstructed filtered signal.\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(3, 1, figsize=(8, 6), tight_layout=True)\nax[0].set_title(\"Original and noise-added signals\", fontsize=9)\nax[0].plot(t, f, color=\"red\", linewidth=1., alpha=.75, label=\"noisy\")\nax[0].plot(t, forig, color=\"black\", linewidth=1.25, alpha=1, label=\"clean\")\nax[0].set_xlim(t[0], t[-1])\nax[0].legend(fontsize=\"small\", loc=\"upper right\")\n\nax[1].set_title(\"Power spectrum\", fontsize=9)\nax[1].plot(freq[:n // 2], psd[:n // 2], color=\"red\", linewidth=1.5)\nax[1].set_xlabel(\"frequency\", fontsize=7)\nax[1].set_ylabel(\"power\", fontsize=7)\n\nax[2].plot(t, ffilt, color=\"purple\", linewidth=1.25, label=\"filtered\")\nax[2].plot(t, forig, color=\"grey\", linewidth=1.25, alpha=.75, label=\"original\")\nax[2].set_title(\"Filtered signal\", fontsize=9)\nax[2].legend(fontsize=\"small\", loc=\"upper right\")\nax[2].set_xlim(t[0], t[-1])\n\nfor ii in range(3):\n    ax[ii].tick_params(axis=\"x\", which=\"major\", direction='in', labelsize=6)\n    ax[ii].tick_params(axis=\"y\", which=\"major\", direction='in', labelsize=6)\n\nplt.show();\n\n\n\n\nOriginal, power spectrum and filtered signal for generated signal with noise.\n\n\n\n\nThis tells us that even though the original signal is noisy, much of the signal is at 50Hz and 120Hz (verified by running np.where(psd[:n // 2]&gt;thresh)[0]), or the frequencies with power value in excess of the black horizontal dashed line in the center facet. Overlaying the filtered and original signals in the bottom facet, we see they line up perfectly.\n\n\nFiltering an Audio File\nIn practice, the signal of interest will not be generated from an analytic function. Typically we’ll have a signal, and need to identify the dominant frequencies for downstream analysis. In the code that follows, the frequency spectrum is generated for a signal of arbitrary origin, which can be extended to any time domain signal. The signal is filtered, then we approximate the original signal using only a small number of frequencies. The focus will be on an audio clip of a vintage telephone ringer, available here.\n\n\n# Code to read in a .wav file, perform FFT and generate filtered signal.\nfrom scipy.fftpack import fft, fftfreq\nfrom scipy.io.wavfile import read\n\n# Load .wav file. data has dimension nx2, with 2 representing\n# the channels. We focus on the first channel only. \ncensor = 2**16\nfs, data = read(\"vintage-telephone-ringtone.wav\")\ns = data[:, 0].astype(float)[:censor]\nn = len(s)\nt = np.arange(n)\nsample_secs = len(s) / fs   # Duration of sample audio clip. \ndt = 1 / fs                 # Time in seconds between samples. \nfhat = fft(s)               # Generate Fourier coefficients.\npsd = np.abs(fhat) / n      # Power spectrum.\n\n# x-axis for plotting. Can also call fftfreq(s.shape[0], d=dt).\nfreq = (1 / (dt * n)) * np.arange(n) \n\n# Retain frequencies with power above this threshold.\nthresh = 500\n\n# Find all freqs with power &gt; thresh. Zero out small Fourier coeffs.\nindices = psd &gt; thresh\nffilt = np.fft.ifft(indices * fhat) # Inverse fft for filtered time signal.\n\nWe then plot the original signal, power spectrum and filtered signal:\n\nfig, ax = plt.subplots(3, 1, figsize=(8, 6), tight_layout=True)\nax[0].set_title(\"original signal (first 2^16 samples,~1.5 secs)\", fontsize=9)\nax[0].plot(t, s, color=\"blue\", linewidth=.5, alpha=.75, label=\"original\")\nax[0].set_xlim(t[0], t[-1])\n\nax[1].set_title(\"Power spectrum\", fontsize=9)\nax[1].plot(freq[:n // 2], psd[:n // 2], color=\"red\", linewidth=1.5)\nax[1].axhline(thresh, color=\"black\", linestyle=\"--\", linewidth=1.25)\nax[1].set_xlabel(\"frequency\", fontsize=7)\nax[1].set_ylabel(\"power\", fontsize=7)\n\nax[2].set_title(\"Filtered signal with original\", fontsize=9)\nax[2].plot(t, ffilt, color=\"purple\", linewidth=1.25, label=\"filtered\")\nax[2].plot(t, s, color=\"grey\", linewidth=1.25, alpha=.75, label=\"original\")\nax[2].legend(fontsize=\"small\")\nax[2].set_xlim(t[0], t[-1])\n\nfor ii in range(3):\n    ax[ii].tick_params(axis=\"x\", which=\"major\", direction='in', labelsize=6)\n    ax[ii].tick_params(axis=\"x\", which=\"minor\", direction='in', labelsize=6)\n    ax[ii].tick_params(axis=\"y\", which=\"major\", direction='in', labelsize=6)\n    ax[ii].tick_params(axis=\"y\", which=\"minor\", direction='in', labelsize=6)\n\nplt.show();\n\n\n\n\nOriginal, power spectrum and filtered signal for audio signal.\n\n\n\n\nWe set the threshold to retain only those frequencies with power in excess of 500. In the bottom facet, we see that by retaining only a small number of frequencies, we are able to capture a reasonable approximation to the original signal, especially away from the endpoints.\nThe filtered signal can be written to file as follows:\n# Optionally export filtered audio.\nfrom scipy.io.wavfile import write\nwrite(\"filtered-telephone2.wav\", fs, np.real(ffilt).astype(np.int16))"
  },
  {
    "objectID": "posts/geohash-python/geohash-python.html",
    "href": "posts/geohash-python/geohash-python.html",
    "title": "GeoHashing from Scratch in Python",
    "section": "",
    "text": "I recently became interested in GeoHashing, and wanted to develop an understanding of the algorithm with the goal of implementing it myself. I was surprised to find it to be quite simple and intuitive. In what follows, I’ll demonstrate how to generate GeoHashes for a given latitude and longitude and compare results against pygeohash, a Python library for all things GeoHash-related. I’ll also walkthrough an approach that can be used to find the neighboring cells of a given GeoHash, and render them on a map.\nA Geohash is a unique identifier of a specific region on the Earth. The basic idea is the Earth gets divided into rectangular regions of user-defined size, and each region is assigned a unique id which is called its Geohash. For a given location on earth, the algorithm converts an arbitrary precision latitude and longitude into a string, the idea being that regions with a similar string prefix will be closer together. Conceptually, GeoHashing reduces proximity search to string prefix matching. As each character encodes additional precision, shared prefixes denote geographic proximity. But the converse is not true: Two points may be close in terms of relative proximity, but have no common GeoHash characters (think of two points on opposite sides of the prime meridian).\nGeohashes also provide a degree of anonymity since it isn’t necessary to expose exact GPS coordinates. The location of an entity up to a bounding box cell at a given precision is all that is known.\nBefore we begin, it is important to note that GeoHash libraries in the wild use a much more efficient generating mechanism than what is presented here. My goal is to demonstrate the concept with maximum clarity as opposed to maximum efficiency. If you decide to use GeoHashing in a real-world application, use an existing library.\n\nAlgorithm\nA GeoHash is a hierarchical spatial index: In order to represent a point, the world is recursively divided into smaller and smaller grid cells with each additional bit until the desired precision is reached. Functionally, the algorithm works by storing the intermediate results of two binary searches. In the resulting bit string, even-indexed bits represent longitude, while odd-indexed bits represent latitude. The user specifies a level of precision, usually between 1 and 12, and a GeoHash of that length is returned. The table below gives the dimensions of GeoHash cells at each level of precision (taken from here):\nPrecision          Dimension     \n        1: 5,000km x 5,000km\n        2:   1,250km x 625km\n        3:     156km x 156km\n        4:   31.9km x 19.5km\n        5:   4.89km x 4.89km\n        6:   1.22km x 0.61km\n        7:       153m x 153m\n        8:     38.2m x 19.1m\n        9:     4.77m x 4.77m\n       10:    1.19m x 0.596m \n       11:     149mm x 149mm\n       12:   37.2mm x 18.6mm\nThe values in the table represent the maximum dimension at each level of precision: As locations move away from the equator, the dimensions of each cell get smaller.\nFor a GeoHash of length 12, the coordinate can be found in a cell having dimension 37.2mm x 18.6mm, which is an impressive level of precision for a 12 character string!\nThe GeoHash symbol map consists of 32 characters:\nbase32 = \"0123456789bcdefghjkmnpqrstuvwxyz\"\nbase32 consists of digits 0 thru 9 plus all lowercase letters excluding a, i, l, o. As the search space is recursively partitioned, each group of 5 bits, when converted to a decimal integer, maps to one of the values in base32 above.\nI’ll present the algorithm first, then walkthrough it step-by-step. What follows is a basic implementation of GeoHashing, which takes as input the latitude and longitude for a point of interest, and returns the GeoHash and bounding box to the specified level of precision:\n\n\"\"\"\nVery inefficient GeoHashing subroutine. For demonstration purposes only. \n\"\"\"\n\ndef get_geohash(lat, lon, precision=12):\n    \"\"\"\n    Generate GeoHash of lat/lon at specified precision.\n\n    Parameters\n    ----------\n    lat: float\n        Latitude of point of interest.\n\n    lon: float\n        Longitude of point of interest\n\n    precision: int\n        Precision of GeoHash. Higher values result in \n        smaller bounding regions.\n\n    Returns\n    -------\n    geohash, bbox as list\n    \"\"\"\n    base32 = \"0123456789bcdefghjkmnpqrstuvwxyz\"\n    bits = []\n\n    min_lat, max_lat = -90, 90\n    min_lon, max_lon = -180, 180\n\n    for ii in range(5 * precision):\n\n        if ii % 2 == 0:\n            # Bisect longitude (E-W).\n            mid_lon = (min_lon + max_lon) / 2\n            if lon &gt;= mid_lon:\n                bits.append(1)\n                min_lon = mid_lon\n            else:\n                bits.append(0)\n                max_lon = mid_lon\n\n        else:\n            # Bisect latitude (N-S).\n            mid_lat = (min_lat + max_lat) / 2\n            if lat &gt;= mid_lat:\n                bits.append(1)\n                min_lat = mid_lat\n            else:\n                bits.append(0)\n                max_lat = mid_lat\n\n    # Create single bit string from list of 0/1s. \n    bitstr = \"\".join([str(ii) for ii in bits])\n\n    # Partition bitstr into groups of 5 bits. \n    quints = [bitstr[(5 * ii):(5 * (ii + 1))] for ii in range(precision)]\n\n    # Convert binary digits to decimal digits to get indices into base32. \n    indices = [int(ii, base=2) for ii in quints]\n\n    # Lookup characters associated with each index and concatenate.\n    geohash = \"\".join([base32[ii] for ii in indices]) \n\n    # GeoHash bounding box is just the final, min_lat, min_lon, max_lat, max_lon.\n    bbox = [min_lat, min_lon, max_lat, max_lon]\n\n    return(geohash, bbox)\n\nWe define base32 and initialize bits to hold our 0s and 1s. The initial minimum and maximum latitudes and longitudes define the bounds of the search space.\nThe range of iteration is specified as range(5 * precision). For a GeoHash having precision=6, the bits list will contain 5 * 6 = 30 values. We multiply by 5 since the base32 character map has length 32, and 2^5 bit arrangements cover each of the 32 indices.\nIf ii is even, we bisect longitude. If lon &gt;= mid_lon we append 1 to bits and update min_lat. Otherwise we append 0 and update max_lon. If ii is odd, we execute the same logic but for latitude. In this way, the final bit string represents interleaved longitude and latitude bits.\nOnce iteration completes, the individual elements of bits are concatenated into a single bit string bitstr. Next, we partition bitstr into groups of 5 bits (quints), which are converted to decimal integers, which are then used as indices to lookup elements from base32. These characters are concatenated into a single string representing the GeoHash.\nTo demonstrate, lets find the GeoHash for the Merchandise Mart (41.88876, -87.63516) at precision = 6. Once iteration completes, bits will consist of 30 1s and 0s:\n[0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1]\nWhich gets combined into a single bit string:\n'011001010100011111001001101011'\nPartitioned into groups of 5 bits:\n['01100', '10101', '00011', '11100', '10011', '01011']\nConverted to decimal integers:\n[12, 21, 3, 28, 19, 11]\nWhich serve as indices into base32, whose corresponding elements and are concatenated into the GeoHash:\n'dp3wmc'\nOur function also returns the bounding box of the precision = 6 GeoHash:\n[41.8853759765625, -87.637939453125, 41.890869140625, -87.626953125]\nLet’s compare the results of our implementation against pygeohash to ensure consistency:\n\n\nimport pygeohash\nlat, lon, precision = 41.88876, -87.63516, 6\npygeohash.encode(latitude=lat, longitude=lon, precision=precision)\n\n'dp3wmc'\n\n\nLet’s visualize the precision = 6 GeoHash using folium:\n\n\nimport folium \nfrom folium.features import DivIcon\n\n\nlat, lon, precision = 41.88876, -87.63516, 6\n\n# Use our subroutine to get geohash and bounding box. \ngeohash, bbox = get_geohash(lat, lon, precision=precision)\n\n# Unpack bbox.\nmin_lat, min_lon, max_lat, max_lon = bbox\n\n# Get mid_lat and mid_lon for GeoHash id placement. \nmid_lat = (min_lat + max_lat) / 2\nmid_lon = (min_lon + max_lon) / 2\n\nm = folium.Map(\n    location=[lat, lon], \n    #width=900, \n    #height=600, \n    zoom_start=16, \n    zoom_control=True, \n    no_touch=True,\n    tiles=\"OpenStreetMap\"\n    )\n\n# precision = 6 GeoHash bounding box. \nfolium.Rectangle(\n    [(min_lat, min_lon), (max_lat, max_lon)], \n    fill_color=\"blue\", fill_opacity=.15\n    ).add_to(m)\n\n# Red dot at Merchandise Mart. \nfolium.CircleMarker(\n    location=[lat, lon], radius=5, color=\"red\", fill_color=\"red\", \n    fill_opacity=1\n    ).add_to(m)\n\n# precision = 6 GeoHash id.\nfolium.map.Marker(\n    [mid_lat, mid_lon],\n    icon=DivIcon(\n        icon_size=(250,36),\n        icon_anchor=(100,50),\n        html=f'&lt;div style=\"font-size: 40pt\"&gt;{geohash}&lt;/div&gt;',\n        )\n    ).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nIdentify GeoHash Neighboring Cells\nWe can use the get_geohash function to help identify cells that neighbor a given GeoHash. Once the bounding box for the target GeoHash is known we simply increment those coordinates by a small amount, then lookup the GeoHash and bounding box associated with the new coordinate:\n\n\nlat, lon, precision = 41.88876, -87.63516, 6\n\neps = 1e-10\n\n# Center GeoHash id and bounding box.\ngeohash, bbox = get_geohash(lat, lon, precision=precision)\n\nmin_lat, min_lon, max_lat, max_lon = bbox\n\n# Get GeoHash id and bounding box for Northwest cell.\ngh_nw, bb_nw = get_geohash(max_lat + eps, min_lon - eps, precision=precision)\n\n# Get GeoHash id and bounding box for Northeast cell.\ngh_ne, bb_ne = get_geohash(max_lat + eps, max_lon + eps, precision=precision)\n\n# Get GeoHash id and bounding box for Southeast cell.\ngh_se, bb_se = get_geohash(min_lat - eps, max_lon + eps, precision=precision)\n\n# Get GeoHash id and bounding box for Southwest cell.\ngh_sw, bb_sw = get_geohash(min_lat - eps, min_lon - eps, precision=precision)\n\nWhich gives us the bounding boxes in NW, NE, SE and SW directions. Notice that all 5 boxes share the same prefix out to 4 characters:\n\nUsing the results from the NW, NE, SE and SW directions, we obtain the GeoHashes in the N, S, E and W directions:\n\n\n# For N, get mid point between NE and NW cells.\nmin_lat_nw, min_lon_nw, max_lat_nw, max_lon_nw = bb_nw\nmin_lat_ne, min_lon_ne, max_lat_ne, max_lon_ne = bb_ne\nn_lat = (min_lat_nw + max_lat_nw) / 2\nn_lon = (min_lon_nw + max_lon_ne) / 2\n\n# For S, get mid point between SE and SW cells.\nmin_lat_sw, min_lon_sw, max_lat_sw, max_lon_sw = bb_sw\nmin_lat_se, min_lon_se, max_lat_se, max_lon_se = bb_se\ns_lat = (min_lat_sw + max_lat_sw) / 2\ns_lon = (min_lon_sw + max_lon_se) / 2\n\n# For E, get mid point between SE and NE cells.\nmin_lat_ne, min_lon_ne, max_lat_ne, max_lon_ne = bb_ne\nmin_lat_se, min_lon_se, max_lat_se, max_lon_se = bb_se\ne_lat = (max_lat_ne + min_lat_se) / 2\ne_lon = (min_lon_se + max_lon_se) / 2\n\n# For W, get mid point between SW and NW cells.\nmin_lat_nw, min_lon_nw, max_lat_nw, max_lon_nw = bb_nw\nmin_lat_sw, min_lon_sw, max_lat_sw, max_lon_sw = bb_sw\nw_lat = (max_lat_nw + min_lat_sw) / 2\nw_lon = (min_lon_sw + max_lon_sw) / 2\n\nWhich we can visualize using folium, resulting in:\n\nWe can encapsulate this logic within a function we’ll call get_neighbors:\n\n\n\ndef get_neighbors(lat, lon, precision=12):\n    \"\"\"\n    Find 8 adjacent neighbors to GeoHash associated\n    with (lat, lon) at specified precision.\n\n    Parameters\n    ----------\n    lat: float\n        Latitude of point of interest.\n\n    lon: float\n        Longitude of point of interest\n\n    precision: int\n        Precision of GeoHash. Higher values result in \n        smaller bounding regions.\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    dneighbors = {}\n    eps = 1e-10\n\n    # Center GeoHash id and bounding box.\n    geohash, bbox = get_geohash(lat, lon, precision=precision)\n    min_lat, min_lon, max_lat, max_lon = bbox\n\n    dneighbors[\"center\"] = {\"geohash\": geohash, \"bbox\": bbox}\n\n    dcorners = {\n        \"nw\": {\"lat\": max_lat + eps, \"lon\": min_lon - eps},\n        \"ne\": {\"lat\": max_lat + eps, \"lon\": max_lon + eps},\n        \"sw\": {\"lat\": min_lat - eps, \"lon\": min_lon - eps},\n        \"se\": {\"lat\": min_lat - eps, \"lon\": max_lon + eps},\n        }\n\n    for kk in dcorners:\n        lat_, lon_ = dcorners[kk][\"lat\"], dcorners[kk][\"lon\"]\n        gh, bb = get_geohash(lat_, lon_, precision=precision)\n        dneighbors[kk] = {\"geohash\": gh, \"bbox\": bb}\n\n    # Find GeoHash for N, S, E and W directions.\n    min_lat_nw, min_lon_nw, max_lat_nw, max_lon_nw = dneighbors[\"nw\"][\"bbox\"]\n    min_lat_ne, min_lon_ne, max_lat_ne, max_lon_ne = dneighbors[\"ne\"][\"bbox\"]\n    n_lat = (min_lat_nw + max_lat_nw) / 2\n    n_lon = (min_lon_nw + max_lon_ne) / 2\n    n_gh, n_bb = get_geohash(n_lat, n_lon, precision=precision)\n    dneighbors[\"n\"] = {\"geohash\": n_gh, \"bbox\": n_bb}\n\n    min_lat_sw, min_lon_sw, max_lat_sw, max_lon_sw = dneighbors[\"sw\"][\"bbox\"]\n    min_lat_se, min_lon_se, max_lat_se, max_lon_se = dneighbors[\"se\"][\"bbox\"]\n    s_lat = (min_lat_sw + max_lat_sw) / 2\n    s_lon = (min_lon_sw + max_lon_se) / 2\n    s_gh, s_bb = get_geohash(s_lat, s_lon, precision=precision)\n    dneighbors[\"s\"] = {\"geohash\": s_gh, \"bbox\": s_bb}\n\n    min_lat_ne, min_lon_ne, max_lat_ne, max_lon_ne = dneighbors[\"ne\"][\"bbox\"]\n    min_lat_se, min_lon_se, max_lat_se, max_lon_se = dneighbors[\"se\"][\"bbox\"]\n    e_lat = (max_lat_ne + min_lat_se) / 2\n    e_lon = (min_lon_se + max_lon_se) / 2\n    e_gh, e_bb = get_geohash(e_lat, e_lon, precision=precision)\n    dneighbors[\"e\"] = {\"geohash\": e_gh, \"bbox\": e_bb}\n\n    min_lat_nw, min_lon_nw, max_lat_nw, max_lon_nw = dneighbors[\"nw\"][\"bbox\"]\n    min_lat_sw, min_lon_sw, max_lat_sw, max_lon_sw = dneighbors[\"sw\"][\"bbox\"]\n    w_lat = (max_lat_nw + min_lat_sw) / 2\n    w_lon = (min_lon_sw + max_lon_sw) / 2\n    w_gh, w_bb = get_geohash(w_lat, w_lon, precision=precision)\n    dneighbors[\"w\"] = {\"geohash\": w_gh, \"bbox\": w_bb}\n    \n    return(dneighbors)\n\nRunning get_neighbors and iterating over the results yields:\n\n\nlat, lon, precision = 41.88876, -87.63516, 6\n\ndn = get_neighbors(lat, lon, precision=precision)\n\nfor kk in dn:\n    gh, bb = dn[kk][\"geohash\"], dn[kk][\"bbox\"]\n    print(f\"{[kk]}: geohash={gh}, bbox={bb}\")\n    \n\n['center']: geohash=dp3wmc, bbox=[41.8853759765625, -87.637939453125, 41.890869140625, -87.626953125]\n['nw']: geohash=dp3wmd, bbox=[41.890869140625, -87.64892578125, 41.8963623046875, -87.637939453125]\n['ne']: geohash=dp3wq4, bbox=[41.890869140625, -87.626953125, 41.8963623046875, -87.615966796875]\n['sw']: geohash=dp3wm8, bbox=[41.8798828125, -87.64892578125, 41.8853759765625, -87.637939453125]\n['se']: geohash=dp3wq0, bbox=[41.8798828125, -87.626953125, 41.8853759765625, -87.615966796875]\n['n']: geohash=dp3wmf, bbox=[41.890869140625, -87.637939453125, 41.8963623046875, -87.626953125]\n['s']: geohash=dp3wmb, bbox=[41.8798828125, -87.637939453125, 41.8853759765625, -87.626953125]\n['e']: geohash=dp3wq1, bbox=[41.8853759765625, -87.626953125, 41.890869140625, -87.615966796875]\n['w']: geohash=dp3wm9, bbox=[41.8853759765625, -87.64892578125, 41.890869140625, -87.637939453125]\n\n\nFinally, we demonstrate how to recreate the previous figure (target GeoHash cell with eight adjacent neighbors):\n\nimport folium \nfrom folium.features import DivIcon\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nlat, lon, precision = 41.88876, -87.63516, 6\n\n# Get  GeoHashes and bounding boxes. \ndn = get_neighbors(lat, lon, precision=6)\n\n# Separate color for each cell.\nfcolors = mpl.colormaps[\"gist_rainbow\"]\ncolors_rgba = [fcolors(ii) for ii in np.linspace(0, 1, len(dn))]\ncolors_hex = [mpl.colors.to_hex(ii, keep_alpha=False) for ii in colors_rgba]\n\n\nm = folium.Map(\n    location=[lat, lon], \n    # width=900, \n    # height=600, \n    zoom_start=16, \n    zoom_control=True, \n    no_touch=True,\n    tiles=\"OpenStreetMap\"\n    )\n\nfor ii, kk in enumerate(dn.keys()):\n    color = colors_hex[ii]\n    geohash, bbox = dn[kk][\"geohash\"], dn[kk][\"bbox\"]\n    min_lat, min_lon, max_lat, max_lon = bbox\n    mid_lat = (min_lat + max_lat) / 2\n    mid_lon = (min_lon + max_lon) / 2\n    \n    # GeoHash bounding box. \n    folium.Rectangle(\n        [(min_lat, min_lon), (max_lat, max_lon)], \n        fill_color=color, fill_opacity=.25\n        ).add_to(m)\n    \n    folium.map.Marker(\n        [mid_lat, mid_lon],\n        icon=DivIcon(\n            icon_size=(250,36),\n            icon_anchor=(75,25),\n            html=f'&lt;div style=\"font-size: 30pt\"&gt;{geohash}&lt;/div&gt;',\n            )\n        ).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "posts/gibbs-sampling-linear-regression/gibbs-sampling-linear-regression.html",
    "href": "posts/gibbs-sampling-linear-regression/gibbs-sampling-linear-regression.html",
    "title": "Bayesian Linear Regression via Gibbs Sampling",
    "section": "",
    "text": "Markov Chain Monte Carlo (MCMC) methods can be used to estimate the parameters of linear regression models. In this post, the distributional forms and algorithms associated with two Gibbs sampling approaches are outlined: the full conditionals approach and the composition method.\nThe dataset used in what follows is available here and represents refrigerator price as a function of a number of features. The objective is to estimate PRICE as a function of ECOST, FSIZE, SHELVES and FEATURES, where:\n\nPRICE: Response, cost of refrigerator.\n\nFSIZE: Size of freezer compartment.\n\nECOST: Annual energy cost to operate refrigerator.\n\nSHELVES: Number of shelves.\nFEATURES: Number of features.\n\n\nI. The Full Conditionals Method\nFirst determine the joint kernel, which is the product of the likelihood and all prior distributions. The joint kernel will be proportional to the posterior distribution:\n\\[\n\\begin{align}\nf(\\beta, \\sigma^{2}|X, y) &\\propto f(y|X, \\beta, \\sigma^{2}) \\times f(\\beta) \\times f(\\sigma^{2}).\n\\end{align}\n\\]\n\nThe likelihood, \\(f(y|X, \\beta, \\sigma^{2})\\), is given by:\n\\[\n\\begin{align}\ny|X, \\beta, \\sigma^{2} &\\sim \\mathcal{N}(X\\beta, \\sigma^{2}I)\\\\\n&= \\prod_{i=1}^{n} f(y_{i}|x_{i}, \\beta, \\sigma^{2}) \\\\\n&= (2\\pi\\sigma^{2})^{-n/2} \\mathrm{exp}\\Big\\{\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n} (y_{i} - \\beta^{T}x_{i})^{2}\\Big\\}\\\\\n&= (2\\pi\\sigma^{2})^{-n/2}\\mathrm{exp}\\Big\\{-\\frac{1}{2\\sigma^{2}}(y-X\\beta)^{T}(y-X\\beta)\\Big\\}\n\\end{align}\n\\] \nFor the parameter vector \\(\\beta\\) we assume an improper uniform prior over the real line. For the variance, we assume a uniform prior over the real line for \\(\\mathrm{log}(\\sigma^{2})\\). If we transform the uniform prior on \\(\\mathrm{log}(\\sigma^{2})\\) into a density for \\(\\sigma^{2}\\), we obtain \\(f(\\sigma^{2}) \\propto 1/\\sigma^{2}\\). This is a common reference prior for the variance used within the context of Bayesian linear regression.\nSince the prior distribution for the parameter vector \\(f(\\beta)\\) can be treated as a multiplicative constant which can be ignored, the expression for the posterior reduces to:\n\\[\n\\begin{align}\nf(\\beta, \\sigma^{2}|X, y) &\\propto \\frac{1}{\\sigma^{2}} \\times (2\\pi\\sigma^{2})^{-n/2}\\mathrm{exp}\\Big\\{-\\frac{1}{2\\sigma^{2}}(y-X\\beta)^{T}(y-X\\beta)\\Big\\}\\\\\n&\\propto (\\sigma^{2})^{-(n/2 + 1)}\\mathrm{exp}\\Big\\{-\\frac{1}{2\\sigma^{2}}(y-X\\beta)^{T}(y-X\\beta)\\Big\\}.\\\\\n\\end{align}\n\\]\nNote that posterior distribution is asymptotically equivalent to the expression for the model likelihood.\nGibbs sampling requires identifying the full conditional distribution for each parameter, holding all other parameters constant. To find the full conditional distribution for \\(\\beta\\), select only the terms from the joint kernel that include \\(\\beta\\). Doing so results in:\n\\[\nf(\\beta|X, y, \\sigma^{2}) \\propto \\mathrm{exp}\\Big\\{-\\frac{1}{2\\sigma^{2}}(y-X\\beta)^{T}(y-X\\beta)\\Big\\}\n\\] \nAfter distributing the transpose, removing multiplicative constants and performing a bit of reorganization, we arrive at:\n\\[\nf(\\beta|X, y, \\sigma^{2}) \\propto \\mathrm{exp}\\Big\\{-\\frac{1}{2\\sigma^{2}}[\\beta^{T}X^{T}X\\beta - 2\\beta^{T}X^{T}y]\\Big\\}.\\\\\n\\]\nUpon completing the square, we find the distribution for \\(\\beta\\) to be normal with mean \\((X^{T}X)^{-1}X^{T}y\\hspace{.25em}\\) and variance \\(\\hspace{.25em}\\sigma^{2}(X^{T}X)^{-1}\\).\nFor \\(\\sigma^{2}\\), with \\(\\beta\\) assumed fixed, we obtain:\n\\[\nf(\\sigma^{2}|X, y, \\beta) \\propto (\\sigma^{2})^{-(n/2 + 1)}\\mathrm{exp}\\Big\\{-\\frac{\\mathrm{SSR}}{2\\sigma^{2}}\\Big\\},\n\\]\nwhere \\(\\mathrm{SSR}\\) represents the sum of squared residuals under the specified value of \\(\\beta\\). This is proportional to an inverse gamma distribution with \\(a = n/2\\) and \\(b = \\mathrm{SSR}/2\\).\n\nFull Conditionals Approach for Sampling from the Posterior Distribution:\n\nEstablish starting values for \\(\\beta_{p}\\) and \\(\\sigma^{2}\\). We use \\(\\beta^{(0)} = \\hat\\beta_{ols}\\) and \\(\\hat \\sigma_{0}^{2} = \\mathrm{SSR} / (n - p)\\)\nSample \\(\\beta_{p}\\) from multivariate normal distribution with \\(\\sigma^{2}\\) fixed.\nSample \\(\\sigma^{2}\\) from inverse gamma distribution with \\(\\beta_{p}\\) fixed.\n\nIn the code below, we run the sampler for 10,000 iterations, discarding the first 1,000 samples.\n\n\"\"\"\nThe full conditionals Gibbs sampling method. \n\n    PRICE ~ \"ECOST\" + \"FSIZE\" + \"SHELVES\" + \"FEATURES\"\n\"\"\"\nfrom math import sqrt\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np \nimport pandas as pd\nfrom scipy import stats\nfrom scipy.linalg import cholesky\n\npd.options.mode.chained_assignment = None\npd.set_option('display.max_columns', 1000)\npd.set_option('display.width', 10000)\nnp.set_printoptions(suppress=True)\n\n\ndata_url = \"https://gist.github.com/jtrive84/96757393423b2599c7d5da361fdf024b/raw/82000835b7c3b70dcf21a9ab43c59db700b78136/Refrigerator.csv\"\n\ndf = pd.read_csv(data_url)\n\nvariates = [\"ECOST\", \"FSIZE\", \"SHELVES\", \"FEATURES\"]\nX = df[variates].values\ny = df[\"PRICE\"].values.reshape(-1, 1)\nX = np.concatenate([np.ones(y.size).reshape(-1, 1), X], axis=1)\n\nn, p = X.shape\nM = 10000\nburnin = 1000\nprng = np.random.RandomState(516)\n\n# Initialize arrays to hold posterior samples.\nbetas, sigma2 = np.zeros([M, p]), np.ones(M)\n\n# Initialize parameter arrays and compute covariance matrix.\nb_ols = np.linalg.inv(X.T @ X) @ X.T @ y\nV = np.linalg.inv(X.T @ X)\nsigma2[0] = ((y - X @ b_ols).T @ (y - X @ b_ols) / (n - p)).item()\nbetas[0, :] = b_ols.T\n\n# Gibbs sampling from full conditionals. At each iteration, p independent \n# standard normal random variates are sampled, which are transformed into \n# a draw from a multivariate normal density with mean betas[i, :] and \n# covariance V. \nfor ii in range(1, M):\n    \n    # Sample from full conditional distribution for betas.\n    betas[ii,:] = b_ols.T + prng.randn(p).reshape(1, -1) @ cholesky(sigma2[ii - 1] * V)\n    \n    # Sample from full conditional distribution for variance. \n    sigma2[ii] = stats.invgamma.rvs(\n        a=.50 * n, \n        scale=.50 * ((y - X @ betas[ii,:].reshape(-1, 1)).T @ (y - X @ betas[ii,:].reshape(-1, 1))).item(),\n        random_state=prng\n        )\n\n# Remove burnin samples.\nbetas = betas[burnin:,:]\nsigma2 = sigma2[burnin:]\n\nTraceplots can be produced to visualize sampling variability across iterations:\n\n# Combine beta and sigma2 arrays to create traceplots. \nvarnames = [\"intercept\"] + variates + [\"sigma2\"]\nXall1 = np.hstack([betas, sigma2.reshape(-1, 1)])\n\nfig, ax = plt.subplots(nrows=2, ncols=3, figsize=(10, 5), tight_layout=True) \n\nindices = [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2)]\n\nfor indx, (varname, (ii, jj)) in enumerate(zip(varnames, indices)):\n    \n    ax[ii, jj].set_title(varname, color=\"#000000\", loc=\"center\", fontsize=8)\n    ax[ii, jj].plot(Xall1[:,indx], color=\"#000000\", linewidth=.25, linestyle=\"-\")\n    ax[ii, jj].xaxis.set_major_formatter(mpl.ticker.StrMethodFormatter(\"{x:,.0f}\"))\n    ax[ii, jj].set_xlabel(\"\")\n    ax[ii, jj].set_ylabel(\"\")\n    ax[ii, jj].tick_params(axis=\"x\", which=\"major\", direction='in', labelsize=7)\n    ax[ii, jj].tick_params(axis=\"y\", which=\"major\", direction='in', labelsize=7)\n    ax[ii, jj].xaxis.set_ticks_position(\"none\")\n    ax[ii, jj].yaxis.set_ticks_position(\"none\")\n    ax[ii, jj].grid(True)   \n    ax[ii, jj].set_axisbelow(True) \n\nplt.suptitle(\"Traceplots: Full Conditionals Approach\", fontsize=10)\nplt.show();\n\n\n\n\nTraceplot for each feature in model using full conditionals approach\n\n\n\n\nHistograms for each parameter can be generated from the posterior samples:\n\n# Create histograms from posterior samples with overlaid ols estimates.\nhist_color = \"#ff7595\"\nfig, ax = plt.subplots(nrows=2, ncols=3, figsize=(10, 6), tight_layout=True) \n\nplt_ols = np.append(b_ols.ravel(), sigma2[0])\n\nindices = [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2)]\n\nfor indx, (varname, (ii, jj)) in enumerate(zip(varnames, indices)):\n    \n    ax[ii, jj].set_title(varname, color=\"#000000\", loc=\"center\", fontsize=8)\n    ax[ii, jj].hist(\n        Xall1[:,indx], 25, density=True, alpha=1, color=hist_color, \n        edgecolor=\"#FFFFFF\", linewidth=1.0\n        )\n    label0 = r\"$\\hat \\sigma_{0}^{2}$\" if indx == 5 else r\"$\\hat \\beta_{OLS}$\" \n    label1 = r\"$\\hat \\sigma_{MCMC}^{2}$\" if indx == 5 else r\"$\\hat \\beta_{MCMC}$\" \n    ax[ii,jj].axvline(plt_ols[indx], color=\"grey\", linewidth=1.25, linestyle=\"--\", label=label0)\n    ax[ii,jj].axvline(Xall1[:,indx].mean(), color=\"blue\", linewidth=1.25, linestyle=\"--\", label=label1)\n    ax[ii, jj].xaxis.set_major_formatter(mpl.ticker.StrMethodFormatter(\"{x:,.0f}\"))\n    ax[ii,jj].set_yticklabels([])\n    ax[ii, jj].set_xlabel(\"\")\n    ax[ii, jj].set_ylabel(\"\")\n    ax[ii, jj].tick_params(axis=\"x\", which=\"major\", direction='in', labelsize=6)\n    ax[ii, jj].tick_params(axis=\"y\", which=\"major\", direction='in', labelsize=6)\n    ax[ii, jj].xaxis.set_ticks_position(\"none\")\n    ax[ii, jj].yaxis.set_ticks_position(\"none\")\n    ax[ii, jj].grid(True)   \n    ax[ii, jj].set_axisbelow(True) \n    ax[ii,jj].legend(loc=\"upper right\", fancybox=True, framealpha=1, fontsize=\"x-small\")\n\nplt.suptitle(\"Histograms: Full Conditionals Approach\", fontsize=10)\nplt.show();\n\n\n\n\nHistogram for each feature using full conditionals approach\n\n\n\n\n\n\n\nII. Composition Method\nThe composition method can also be used to generate posterior samples for \\(\\beta\\) and \\(\\sigma^{2}\\). Using this approach, the posterior distribution is decomposed into 1) the conditional distribution for \\(\\beta\\) given \\(\\sigma^{2}\\) and 2) the marginal distribution for \\(\\sigma^{2}\\):\n\\[\nf(\\beta, \\sigma^{2}| X, y) \\propto f(\\beta|\\sigma^{2}, X, y) \\times f(\\sigma^{2}|X, y)\n\\]\n\\(f(\\sigma^{2}|X, y)\\) is an inverse gamma distribution with \\(\\alpha = (n - p) / 2\\) and \\(\\beta = (n - p) \\times \\mathrm{SSR} / 2\\), where \\(n\\) is the number of observations in the dataset and \\(p\\) the number of predictors in \\(X\\). \\(\\mathrm{SSR}\\) represents the sum of squared residuals obtained from the OLS estimate.\nAs before, the conditional distribution for \\(\\beta\\), \\(f(\\beta|\\sigma^{2}, X, y)\\), is multivariate normal with mean \\((X^{T}X)^{-1}X^{T}y\\) and variance \\(\\sigma^{2}(X^{T}X)^{-1}\\).\nThe composition method is faster than the full conditionals method since we can draw all samples from \\(f(\\sigma^{2}|X, y)\\) upfront.\n\nComposition Method Approach for Sampling from the Posterior Distribution:\n\nCompute \\(\\hat \\beta\\) via OLS and \\(V = (X^{T}X)^{-1}\\).\nCompute \\(SSR = \\frac{1}{n - p}(y - X\\hat \\beta)^{T}(y - X\\hat \\beta)\\).\nDraw \\(m\\) samples from \\(IG\\big(\\frac{n - p}{2}, \\frac{(n - p) \\times \\mathrm{SSR}}{2}\\big)\\).\nFor \\(i = 1, \\cdots, m\\): Draw \\(\\beta^{(i)}\\) from \\(f(\\beta|\\sigma^{2(i)}, X, y)\\): \\(\\beta^{(i)} \\sim \\mathcal{N}(\\hat \\beta, \\sigma^{2(i)}V)\\).\n\n\n\"\"\"\nDrawing posterior samples using the composition method.\n\n    PRICE ~ \"ECOST\" + \"FSIZE\" + \"SHELVES\" + \"FEATURES\"\n\"\"\"\n\nn, p = X.shape\nM = 10000\nburnin = 1000\nprng = np.random.RandomState(516)\n\n# Initialize betas to hold posterior samples.\nbetas = np.zeros([M, p])\n\n# Compute OLS estimate, V and SSR. \nb_ols = np.linalg.inv(X.T @ X) @ X.T @ y\nV = np.linalg.inv(X.T @ X)\nSSR = (y - X @ b_ols).T @ (y - X @ b_ols) / (n - p)\n\n# Draw M samples from marginal distribution for sigma2.\nsigma2 = stats.invgamma.rvs(a=.50 * (n - p), scale=.50 * (n - p) * SSR, size=M, random_state=prng)\n\n# Simulate sequence of beta draws for each value in sigma2.\nfor ii in range(M):\n    betas[ii,:] = b_ols.T + prng.randn(p).reshape(1, -1) @ cholesky(sigma2[ii] * V)\n\n\nAs before, we produce traceplots for each variable and \\(sigma^2\\):\n\nvarnames = [\"intercept\"] + variates + [\"sigma2\"]\nXall2 = np.hstack([betas, sigma2.reshape(-1, 1)])\n\nfig, ax = plt.subplots(nrows=2, ncols=3, figsize=(10, 5), tight_layout=True) \n\nindices = [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2)]\n\nfor indx, (varname, (ii, jj)) in enumerate(zip(varnames, indices)):\n    \n    ax[ii, jj].set_title(varname, color=\"#000000\", loc=\"center\", fontsize=8)\n    ax[ii, jj].plot(Xall2[:,indx], color=\"#000000\", linewidth=.25, linestyle=\"-\")\n    ax[ii, jj].xaxis.set_major_formatter(mpl.ticker.StrMethodFormatter(\"{x:,.0f}\"))\n    ax[ii, jj].set_xlabel(\"\")\n    ax[ii, jj].set_ylabel(\"\")\n    ax[ii, jj].tick_params(axis=\"x\", which=\"major\", direction='in', labelsize=7)\n    ax[ii, jj].tick_params(axis=\"x\", which=\"minor\", direction='in', labelsize=7)\n    ax[ii, jj].tick_params(axis=\"y\", which=\"major\", direction='in', labelsize=7)\n    ax[ii, jj].tick_params(axis=\"y\", which=\"minor\", direction='in', labelsize=7)\n    ax[ii, jj].xaxis.set_ticks_position(\"none\")\n    ax[ii, jj].yaxis.set_ticks_position(\"none\")\n    ax[ii, jj].grid(True)   \n    ax[ii, jj].set_axisbelow(True) \n\nplt.suptitle(\"Traceplots: Composition Method\", fontsize=10)\nplt.show();\n\n\n\n\nTraceplot for each feature in model using composition method\n\n\n\n\nAs well as histograms:\n\nhist_color = \"#ff7595\"\nfig, ax = plt.subplots(nrows=2, ncols=3, figsize=(10, 6), tight_layout=True) \n\nplt_ols = np.append(b_ols.ravel(), sigma2[0])\n\nindices = [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2)]\n\nfor indx, (varname, (ii, jj)) in enumerate(zip(varnames, indices)):\n    \n    ax[ii, jj].set_title(varname, color=\"#000000\", loc=\"center\", fontsize=8)\n    ax[ii, jj].hist(\n        Xall2[:,indx], 25, density=True, alpha=1, color=hist_color, \n        edgecolor=\"#FFFFFF\", linewidth=1.0\n        )\n    label0 = r\"$\\hat \\sigma_{0}^{2}$\" if indx == 5 else r\"$\\hat \\beta_{OLS}$\" \n    label1 = r\"$\\hat \\sigma_{MCMC}^{2}$\" if indx == 5 else r\"$\\hat \\beta_{MCMC}$\" \n    ax[ii,jj].axvline(plt_ols[indx], color=\"grey\", linewidth=1.25, linestyle=\"--\", label=label0)\n    ax[ii,jj].axvline(Xall2[:,indx].mean(), color=\"blue\", linewidth=1.25, linestyle=\"--\", label=label1)\n    ax[ii, jj].xaxis.set_major_formatter(mpl.ticker.StrMethodFormatter(\"{x:,.0f}\"))\n    ax[ii,jj].set_yticklabels([])\n    ax[ii, jj].set_xlabel(\"\")\n    ax[ii, jj].set_ylabel(\"\")\n    ax[ii, jj].tick_params(axis=\"x\", which=\"major\", direction='in', labelsize=6)\n    ax[ii, jj].tick_params(axis=\"y\", which=\"major\", direction='in', labelsize=6)\n    ax[ii, jj].xaxis.set_ticks_position(\"none\")\n    ax[ii, jj].yaxis.set_ticks_position(\"none\")\n    ax[ii, jj].grid(True)   \n    ax[ii, jj].set_axisbelow(True) \n    ax[ii,jj].legend(loc=\"upper right\", fancybox=True, framealpha=1, fontsize=\"x-small\")\n\nplt.suptitle(\"Histograms: Composition Method\", fontsize=10)\nplt.show();\n\n\n\n\nHistogram for each feature using composition method\n\n\n\n\nPosterior percentiles from the full conditionals approach and composition method can be compared:\n\n\"\"\"\nComparing posterior percentiles for full conditionals and composition approaches.\n\"\"\"\n\npeval = [1, 5, 25, 50, 75, 95, 99]\npcols = [f\"{ii:.0f}%\" for ii in peval]\n\nfor indx, varname in enumerate(varnames):\n    \n    # Determine percentiles generated via full-conditionals.\n    df1 = pd.DataFrame(\n        np.round(np.percentile(Xall1[:, indx], q=peval), 2).reshape(1,-1), \n        columns=pcols,index=[f\"{varname}: conditionals\"]\n        )\n    \n    # Determine percentiles generated via composition method.\n    df2 = pd.DataFrame(\n        np.round(np.percentile(Xall2[:, indx], q=peval), 2).reshape(1,-1), \n        columns=pcols, index=[f\"{varname}: composition\"]\n        )\n    df = pd.concat([df1, df2])\n    display(df)\n\n\n\n\n\n\n\n\n1%\n5%\n25%\n50%\n75%\n95%\n99%\n\n\n\n\nintercept: conditionals\n-25.01\n51.28\n150.96\n221.28\n289.90\n386.74\n464.17\n\n\nintercept: composition\n-21.44\n50.76\n152.43\n221.71\n288.77\n387.09\n461.41\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1%\n5%\n25%\n50%\n75%\n95%\n99%\n\n\n\n\nECOST: conditionals\n-10.34\n-8.41\n-5.77\n-4.0\n-2.20\n0.39\n2.62\n\n\nECOST: composition\n-10.20\n-8.34\n-5.76\n-4.0\n-2.21\n0.38\n2.28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1%\n5%\n25%\n50%\n75%\n95%\n99%\n\n\n\n\nFSIZE: conditionals\n38.83\n58.12\n85.95\n103.97\n121.74\n148.78\n169.22\n\n\nFSIZE: composition\n39.05\n57.93\n85.41\n103.99\n122.53\n148.96\n168.51\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1%\n5%\n25%\n50%\n75%\n95%\n99%\n\n\n\n\nSHELVES: conditionals\n-2.56\n5.70\n17.33\n25.19\n32.86\n44.09\n52.25\n\n\nSHELVES: composition\n-2.71\n5.68\n17.42\n24.81\n32.72\n44.32\n52.66\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1%\n5%\n25%\n50%\n75%\n95%\n99%\n\n\n\n\nFEATURES: conditionals\n11.87\n15.60\n21.25\n24.87\n28.68\n34.11\n38.15\n\n\nFEATURES: composition\n11.79\n15.68\n21.13\n24.86\n28.46\n33.87\n37.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1%\n5%\n25%\n50%\n75%\n95%\n99%\n\n\n\n\nsigma2: conditionals\n3209.36\n3706.92\n4620.93\n5465.36\n6527.58\n8570.83\n10609.79\n\n\nsigma2: composition\n3194.00\n3691.71\n4649.74\n5467.54\n6490.48\n8482.73\n10295.27\n\n\n\n\n\n\n\n\n\n\nPosterior Predictive Samples\nFor a new set of predictors \\(\\tilde{X}\\), we may be interested in quantifying the variability in the price estimate given our model. This can be accomplished by sampling from the posterior predictive distribution, \\(f(\\tilde{y}|y)\\). If \\(\\beta\\) and \\(\\sigma^{2}\\) were known exactly, the distribution for \\(\\tilde{y}\\) would be completely specified as \\(\\mathcal{N}(\\tilde{X}\\beta, \\sigma^{2}I)\\). But since we have only estimates of \\(\\beta\\) and \\(\\sigma^{2}\\), we need to sample from the posterior predictive distribution \\(f(\\tilde{y}|y)\\) as follows:\n\nFor \\(i = 1, \\cdots, n\\): Draw \\((\\beta^{(i)}, \\sigma^{2(i)})\\) from posterior samples.\nDraw \\(\\tilde{y}^{(i)} \\sim \\mathcal{N}(\\tilde{X}\\beta^{(i)}, \\sigma^{2{(i)}}I)\\).\n\nThe code that follows generates 5000 posterior predictive samples for a new set of predictors.\n\n\n# Generating posterior predictive samples for a new set of predictors.\n\nn = 5000\nXnew = [1, 82, 5.1, 2, 8]\n\n# Determine which 5000 parameter sets to use for sampling.\nindices = prng.choice(range(Xall2.shape[0]), size=n, replace=False)\nparams = Xall2[indices,:]\nsamples = []\n\nfor ii in range(n):\n    beta, v = params[ii, :-1], params[ii, -1]\n    mu = np.dot(beta, Xnew)\n    y_ii = prng.normal(loc=mu, scale=sqrt(v))\n    samples.append(y_ii)\n\nsamples = np.asarray(samples)\n\nViewing the histogram of posterior predictive samples:\n\nhist_color = \"#ff7595\"\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 5), tight_layout=True) \nax.set_title(\n    \"Posterior predictive samples for new predictors\", color=\"#000000\", loc=\"center\", fontsize=10\n    )\nax.hist(\n    samples, 30, density=True, alpha=1, color=hist_color, \n    edgecolor=\"#FFFFFF\", linewidth=1.0\n    )\nlabel0, label1 = r\"$\\hat{y}_{OLS}$\", r\"$\\hat y_{MCMC}$\" \nax.axvline(np.dot(b_ols.T, Xnew), color=\"grey\", linewidth=1.25, linestyle=\"--\", label=label0)\nax.axvline(samples.mean(), color=\"blue\", linewidth=1.25, linestyle=\"--\", label=label1)\nax.xaxis.set_major_formatter(mpl.ticker.StrMethodFormatter(\"{x:,.0f}\"))\nax.set_yticklabels([])\nax.set_xlabel(\"\")\nax.set_ylabel(\"\")\nax.tick_params(axis=\"x\", which=\"major\", direction='in', labelsize=7)\nax.tick_params(axis=\"y\", which=\"major\", direction='in', labelsize=7)\nax.xaxis.set_ticks_position(\"none\")\nax.yaxis.set_ticks_position(\"none\")\nax.grid(True)   \nax.set_axisbelow(True) \nax.legend(loc=\"upper right\", fancybox=True, framealpha=1, fontsize=\"small\")\nplt.show();\n\n\n\n\nHistogram of posterior predictive samples\n\n\n\n\nFinally, percentiles of the posterior predictive samples:\n\n\npeval = [1, 5, 25, 50, 75, 95, 99]\npcols = [f\"{ii:.0f}%\" for ii in peval]\n\ndf = pd.DataFrame(\n    np.round(np.percentile(samples, q=peval), 2).reshape(1,-1), \n    columns=pcols,index=[\"posterior predictive samples\"]\n    )\ndf.head(1)\n\n\n\n\n\n\n\n\n1%\n5%\n25%\n50%\n75%\n95%\n99%\n\n\n\n\nposterior predictive samples\n464.64\n530.08\n613.37\n673.32\n731.45\n813.15\n883.41"
  },
  {
    "objectID": "posts/grep-zero-width-assertions/grep-zero-width-assertions.html",
    "href": "posts/grep-zero-width-assertions/grep-zero-width-assertions.html",
    "title": "Utilizing Zero-Width Assertions with grep",
    "section": "",
    "text": "Recently, I needed to extract a set of port numbers from a .json file in order to connect to a remotely running IPython kernel instance locally, and learned that it is possible to use zero-width assertions with grep. Zero-width assertions are regular expressions that match a specific pattern without consuming, so they can be used to anchor your regular expression within the target text.\nThe IPython kernel file structure is consistent across invocations, but each new file contains different random port numbers which need to be forwarded between local and remotehost. These ports facilitate the connection to the instance of the IPython kernel running on remotehost. What follows is the content of a typical kernel file with randomly-generated ports (the filename is also random, but follows the format kernel-####.json):\n{\n  \"iopub_port\": 39736,\n  \"control_port\": 59725,\n  \"transport\": \"tcp\",\n  \"shell_port\": 51963,\n  \"key\": \"1fcd997c-ef64-4322-8762-c034af6095e1\",\n  \"stdin_port\": 59714,\n  \"signature_scheme\": \"hmac-sha256\",\n  \"hb_port\": 41128,\n  \"ip\": \"127.0.0.1\"\n}\nOur goal is to extract and forward the randomly generated port numbers using ssh. One approach is provided in the IPython Cookbook recipe, which extracts the ports and forwards them iteratively:\n#!/bin/bash\n\n# Assume kernel connection details reside in `kernel-2323.json`\n\nfor port in $(cat kernel-2323.json | grep '_port' | grep -o '[0-9]\\+'); do\n    ssh remotehost.com -f -N -L $port:127.0.0.1:$port\ndone\nWhere:\n\n-f Requests ssh to go to background just before command execution.\n\n-N Do not execute a remote command. This is useful for just forwarding ports.\n\n-L Specifies that connections to the given port on localhost are to be forwarded to the port on remotehost.\n\nThis method works without issue. However, the pattern extracts port numbers without considering the port’s associated kernel component. For example, if we needed to know which port corresponds to shell_port, this solution falls short.\nAn alternative approach uses grep’s zero-width assertion operator \\K. This option isn’t listed in grep’s help menu or man page, but is nonetheless valid syntactically (the -P flag indicates that the pattern is a Perl regular expression). Simply provide grep with any valid regular expression pattern: If \\K is included within the regular expression, the matching text that follows will be returned if and only if what precedes it also matches. This is also known as a positive lookbehind assertion.\nThe next example parses kernel-2323.json as before, but this time retains the component-to-port mapping. After extracting the kernel component names and ports, we save them to an associative array:\n#!/bin/bash\n\nKERNEL_FILENAME=\"kernel-2323.json\"\n\ndeclare -a portsArr=('hb_port' 'iopub_port' 'control_port' 'shell_port' 'stdin_port');\ndeclare -A kernelDict  # Associative array to hold component-port mapping\n\nfor portname in \"${portsArr[@]}\"\ndo\n    PATTERN=\"[[:space:]]+\\\"${portname}\\\":[[:space:]]+\\K[0-9]{2,5}\"\n    PORTNBR=$(grep -Po ${PATTERN} \"${KERNEL_FILENAME}\")\n    echo \"Now forwarding ${portname}...\"\n    ssh remotehost.com -f -N -L ${PORTNBR}:127.0.0.1:${PORTNBR}\n    # Add component-port mapping to kernelDict.\n    kernelDict[\"${portname}\"]=\"${PORTNBR}\"\ndone\nNotice the placement of \\K: At each iteration, the pattern specifies that a matching string will contain the port name followed by a colon and one or more whitespace characters, followed by 2-5 digits. Since \\K directly precedes “[0-9]{2,5}” successful matches will only return that portion of a matching string.\nOur implementation works as expected but is inefficient: For each port number extracted and forwarded, the kernel file is reopened and reread. For this example it’s not much of a problem, but for larger files, this approach could result in serious performance degradation. A more efficient solution would read the kernel file in one time, storing it in a variable, and searching this variable against the regular expression pattern at each iteration. The change in logic is subtle: the only difference is reading the file into the variable identified as KERNEL_CONTENTS at the start of the script, and the inclusion of &lt;&lt;&lt; after the grep command:\n#!/bin/bash\n\nKERNEL_FILENAME=\"kernel-2323.json\"\nKERNEL_CONTENTS=\"$(cat ${KERNEL_FILENAME})\"\n\ndeclare -a portsArr=('hb_port' 'iopub_port' 'control_port' 'shell_port' 'stdin_port');\ndeclare -A kernelDict  # Associative array to hold component-port mapping\n\nfor portname in \"${portsArr[@]}\"\ndo\n    PATTERN=\"[[:space:]]+\\\"${portname}\\\":[[:space:]]+\\K[0-9]{2,5}\"\n    PORTNBR=$(grep -Po ${PATTERN} &lt;&lt;&lt; \"${KERNEL_CONTENTS}\")\n    echo \"Now forwarding ${portname}...\"\n    ssh remotehost.com -f -N -L ${PORTNBR}:127.0.0.1:${PORTNBR}\n    # Add component-port mapping to kernelDict.\n    kernelDict[\"${portname}\"]=\"${PORTNBR}\"\ndone\nThe &lt;&lt;&lt; syntax is used to indicate a here string, a form of input redirection which allows variables containing text to be interpreted as a file-like object. See this link for more information."
  },
  {
    "objectID": "posts/lle-optim-r/lle-optim-r.html",
    "href": "posts/lle-optim-r/lle-optim-r.html",
    "title": "LogLikelihood Estimation in R with optim",
    "section": "",
    "text": "There are many R packages available to assist with finding maximum likelihood estimates based on a given set of data (for example, fitdistrplus), but implementing a routine to find MLEs is a great way to learn how to use the optim subroutine. In the example that follows, I’ll demonstrate how to find the shape and scale parameters for a Gamma distribution using synthetically generated data via maximum likelihood.\nThe available parameters for optim are given below:\noptim(par, fn, gr=NULL, ...,\n      method=c(\"Nelder-Mead\", \"BFGS\", \"CG\", \"L-BFGS-B\", \"SANN\", \"Brent\"),\n      lower=-Inf, upper=Inf,\n      control=list(), hessian=FALSE)\n\npar \nInitial values for the parameters to be optimized over.\n\nfn  \nA function to be minimized (or maximized), with first argument the vector of \nparameters over which minimization is to take place. It should return a scalar \nresult.\n\ngr  \nA function to return the gradient for the \"BFGS\", \"CG\" and \"L-BFGS-B\" methods. \nIf it is NULL, a finite-difference approximation will be used.\n\nFor the \"SANN\" method it specifies a function to generate a new candidate \npoint. If it is NULL a default Gaussian Markov kernel is used.\n\n... \nFurther arguments to be passed to fn and gr.\n\nmethod  \nThe method to be used. See 'Details'. Can be abbreviated.\n\nlower, upper    \nBounds on the variables for the \"L-BFGS-B\" method, or bounds in which to search \nfor method \"Brent\".\n\ncontrol \na list of control parameters. See 'Details'.\n\nhessian \nLogical. Should a numerically differentiated Hessian matrix be returned?\n\noptim is passed a function which takes a single vector argument representing the parameters we hope to find (fn) along with a starting point from which the selected optimization routine begins searching (par), with one starting value per parameter.\nThe gamma distribution can be parameterized in a number of ways, but for this demonstration, we use the shape-scale parameterization, with density given by:\n\\[\nf(x|\\theta, \\alpha) = \\frac{x^{\\alpha-1}e^{-x/\\theta}}{\\theta^{\\alpha} \\Gamma(\\alpha)},\n\\hspace{.50em} x &gt;= 0; \\hspace{.50em}\\alpha, \\theta &gt; 0,\n\\]\nwhere \\(\\boldsymbol{\\alpha}\\) represents the shape parameter and \\(\\boldsymbol{\\theta}\\) the scale parameter. The Gamma distribution has variance in proportion to the mean, which differs from the normal distribution which has constant variance across observations. Specifically, for gamma distributed random variable \\(X\\), the mean and variance are:\n\\[\nE[X] = \\alpha \\theta \\hspace{1.0em} Var[X] =  \\alpha \\theta^{2}\n\\]\nA useful feature of the gamma distribution is that it has a constant coefficient of variation:\n\\[\n\\mathrm{CV} = \\sigma / \\mu = \\sqrt{\\mathrm{Var}[X]} / \\mathrm{E}[X] = \\sqrt{\\alpha \\theta^{2}} / \\alpha \\theta = 1 / \\sqrt{\\alpha}.\n\\]\nThus, for any values of \\(x\\) fit to a gamma distribution with parameters \\(\\theta\\) and \\(\\alpha\\), the ratio of the standard deviation to the mean will always equate to \\(1 / \\sqrt{\\alpha}\\).\n\nMaximum Likelihood Estimation\nMaximum likelihood estimation (MLE) is a technique used to estimate parameters for a candidate model or distributional form. Essentially, MLE aims to identify which parameter(s) make the observed data most likely, given the specified model. In practice, we do not know the values of the proposed model parameters but we do know the data. We use the likelihood function to observe how the function changes for different parameter values while holding the data fixed. This can be used to judge which parameter values lead to greater likelihood of the sample occurring.\nThe joint density of \\(n\\) independently distributed observations is given by:\n\\[\nf(\\mathbf{x}|\\boldsymbol{\\beta}) = \\prod_{i=1}^{n} f_{i}(x_{i}|\\beta), \\hspace{0.5em} 1 \\leq i \\leq n.\n\\]\nWhen this expression is interpreted as a function of unknown \\(\\boldsymbol{\\beta}\\) given known data \\(x\\), we obtain the likelihood function:\n\\[\n\\mathcal{L}(\\mathbf{\\beta}|\\mathbf{x}) = \\prod_{i=1}^{n} f_{i}(x_{i}|\\mathbf{\\beta}).\n\\]\nSolving the likelihood equation can be difficult. This can be partially alleviated by logging the likelihood expression, which results in an expression for the loglikelihood. When solving the likelihood, it is often necessary to take the derivative of the expression to find the optimum (although not all optimizers are gradient based). It is much more computationally stable and conceptually straightforward to take the derivate of an additive function of independent observations (loglikelihood) as opposed to a multiplicative function of independent observations (likelihood). The loglikelihood is given by:\n\\[\n\\mathcal{l}(\\mathbf{\\beta}|\\mathbf{x}) = \\sum_{i=1}^{n} \\mathrm{Ln}(f_{i}(x_{i}|\\mathbf{\\beta})), \\hspace{0.5em} 1 \\leq i \\leq n.\n\\]\nReferring back to the shape-scale parameterized gamma distribution, the joint density is represented as\n\\[\nf(x|\\theta, \\alpha) = \\prod_{i=1}^{n}\\frac{x_{i}^{\\alpha-1}e^{-x_{i}/\\theta}}{\\theta^{\\alpha} \\Gamma(\\alpha)},\n\\hspace{.50em} x_{i} &gt;= 0; \\hspace{.50em}\\alpha, \\theta &gt; 0,\n\\]\nTaking the natural log of the joint density results in the loglikelihood for our proposed distributional form:\n\\[\n\\mathcal{l}(\\mathbf{\\theta, \\alpha}|\\mathbf{x}) = \\prod_{i=1}^{n}\\mathrm{Ln}\\Big(\\frac{x_{i}^{\\alpha-1}e^{-x_{i}/\\theta}}{\\theta^{\\alpha} \\Gamma(\\alpha)}\\Big)\n\\]\nExpanding the loglikelihood and focusing on a single observation \\(x_{i}\\) yields:\n\\[\n\\mathrm{Ln}(x_{i}^{\\alpha -1}) + \\mathrm{Ln}(e^{-x_{i} / \\theta}) - \\mathrm{Ln}(\\theta^{\\alpha}) - \\mathrm{Ln}(\\Gamma(\\alpha)).\n\\]\nNow considering all observations, after a bit of rearranging and simplification we obtain\n\\[\n\\mathcal{l}(\\mathbf{\\theta, \\alpha}|\\mathbf{x}) = (\\alpha -1)\\sum_{i=1}^{n}\\mathrm{Ln}(x_{i}) - \\theta^{-1}\\sum_{i=1}^{n}x_{i} - n \\alpha \\mathrm{Ln}(\\theta) -n\\mathrm{Ln}(\\Gamma(\\alpha)).\n\\]\nNext, the the partial derivatives w.r.t. \\(\\theta\\) and \\(\\alpha\\) would be obtained and set equal to zero in order to find the solutions directly or in an iterative fashion. But when using optim, we only need to go as far as producing an expression for the joint loglikelihood over the set of observations.\n\n\nImplementation\nThe function eventually passed along to optim will be implemented as a closure. A closure is a function which returns another function. A trivial example would be one in which an outer function wraps and inner function that computes the product of two numbers, which is then raised to a power specified as an argument accepted by the outer function. You’d be correct to think this problem could just as easily be solved as a 3-parameter function. However, you’d be required to pass the 3rd argument representing the degree to which the product should be raised every time the function is called. An advantage of closures is that any parameters associated with the outer function are global variables from the perspective of the inner function, which is useful in many scenarios.\nNext we implement a closure as described in the previous paragraph: The inner function takes two numeric values, a and b, and returns the product a * b. The outer function takes a single numeric argument, pow, which determines the degree to which the product should be raised. The final value returned by the function will be (a * b)^pow:\n# Closure in which a power is specified upon initialization. Then, for each subsequent\n# invocation, the product a * b is raised to pow. \nprodPow = function(pow) {\n    function(a, b) {\n        return((a * b)^pow)\n    }\n}\nTo initialize the closure, we call prodPow, but only pass the argument for pow. We will inspect the result and show that it is a function:\n&gt; func = proPow(pow=3)\n&gt; class(func)\n[1] \"function\"\n\n&gt; names(formals(func))\n[1] \"a\" \"b\"\nThe object bound to func is a function with two arguments, a and b. If we invoke func by specifying arguments for a and b, we expect a numeric value to be returned representing the product raised to the 3rd power:\n&gt; func(2, 4)\n[1] 512\n\n&gt; func(7, 3)\n[1] 9261\n\n\nGamma LogLikelihood\nWe next implement the gamma loglikelihood as a closure. The reason for doing this has to do with the way in which optim works. The arguments associated with the function passed to optim should consist of a vector of the parameters of interest, which in our case is a vector representing \\((\\alpha, \\theta)\\). By implementing the function as a closure, we can reference the set of observations (vals) from the scope of the inner function without having to pass the data as an argument for the function being optimized. This is very useful, since, if you recall from our final expression for the loglikelihood, it is necessary to compute \\(\\sum_{i=1}^{n}\\mathrm{Ln}(x_{i})\\) and \\(\\sum_{i=1}^{n}x_{i}\\) at each evaluation. It is far more efficient to compute these quantities once then reference them from the inner function as necessary without requiring recomputation.\nThe Gamma loglikelihood closure is provided below:\n# Loglikelihood for the Gamma distribution. The outer function accepts the set of observations.\n# The inner function takes a vector of parameters (alpha, theta), and returns the loglikelihood. \ngammaLLOuter = function(vals) {\n    # -------------------------------------------------------------\n    # Return a function representing the the Gamma loglikelihood. |\n    # `n` represents the number of observations in vals.          |\n    # `s` represents the sum of vals with NAs removed.            |\n    # `l` represents the sum of log(vals) with NAs removed.       |\n    # -------------------------------------------------------------\n    n = length(vals)\n    s = sum(vals, na.rm=TRUE)\n    l = sum(log(vals), na.rm=TRUE)\n    \n    function(v) {\n        # ---------------------------------------------------------\n        # v represents a length-2 vector of parameters alpha      |\n        # (shape) and theta (scale).                              | \n        # Returns the loglikelihood.                              |\n        # ---------------------------------------------------------\n        a = v[1]\n        theta = v[2]\n        return((a - 1) * l  - (theta^-1) * s  - n * a * log(theta) -n * log(gamma(a)))\n    }\n}\n50 random observations from a gamma distribution with Gaussian noise are generated using \\(\\alpha = 5\\) and \\(\\theta = 10000\\). We know beforehand the data originate from a gamma distribution with noise. We want to verify that the optimization routine can recover these parameters given only the data:\na = 5         # shape\ntheta = 10000 # scale\nn = 50\nvals = rgamma(n=n, shape=a, scale=theta) + rnorm(n=n, 0, theta / 100)\nReferring back to the call signature for optim, we need to provide initial parameter values for the optimization routine (the par argument). Using 0 can sometimes suffice, but since \\(\\alpha, \\theta &gt; 0\\), different values must be provided. We can leverage the gamma distribution’s constant coefficient of variation to get an initial estimate of the shape parameter \\(\\alpha_{0}\\), then use \\(\\alpha_{0}\\) along with the empirical mean to back out an initial scale parameter estimate, \\(\\theta_{0} = E[X] / \\alpha_{0}\\). In R, this is accomplished as follows:\n# Get initial estimates for a, theta. \n valsMean = mean(vals, na.rm=TRUE) # empirical mean of vals.\n valsStd = sd(vals, na.rm=TRUE)    # empirical standard deviation of vals.\n a0 = (valsMean / valsStd)^2       # initial shape parameter estimate.\n theta0 = valsMean / a0            # initial scale parameter estimate.\nWe have everything we need to compute maximum likelihood estimates. Two final points: By default, optim minimizes the function passed into it. Since we’re looking to maximize the loglikelihood, we need to include an additional argument to the control parameter as list(fnscale=-1) to ensure optim returns the maximum. Second, there are a number of different optimizers from which to choose. A generally good choice is “BFGS”, which I use here.\nWe initialize gammaLLOuter with vals, then pass the initial parameter values along with method=\"BFGS\" and control=list(fnscale=-1) into optim:\n# Generating maximum likelihood estimates from data assumed to follow a gamma distribution. \noptions(scipen=-9999)\nset.seed(516)\n\n\ngammaLLOuter = function(vals) {\n    # -------------------------------------------------------------\n    # Return a function representing the the Gamma loglikelihood. |\n    # `n` represents the number of observations in vals.          |\n    # `s` represents the sum of vals with NAs removed.            |\n    # `l` represents the sum of log(vals) with NAs removed.       |\n    # -------------------------------------------------------------\n    n = length(vals)\n    s = sum(vals, na.rm=TRUE)\n    l = sum(log(vals), na.rm=TRUE)\n    \n    function(v) {\n        # ---------------------------------------------------------\n        # `v` represents a length-2 vector of parameters alpha    |\n        # (shape) and theta (scale).                              | \n        # Returns the loglikelihood.                              |\n        # ---------------------------------------------------------\n        a = v[1]\n        theta = v[2]\n        return((a - 1) * l  - (theta^-1) * s  - n * a * log(theta) -n * log(gamma(a)))\n    }\n}\n\n\n# Generate 50 random Gamma observations with Gaussian noise. \na = 5         # shape\ntheta = 10000 # scale\nn = 50\nvals = rgamma(n=n, shape=a, scale=theta) + rnorm(n=n, 0, theta / 100)\n\n# Initialize gammaLL.\ngammaLL = gammaLLOuter(vals)\n\n# Determine initial estimates for shape and scale.\n valsMean = mean(vals, na.rm=TRUE)\n valsStd = sd(vals, na.rm=TRUE)\n a0 = (valsMean / valsStd)^2 \n theta0 = valsMean / a0\n paramsInit = c(a0, theta0)\n\n# Dispatch arguments to optim.\nparamsMLE = optim(\n    par=paramsInit, fn=gammaLL, method=\"BFGS\", control=list(fnscale=-1)\n    )\noptim returns a list with a number of elements. We are concerned primarily with three elements:\n\nconvergence: Specifies whether the optimization converged to a solution. 0 means yes,\nany other number means it did not converge.\npar: A vector of parameter estimates.\nvalue: The maximized loglikelihood.\n\nIf you plug in the values for \\(\\alpha\\) and \\(\\theta\\) from paramsMLE$par, you would get a value equal to paramsMLE$value.  Checking the values returned by the call tooptim` above, we have:\n&gt; paramsMLE$convergence\n[1] 0\n&gt; paramsMLE$par\n[1]    5.234362 9515.717485\n&gt; paramsMLE$value\n[1] -566.4171\nLet’s compare our estimates against estimates produced by fitdistrplus. We need to scale our data by 100, otherwise fitdist throws an error:\n&gt; fitdistrplus::fitdist(vals/100, \"gamma\")\nFitting of the distribution ' gamma ' by maximum likelihood \nParameters:\n        estimate  Std. Error\nshape 5.35617640 0.983440540\nrate  0.01077757 0.002056568\nIf we reciprocate the estimate for “rate” and multiply by 100 (the amount we divided vals by in the call to fitdist), we get a scale estimate of 9278.53. Note that dividing by 100 works for the Gamma density, since it is an exponential scale family distribution, but this will not work for distributions generally. In summary:\n                          shape        scale\nOur estimate         : 5.234362   9515.71748\nfitdistrplus estimate: 5.356176   9278.52939\n% difference         : 2.22743%     2.55631%\nWe find the estimates for each parameter are within 3% of one another."
  },
  {
    "objectID": "posts/logistic-regression-gd/logistic-regression-gd.html",
    "href": "posts/logistic-regression-gd/logistic-regression-gd.html",
    "title": "Gradient Descent for Logistic Regression",
    "section": "",
    "text": "Within the GLM framework, model coefficients are estimated using iterative reweighted least squares (IRLS), sometimes referred to as Fisher Scoring. This works well, but becomes inefficient as the size of the dataset increases: IRLS relies on the matrix of second partial derivatives which is expensive to compute. Instead, we can estimate logistic regression coefficients using gradient descent, which only relies on the first derivative of the cost function. This is much more efficient to compute, and generally provides good estimates once features have been standardized.\nExpression for linear predictions:\n\\[\nz = \\boldsymbol{X} \\boldsymbol{\\theta}\n\\]\nExpression for predicted probabilities:\n\\[\n\\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n\\]\nGradient descent:\n\\[\n\\theta_{t + 1} := \\theta_{t} - \\eta \\nabla L(\\hat{y}, y)\n\\]\n\n\\(\\eta\\) is the learning rate.\n\\(\\nabla L\\) represents the gradients of the loss function.\n\nOptimal parameters:\n\\[\n\\boldsymbol{\\hat{\\theta}} = \\operatorname*{argmin}_\\theta \\frac{1}{m} \\sum_{i=1}^{m} L(f(x^{(i)}; \\theta), y^{(i)})\n\\]\nThe loss function is used to determine how much predictions differs from labels. Here we use binary cross entropy loss:\n\\[\nL(\\hat{y}, y) = -\\frac{1}{m} \\sum_{i=1}^{m} y \\cdot \\mathrm{log}(\\hat{y}) + (1 - y) \\cdot \\mathrm{log}(1 - \\hat{y}),\n\\]\nwhich we obtain from the log-likelihood of a single observation assumed to follow a binomial distribution. We flip the sign (the leading -) in order to minimize the loss. Note that \\(\\hat{y} = \\sigma(z)\\).\nExpression for gradient of loss function:\n\\[\n\\nabla L(\\hat{y}, y) = \\frac{1}{m}(\\hat{y} - y)x^{T}\n\\]\nWe choose \\(\\boldsymbol{\\theta}\\) that maximizes the log-probability of the true y labels in the training data given the observations \\(X\\). The goal is to find the set of weights which minimize the loss function, averaged over all examples. For each variable \\(\\theta_{j}\\) in \\(\\boldsymbol{\\theta}\\), the gradient will have a component that tells us the slope w.r.t. that variable.\nThe breast cancer dataset is loaded from scikit-learn, the features are standardized and model coefficients estimated using gradient descent. Results are then compared with statsmodels coefficient estimates using the same data to ensure correctness.\n\n\"\"\"\nLoad breast cancer dataset from scikit-learn. Standardize features.\n\"\"\"\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\n\ndata_loader = load_breast_cancer()\nX_data, y_data = data_loader.data, data_loader.target\ncolumn_names = data_loader.feature_names\ndf = pd.DataFrame(X_data, columns=column_names)\n\nkeep_features = [\n    'mean radius', 'mean texture', 'mean perimeter', 'mean area',\n    'mean smoothness'\n    ]\n\nX = df[keep_features].values\ny = y_data[:,None]\n\n# Standardize features. \nsclr = StandardScaler()\nX = sclr.fit_transform(X)\n\n# Add bias term.\nbias = np.ones(X.shape[0])[:, None]\nX = np.concatenate([bias, X], axis=1)\n\neta represents the learning rate and is a hyperparameter. num_epochs can be set as desired. For this example, we haven’t incorporated early stopping logic, but this would be straightforward to implement. We initialize the weight vector w to all 0s. w has the same length as the number of features + 1 for the intercept term.\n\n\neta = .50\nnum_epochs = 50000\nw = np.zeros((X.shape[1], 1))\nloss = []\n\nfor _ in range(num_epochs):\n    z = X @ w\n    ypred = 1. / (1. + np.exp(-z))\n    L = -np.mean(y * np.log(ypred) + (1 - y) * np.log(1 - ypred))\n    dL = np.mean(((ypred - y) * X), axis=0)[:, None]\n    loss.append(L)        \n    w-=eta * dL\n\nprint(f\"w: {w.ravel()}\")\n\nw: [ -0.4592425   22.09479813  -1.56463209 -14.7402888  -14.68918055\n  -1.66460167]\n\n\nEstimating the coefficients using statsmodels yields:\n\nimport statsmodels.formula.api as smf\n\ndf2 = pd.DataFrame(X[:,1:], columns=keep_features)\ndf2[\"target\"] = y\ndf2.columns = [ii.replace(\" \", \"_\") for ii in df2.columns]\nfeatures = \" + \".join([ii for ii in df2.columns if ii!=\"target\"])\nmdl_expr = \"target ~ \" + features\nmdl = smf.logit(mdl_expr, data=df2).fit()\nmdl.params\n\nOptimization terminated successfully.\n         Current function value: 0.148702\n         Iterations 10\n\n\nIntercept          -0.459246\nmean_radius        22.094852\nmean_texture       -1.564632\nmean_perimeter    -14.740317\nmean_area         -14.689213\nmean_smoothness    -1.664601\ndtype: float64\n\n\nWhich matches closely with the results produced via gradient descent."
  },
  {
    "objectID": "posts/mat-fact-lin-reg/mat-fact-lin-reg.html",
    "href": "posts/mat-fact-lin-reg/mat-fact-lin-reg.html",
    "title": "A Matrix Factorization Approach to Linear Regression",
    "section": "",
    "text": "This post is intended to shed light on why the closed form solution to linear regression estimates is avoided in statistical software packages. But we start by first by deriving the solution to the normal equations within the standard multivariate regression setting.\nThe normal linear regression model specifies that the sampling variability around the mean is i.i.d. from a normal distribution:\n\\[\n\\epsilon_{1}, \\cdots, \\epsilon_{n} \\sim \\mathrm{i.i.d.}\\hspace{.25em}\\mathrm{normal}(0, \\sigma^{2})\\\\\ny_{i} = \\beta^{T}x_{i} + \\epsilon_{i}.\n\\]\nTherefore the specification for the joint PDF of observed data conditional upon and values is given by:\n\\[\n\\begin{align*}\nf(y_{1}, \\cdots, y_{n}|x_{1}, \\cdots, x_{n}, \\beta, \\sigma^{2}) &= \\prod_{i=1}^{n} f(y_{i}|x_{i}, \\beta, \\sigma^{2})\\\\\n&= (2\\pi \\sigma^{2})^{-n/2} \\mathrm{exp}\\big(-\\frac{1}{2\\sigma^{2}} \\sum_{i=1}^{n}(y_{i} - \\beta^{T}x_{i})^{2}\\big).\n\\end{align*}\n\\]\nAlternatively, the joint PDF can be represented in terms of the multivariate normal distribution. Let \\(y\\) be the n-dimensional response vector, and \\(X\\) the \\(n \\times p\\) design matrix whose \\(i^{th}\\) row is \\(x_{i}\\). We have:\n\\[\ny|X,\\beta, \\sigma^{2} \\sim \\mathrm{MVN}(X\\beta, \\sigma^{2} \\mathrm{I})\n\\]\nWhere \\(I\\) represents the \\(p \\times p\\) identity matrix.\nThe density depends on \\(\\beta\\) through the residuals. Given the observed data, the term in the exponent \\((y_{i} - \\beta^{T}x_{i})\\) is maximized when the sum of squared residuals, \\(\\mathrm{SSR} = \\sum_{i=1}^{n} (y_{i} - \\beta^{T}x_{i})\\) is minimized. To find the optimal \\(\\beta\\), we expand the expression for the residual sum of squares:\n\\[\n\\begin{align*}\n\\mathrm{SSR} &= \\sum_{i=1}^{n} (y_{i} - \\beta^{T}x_{i})\\\\\n&=(y - X\\beta)^{T}(y - X\\beta)\\\\\n&=y^{T}y - 2\\beta^{T}X^{T}y + \\beta^{T}X^{T}X\\beta.\n\\end{align*}\n\\]\nComputing the first derivative of the last expression above w.r.t. \\(\\beta\\) and setting equal to 0 yields \\(-2X^{T}y + 2X{T}X\\beta = 0\\), which can be rearranged to obtain the familiar expression for the ordinary least squares solution:\n\\[\n\\beta = (X^{T}X)^{-1}X^{T}y.\n\\]\nWhy isn’t this expression implemented in linear model solvers directly?\nThe condition number of a matrix is the ratio of maximum-to-minimum singular values (which, for a normal matrix, is the ratio of the maximum-to-minimum absolute value of eigenvalues). Essentially, the condition number tells you how much solving a linear system will magnify any noise in your data. It can be thought of as a measure of amplification. The smaller the condition number, the better (the best value being 1).\n\n\"\"\"\nDemonstrating the equivalence of computing the condition\nnumber as the ratio of maximum-to-minimum singular values\nand using np.linalg.cond, as well as a comparison of the \ncondition numbers for X vs. X^T*X. \n\"\"\"\nimport numpy as np\nrng = np.random.default_rng(516)\n\nX = rng.normal(size=(50, 10))\n\n# SVD for X. \nU0, S0, Vt0 = np.linalg.svd(X, full_matrices=True)\nc0 = np.linalg.cond(X, p=None)\n\n# SVD for X^T*X. \nU1, S1, Vt1 = np.linalg.svd(X.T @ X, full_matrices=True)\nc1 = np.linalg.cond(X.T @ X, p=None)\n\n# S0 and S1 represent the singular values of X and X^T*X.\nprint(f\"S0.max() / S0.min()       : {S0.max() / S0.min():.8f}.\")\nprint(f\"Condition number of X     : {c0:.8f}.\")\nprint(f\"S1.max() / S1.min()       : {S1.max() / S1.min():.8f}.\")\nprint(f\"Condition number of X^T*X : {c1:.8f}.\")\n\n\nS0.max() / S0.min()       : 2.44498390.\nCondition number of X     : 2.44498390.\nS1.max() / S1.min()       : 5.97794628.\nCondition number of X^T*X : 5.97794628.\n\n\nIn terms of numerical precision, computing \\(X^{T}X\\) roughly squares the condition number. As an approximation, \\(\\mathrm{log}_{10}(\\mathrm{condition})\\) represents the number of digits lost in a given matrix computation. So by merely forming the Gram matrix, we’ve doubled the loss of precision in our final result, since\n\\[\n\\mathrm{log}_{10}(\\mathrm{condition}(X^{T}X)) \\approx 2 \\times \\mathrm{log}_{10}(\\mathrm{condition}(X)).\n\\]\nIf the condition number of \\(X\\) is small, forming the Gram matrix and solving the system via \\(\\beta = (X^{T}X)^{-1}X^{T}y\\) should be fine. But as the condition number grows, solving the normal equations becomes increasingly unstable, ultimately resulting in a solution devoid of accuracy. Since statistical software packages need to handle an incredible variety of potential design matrices with a wide range of condition numbers, the normal equations approach cannot be relied upon.\n\nThe QR Decomposition\nThe QR Decomposition factors a matrix \\(X\\) into the product of an orthonormal matrix \\(Q\\) and an upper triangular matrix \\(R\\), \\(X = QR\\). Because \\(Q\\) is orthonormal, \\(Q^{T} = Q^{-1}\\). Beginning with a version of the normal equations solution, substitute \\(QR\\) for \\(X\\), rearrange and arrive at:\n\\[\n\\begin{align*}\nX^{T}X \\beta &= X^{T}y\\\\\n(QR)^{T}(QR) \\beta &= (QR)^{T}y\\\\\nR^{T}(Q^{T}Q)R \\beta &= R^{T} Q^{T} y\\\\\nR^{T}R \\beta &= R^{T} Q^{T} y\\\\\n(R^{T})^{-1} R^{T} R \\beta &= (R^{T})^{-1} R^{T} Q^{T} y\\\\\nR \\beta &= Q^{T} y,\n\\end{align*}\n\\]\nwhere we’ve taken advantage of how transpose distributes over matrix products (i.e. \\((AB)^{T} = B^{T}A^{T}\\)), and the fact that since \\(Q\\) is orthonormal, \\(Q^{T}Q = I\\).\nBecause \\(R\\) is upper triangular, \\(\\beta\\) can be solved for using back substitution. A quick demonstration of how this can be accomplished with Scipy:\n\n\n# Solving for regression coefficients using normal equations and QR decomposition. \nimport numpy as np\nfrom scipy.linalg import solve_triangular\nrng = np.random.default_rng(516)\n\nX = rng.normal(size=(50, 5))\ny = rng.normal(scale=5, size=50)\n\n# Normal equations solution.\nB0 = np.linalg.inv(X.T @ X) @ X.T @ y\n\n# QR Decomposition solution.\nQ, R = np.linalg.qr(X, mode=\"reduced\")\nB1 = solve_triangular(R, Q.T @ y)\n\nprint(f\"B0: {B0}\")\nprint(f\"B1: {B1}\")\nprint(f\"np.allclose(B0, B1): {np.allclose(B0, B1)}\")\n\nB0: [ 0.42402765 -1.21951527  0.22396056  0.26773935 -0.72067314]\nB1: [ 0.42402765 -1.21951527  0.22396056  0.26773935 -0.72067314]\nnp.allclose(B0, B1): True\n\n\nUsing the QR Decomposition, we’ve eliminated the need to explicitly create the Gram matrix, and no longer need to invert matrices, which is computationally expensive and has its own set of precision degradation issues.\n\n\nThe Singular Value Decomposition\nThe Singular Value Decomposition is a generalization of the Eigendecomposition to any \\(nxp\\) matrix. The SVD decomposes a matrix \\(A\\) into 3 matrices (\\(A = U \\Sigma V^{T}\\)):\n\n\\(U\\) is a \\(n \\times p\\) orthogonal matrix (assuming \\(A\\) is real); columns represent left singular vectors.\n\\(\\Sigma\\) is a \\(p \\times p\\) diagonal matrix with diagonal entries representing the singular values of \\(A\\).\n\\(V^{T}\\) is a \\(p \\times p\\) orthogonal matrix (assuming \\(A\\) is real); rows represent right singular vectors.\n\nBeginning with the normal equations solution, replace \\(X\\) with \\(U \\Sigma V^{T}\\) and solve for \\(\\beta\\):\n\\[\n\\begin{align*}\nX^{T}X \\beta &= X^{T}y\\\\\n(U \\Sigma V^{T})^{T}U \\Sigma V^{T}\\beta &= (U \\Sigma V^{T})^{T}y\\\\\nV \\Sigma^{T} U^{T} U \\Sigma V^{T} \\beta &= V \\Sigma U^{T} y\\\\\nV \\Sigma^{T} \\Sigma V^{T} \\beta &=V \\Sigma U^{T} y\\\\\nV V^{T} \\beta &= V \\Sigma^{-1} U^{T} y\\\\\n\\beta &= V \\Sigma^{-1} U^{T} y\n\\end{align*}\n\\]\nSince \\(\\Sigma\\) is a diagonal matrix, the inverse is simply the reciprocal of the diagonal elements, which doesn’t incur the runtime associated with a conventional matrix inversion. In addition, \\(VV^{T} = I\\). Note that we assume the singular values are strictly greater than 0. If this is not the case, the condition number would be infinite, and a well-defined solution wouldn’t exist.\nDetermining the estimated coefficients using SVD can be accomplished as follows:\n\n\n# Solving for regression coefficients using normal equations and SVD. \nimport numpy as np\nrng = np.random.default_rng(516)\n\nX = rng.normal(size=(50, 5))\ny = rng.normal(scale=5, size=50)\n\n# Normal equations solution.\nB0 = np.linalg.inv(X.T @ X) @ X.T @ y\n\n# SVD solution.\nU, S, Vt = np.linalg.svd(X, full_matrices=False)\nB1 = Vt.T @ np.diag(1 / S) @ U.T @ y\n\nprint(f\"B0: {B0}\")\nprint(f\"B1: {B1}\")\nprint(f\"np.allclose(B0, B1): {np.allclose(B0, B1)}\")\n\nB0: [ 0.42402765 -1.21951527  0.22396056  0.26773935 -0.72067314]\nB1: [ 0.42402765 -1.21951527  0.22396056  0.26773935 -0.72067314]\nnp.allclose(B0, B1): True\n\n\nEach of these methods (normal equations, QR Decomposition, SVD) incur different computational cost. From Golub & Van Loan, the flop count associated with each algorithm is presented below:\n-------------------------------------------\nNormal Equations       |  mn^2 + n^3 / 3  |\n-------------------------------------------\nHouseholder QR         |  n^3 / 3         |\n-------------------------------------------\nModified Gram-Schmidt  |  2mn^2           |\n-------------------------------------------\nSVD                    |  2mn^2 + 11n^3   |\n-------------------------------------------\nHouseholder and Modified Gram-Schmidt are two approaches used in the first step of the QR Decomposition. SVD offers far more stability, but comes with added runtime complexity. Other matrix decomposition methods such as LU and Cholesky can be leveraged, but the QR Decomposition represents a good trade-off between runtime and numerical stability. This explains its wide use in statistical software packages. Check out A Deep Dive into how R Fits a Linear Model for an in-depth explanation of how the QR Decomposition is used to fit linear models in R."
  },
  {
    "objectID": "posts/parameterized-decorators/parameterized-decorators.html",
    "href": "posts/parameterized-decorators/parameterized-decorators.html",
    "title": "Parameterized Decorators in Python",
    "section": "",
    "text": "A decorator is a function that takes another function and extends the behavior of the latter function without explicitly modifying it. In what follows, I demonstrate a practical use case for parameterized decorators focusing on units conversion.\nTo illustrate, we refer to the get_speed function used to estimate the spped of the International Space Station w.r.t. the surface of the Earth. get_speed returns a scalar representing the average speed of the ISS over a given time differential. The result is returned in kilometers per hour, with no parameter available to apply a change of units. To incorporate this functionality, we can either (1) add a units parameter to change the units of the calculated speed prior to get_speed returning, or (2) implement a decorator specifying the units of the calculated value fully outside of get_speed. The declarations for haverdist, getiss and get_speed are provided below:\n\n\n\nimport datetime\nimport math\nimport requests\n\n\ndef haverdist(coords1, coords2):\n    \"\"\"\n    Compute distance between geographic coordinate pairs.\n\n    Parameters\n    ----------\n    coords1: tuple or list;\n        (lat1, lon1) of first geolocation.\n        \n    coords2: tuple or list\n        (lat2, lon2) of second geolocation.\n\n    Returns\n    -------\n    float\n        Distance in kilometers between coords1 and coords2.\n    \"\"\"\n    # Convert degrees to radians then compute differences.\n    R = 6367 \n    rlat1, rlon1 = [ii * math.pi / 180 for ii in coords1]\n    rlat2, rlon2 = [ii * math.pi / 180 for ii in coords2]\n    drlat, drlon = (rlat2 - rlat1), (rlon2 - rlon1)\n    inner = (math.sin(drlat / 2.))**2 + (math.cos(rlat1)) * \\\n            (math.cos(rlat2)) * (math.sin(drlon /2.))**2\n    return 2.0 * R * math.asin(min(1., math.sqrt(inner)))\n\n\ndef getiss():\n    \"\"\"\n    Get timestamped geo-coordinates of International Space Station.\n\n    Returns\n    -------\n    dict\n        Dictionary with keys \"latitude\", \"longitude\" and \n        \"timestamp\" indicating time and position of ISS. \n    \"\"\"\n    dpos = dict()\n    resp = requests.get(\"http://api.open-notify.org/iss-now.json\").json()\n    if resp[\"message\"] != \"success\":\n        raise RuntimeError(\"Unable to access Open Notify API.\")\n    dpos[\"timestamp\"] = resp[\"timestamp\"]\n    dpos[\"latitude\"]  = float(resp[\"iss_position\"][\"latitude\"])\n    dpos[\"longitude\"] = float(resp[\"iss_position\"][\"longitude\"])\n    return dpos\n\n\n\ndef get_speed(dloc1, dloc2):\n    \"\"\"\n    Compute speed of ISS relative to Earth's surface using a pair of coordinates \n    retrieved via `getiss`. \n\n    Parameters\n    ----------\n    dloc1: dict\n        Dictionary with keys \"latitude\", \"longitude\" \"timestamp\"\n        associated with the first positional snapshot.\n    dloc2: dict\n        Dictionary with keys \"latitude\", \"longitude\" \"timestamp\"\n        associated with the second positional snapshot.\n\n    Returns\n    -------\n    float\n        Scalar value representing the average speed in km/s of the\n        International Space Station relative to the Earth in translation \n        from `dloc1` to `dloc2`. \n    \"\"\"\n    # Convert unix epochs to timestamp datetime objects.\n    ts1  = datetime.datetime.fromtimestamp(dloc1['timestamp'])\n    ts2  = datetime.datetime.fromtimestamp(dloc2['timestamp'])\n    secs = abs((ts2 - ts1).total_seconds())\n    loc1 = (dloc1[\"latitude\"], dloc1[\"longitude\"])\n    loc2 = (dloc2[\"latitude\"], dloc2[\"longitude\"])\n    dist = haverdist(loc1, loc2)\n    return (dist / secs) * 3600\n\n\nIn this case, the first option seems like a good choice. But instead of a simple function like get_speed, imagine a different function (call it legacyfunc) that we didn’t author and which has been in production for a very long time, which has lots of unfamiliar optional parameters and many more lines of code than get_speed, is responsible for returning the value and this value is the one requiring a change of units. In this case, leaving legacyfunc unmodified and wrapping its result with logic to handle the change of units would be preferable.\nWe’ll implement a function to handle the change of units conversion. This will result in a parameterized decorator, the parameter indicating which units the final result should be converted to. For this example, the only options will be kilometers per hour or miles per hour, but the decorator can be extended to facilitate any number of additional distance or time conversions.\n\n\nimport functools\n\n\ndef units(spec):\n    \"\"\"\n    Specify the units to represent orbital speed. \n\n    Parameters\n    ----------\n    spec: str {\"kph\", \"mph\"}\n        Determines the final units for representing orbital speed,\n        \"kph\" for kilometers-per-hour, \"mph\" for miles-per-hour. \n    \"\"\"\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            result_init = func(*args, **kwargs)\n            # result_init is result returned by func.\n            if spec == \"mph\":\n                result = result_init * 0.62137119 \n            else: \n                result = result_init\n            return(result)\n        return(wrapper)\n    return(decorator)\n\n\nNext, get_speed is modified by referencing the units decorator as follows:\n\n\n\n@units(\"mph\")\ndef get_speed(dloc1: dict, dloc2: dict) -&gt;float:\n    \"\"\"\n    Compute speed of ISS relative to Earth's surface using\n    a pair of coordinates retrieved from `getiss` in \n    kilometers-per-hour.\n\n    Parameters\n    ----------\n    dloc1: dict\n        Dictionary with keys \"latitude\", \"longitude\" \"timestamp\"\n        associated with the first positional snapshot.\n    dloc2: dict\n        Dictionary with keys \"latitude\", \"longitude\" \"timestamp\"\n        associated with the second positional snapshot.\n\n    Returns\n    -------\n    float\n        Scalar value representing the average speed of the International\n        Space Station relative to the Earth going from dloc1 to \n        dloc2 in kilometers-per-hour.\n    \"\"\"\n    ts1 = datetime.datetime.fromtimestamp(dloc1['timestamp'])\n    ts2 = datetime.datetime.fromtimestamp(dloc2['timestamp'])\n    secs = abs((ts2-ts1).total_seconds())\n    loc1 = (dloc1[\"latitude\"], dloc1[\"longitude\"])\n    loc2 = (dloc2[\"latitude\"], dloc2[\"longitude\"])\n    dist = haverdist(loc1, loc2)\n    return (dist / secs) * 3600\n\nWith this change, the scalar representing kilometers per hour returned by getspeed will be converted to miles per hour. This can be confirmed by calling get_speed:\n\n\nimport time\n\ndpos1 = getiss()\ntime.sleep(10)\ndpos2 = getiss()\nmph_speed = get_speed(dpos1, dpos2)\n\nprint(f\"Speed in mph: {mph_speed:,.0f}.\")\n\nSpeed in mph: 15,466.\n\n\nA speed of 15,420mph is ~25,000km/h. Wikipedia puts the ISS average orbital speed at ~27,000km/h."
  },
  {
    "objectID": "posts/pdf-harvester-python/pdf-harvester-python.html",
    "href": "posts/pdf-harvester-python/pdf-harvester-python.html",
    "title": "A PDF Harvester in 25 Lines of Python",
    "section": "",
    "text": "The goal of this article is to develop a utility that handles the following:\n\nRetrieve HTML from a webpage.\n\nParse the HTML and extract all references to embedded PDF links.\n\nFor each PDF link, download the document and save it locally.\n\nPlenty of 3rd-party libraries can query and retrieve a webpage’s links. However, the purpose of this post is to highlight the fact that by combining elements of the Python Standard Library with the Requests package, we can roll our own, and learn something while we’re at it.\n\nStep I: Acquire HTML\nThis is straightforward using requests. Let’s query the Singular Value Decomposition page on Wikipedia:\n\nimport requests\n\nurl = \"https://en.wikipedia.org/wiki/Singular_value_decomposition\"\n\n# instruct requests object to return HTML as plain text.\nhtml = requests.get(url).text\n\nhtml[:50]\n\n'&lt;!DOCTYPE html&gt;\\n&lt;html class=\"client-nojs vector-fe'\n\n\nThe HTML has been obtained. Next we’ll identify and extract references to all embedded PDF links.\n\n\nStep II: Extract PDF URLs from HTML\nA cursory review of the HTML from webpages with embedded PDF links revealed the following:\n\nValid PDF URLs will in almost always be embedded within an href tag.\n\nValid PDF URLs will in all cases be preceded by http or https.\n\nValid PDF URLs will in all cases be enclosed by a trailing &gt;.\n\nValid PDF URLs cannot contain whitespace.\n\nAfter some trial and error, the following regular expression was found to have acceptable performance for our test cases:\n\"(?=href=).*(https?://\\S+.pdf).*?&gt;\"\nAn excellent site to practice building and testing regular expressions is Pythex . The app allows you to construct regular expressions and determine how they match against the target text. I find myself using it on a regular basis.\nHere is the logic associated with steps I and II combined:\n\n\nimport re\nimport requests\n\nurl = \"https://en.wikipedia.org/wiki/Singular_value_decomposition\"\n\n# instruct requests object to return HTML as plain text.\nhtml = requests.get(url).text\n\n# Search html and compile PDF URLs in a list.\npdf_links = re.findall(r\"(?=href=).*(https?://\\S+.pdf).*?&gt;\", html)\n\nfor link in pdf_links:\n    print(link)\n\nhttp://www.wou.edu/~beavers/Talks/Willamette1106.pdf\nhttp://www.alterlab.org/research/highlights/pone.0078913_Highlight.pdf\nhttp://math.mit.edu/~edelman/publications/distribution_of_a_scaled.pdf\nhttp://files.grouplens.org/papers/webKDD00.pdf\nhttps://stanford.edu/~rezab/papers/dimsum.pdf\nhttp://faculty.missouri.edu/uhlmannj/UC-SIMAX-Final.pdf\n\n\nNote that the regular expression is prepended with an r when passed to re.findall. This instructs Python to interpret what follows as a raw string and to ignore escape sequences.\nre.findall returns a list of matches extracted from the source text. In our case, it returns a list of URLs referencing the PDF documents found on the page.\nFor the last step we need to retrieve the documents associated with our collection of links and write them to file locally. We introduce another module from the Python Standard Library, os.path, which facilitates the partitioning of absolute filepaths into components in order to retain filenames when saving documents to file.\nFor example, consider the following url:\nhttps://stanford.edu/~rezab/papers/dimsum.pdf\nTo capture dimsum.pdf, we pass the absolute URL to os.path.split, which returns a tuple of everything preceding the filename as the first element, along with the filename and extension as the second element:\n\n\nimport os\n\nurl = \"https://stanford.edu/~rezab/papers/dimsum.pdf\"\nos.path.split(url)\n\n('https://stanford.edu/~rezab/papers', 'dimsum.pdf')\n\n\nThis will be used to preserve the filename of the documents we save locally.\n\n\nStep III: Write PDFs to File\nThis step differs from the initial HTML retrieval in that we need to request the content as bytes, not text. By calling requests.get(url).content, we’re accessing the raw bytes that comprise the PDF, then writing those bytes to file. Here’s the logic for the third and final step:\n\nimport os\nimport re\nimport requests\n\nurl = \"https://en.wikipedia.org/wiki/Singular_value_decomposition\"\nhtml = requests.get(url).text\npdf_links = re.findall(r\"(?=href=).*(https?://\\S+.pdf).*?&gt;\", html)\n\n\n# Request PDF content and write to file for all entries.\nfor pdf in pdf_links:\n\n    # Get filename from url for naming file locally.\n    pdf_name = os.path.split(pdf)[1].strip()\n    \n    try:\n        r = requests.get(pdf).content\n        with open(pdf_name, \"wb\") as f: \n            f.write(r)\n    except:\n        print(f\"Unable to download {pdf_name}.\")\n    else:\n        print(f\"Saved {pdf_name}.\")\n\nSaved Willamette1106.pdf.\nSaved pone.0078913_Highlight.pdf.\nSaved distribution_of_a_scaled.pdf.\nSaved webKDD00.pdf.\nSaved dimsum.pdf.\nUnable to download UC-SIMAX-Final.pdf.\n\n\nNotice that we surround with open(pdfname, \"wb\")... in a try-except block: This handles situations that would prevent our code from downloading a document, such as broken redirects or invalid links.\nAll-in we end up with 16 lines of code excluding comments. We next present the full implementation of the PDF Harvester after a little reorganization:\n\n\nimport os.path\nimport re\nimport requests\n\n\ndef pdf_harvester(url):\n    \"\"\"\n    Retrieve URLs html and extract references to PDFs. Download PDFs, \n    writing to current working directory. \n\n    Parameters\n    ----------\n    url: str\n        Web address to serach for PDF links.\n    \"\"\"\n    html = requests.get(url).text\n    pdf_links = re.findall(r\"(?=href=).*(https?://\\S+.pdf).*?&gt;\", html)\n\n    for pdf in pdf_links:\n        \n        # Get filename from url for naming file locally.\n        pdf_name = os.path.split(pdf)[1].strip()\n\n        try:\n            r = requests.get(pdf).content\n            with open(pdf_name, \"wb\") as f: \n                f.write(r)\n        except:\n            print(f\"Unable to download {pdf_name}.\")\n        else:\n            print(f\"Saved {pdf_name}.\")"
  },
  {
    "objectID": "posts/python-rsa/python-rsa.html",
    "href": "posts/python-rsa/python-rsa.html",
    "title": "Simple Example of RSA Encryption and Decryption",
    "section": "",
    "text": "The RSA algorithm is named after Ron Rivest, Adi Shamir and Len Adleman, who invented the system in 1977. The RSA cryptosystem is the most widely-used public key cryptography algorithm in the world. It can be used to encrypt a message without requiring the exchange of a secret key. RSA exploits the fact that while multiplying large integers is trivial, determining the factors of large integers is much more difficult. The original paper introducing the RSA cryptosystem is quite readable, and is available for downloaded here.\nIn this post, we’ll walk through each step of the RSA algorithm (key generation, key distribution, encryption and decryption) with an illustrative example using small primes."
  },
  {
    "objectID": "posts/python-rsa/python-rsa.html#breaking-rsa-with-brute-force",
    "href": "posts/python-rsa/python-rsa.html#breaking-rsa-with-brute-force",
    "title": "Simple Example of RSA Encryption and Decryption",
    "section": "Breaking RSA with Brute-Force",
    "text": "Breaking RSA with Brute-Force\nRecently I came across a question posted to crypto.statsexchange inquiring about the computing resources that would be required to brute-force enumerate every possible RSA {1024, 2048,4096}-bit key-pair. Here was a response I found particularly helpful:\n\nEven if you had infinite computing power you would not have the space to store all these public/private key pairs (I’ll spare you the semi-mathematical posts comparing the space required to the number of protons in the universe). Many people have trouble perceiving just how big a number \\(2^{2048}\\) is, it’s a common error. A much more practical approach instead is to harvest as many real-life public keys as possible, and trying to find common factors between them. It actually works because of poor key generation practices."
  },
  {
    "objectID": "posts/rcpp-r/rcpp-r.html",
    "href": "posts/rcpp-r/rcpp-r.html",
    "title": "Speeding Up R with Rcpp",
    "section": "",
    "text": "R is an interpreted language, which means it is very flexible, but also has sub-optimal runtime performance when compared with compiled, statically-typed languages such as C/C++. However, we can get the best of both worlds by taking advantage of the Rcpp library, a package which facilitates the integration of executable C++ code within R programs to optimize CPU bottlenecks. In what follows, we’ll demonstrate how Rcpp can be leveraged within your programs for a fairly common task: Estimating the value of some quantity between two periods via interpolation.\nAssume we have a dataset representing estimates of the number of cumulative incurred claims at 12 months. Let’s assume the number of claims at time=0 is a known quantity, say 50. A synthetic simulated dataset of cumulative claims counts at t=0 and t=12 could be created as follows:\nlibrary(\"data.table\")\noptions(scipen=9999)\nset.seed(516)\n\nNBR_SIMS = 10000\n\nsimsDF = data.table(\n    t0=rep(50, NBR_SIMS), \n    t12=sample(seq(45, 90), size=NBR_SIMS, replace=TRUE)\n    )\nReviewing the first 10 rows of simsDF yields:\n&gt; head(simsDF, 10)\n    t0 t12\n 1: 50  56\n 2: 50  78\n 3: 50  81\n 4: 50  88\n 5: 50  45\n 6: 50  54\n 7: 50  70\n 8: 50  51\n 9: 50  66\n10: 50  79\nOur analysis requires cumulative claim count estimates for each of the intermediate months, e.g. t1, t2, …, t11. Essentially, the task boils down to converting a data.table of dimension 10,000 x 2 to one with dimensionality 10,000 x 12 via interpolation.\nA vector of the evaluation points is created (including end points, (0, 12)) along with a list of 2-element vectors for each of the 10000 simulated claim counts at t=0 and t=12:\nlibrary(\"foreach\")\n\n# Period endpoints.\nxvals = c(0, 12) \n\n# Evaluation points at which to perform interpolation.\nevalp = min(xvals):max(xvals)\n\n# List containing 10000 2-element vectors. \nyvals = mapply(                        \n    function(y0, y1) c(y0, y1), y0=simsDF[[\"t0\"]], y1=simsDF[[\"t12\"]], \n    SIMPLIFY=FALSE\n    )\nOur first implementation uses base R via the approx function, which performs interpolation between xvals and yvals at the specified points of evaluation. We record the total execution time for later comparison:\n\nt_init_0 = proc.time()\n\ninterpDF0 = foreach(\n    ii=1:length(yvals), .errorhandling=\"stop\", .combine=\"rbind\",\n    .final=function(m) as.data.table(m, keep.rownames=FALSE)\n) %do% {\n    approx(x=xvals, y=yvals[[ii]], xout=evalp)$y\n}\n\nsetnames(interpDF0, as.character(evalp))\n\nt_total_0 = (proc.time()-t_init_0)[[\"elapsed\"]]\nmessage(\"Runtime for first approach (approx): \", t_total_0, \" seconds.\")\n# Runtime for first approach (approx):  6.37 seconds.\nThe resulting interpolated data.table contains 10,000 rows by 13 columns. Columns 0 and 12 contain the same values from simsDF, and all intermediate columns represent linearly interpolated values:\n        0        1        2     3        4        5    6        7        8     9       10       11 12\n    1: 50 52.66667 55.33333 58.00 60.66667 63.33333 66.0 68.66667 71.33333 74.00 76.66667 79.33333 82\n    2: 50 49.58333 49.16667 48.75 48.33333 47.91667 47.5 47.08333 46.66667 46.25 45.83333 45.41667 45\n    3: 50 52.50000 55.00000 57.50 60.00000 62.50000 65.0 67.50000 70.00000 72.50 75.00000 77.50000 80\n    4: 50 52.16667 54.33333 56.50 58.66667 60.83333 63.0 65.16667 67.33333 69.50 71.66667 73.83333 76\n    5: 50 49.75000 49.50000 49.25 49.00000 48.75000 48.5 48.25000 48.00000 47.75 47.50000 47.25000 47\n   ---                                                                                               \n 9996: 50 53.00000 56.00000 59.00 62.00000 65.00000 68.0 71.00000 74.00000 77.00 80.00000 83.00000 86\n 9997: 50 50.00000 50.00000 50.00 50.00000 50.00000 50.0 50.00000 50.00000 50.00 50.00000 50.00000 50\n 9998: 50 52.16667 54.33333 56.50 58.66667 60.83333 63.0 65.16667 67.33333 69.50 71.66667 73.83333 76\n 9999: 50 50.16667 50.33333 50.50 50.66667 50.83333 51.0 51.16667 51.33333 51.50 51.66667 51.83333 52\n10000: 50 49.83333 49.66667 49.50 49.33333 49.16667 49.0 48.83333 48.66667 48.50 48.33333 48.16667 48\nThe first approach requires ~6.5 seconds to transform the 10000x2 data.table into a 10000x12 interpolated representation.\n\nRcpp\nRcpp is a R library which simplifies the process of extending R with compiled C++ extensions. A function or other callable is first written in C++. The C++ source file gets compiled into a shared library, which is then called from R like any other function. One of the features that makes Rcpp so powerful is that R’s native data structures are available and can be leveraged in a user defined extension module without having to manually allocate and deallocate memory. Although Rcpp has several methods by which compiled C++ functions can be loaded into R, we focus on one method, which is to write the C++ function in a separate file with a.cpp extension, then load it into R using Rcpp::sourceCpp.\nOur first attempt at optimizing the interpolation routine consisted of replacing R’s builtin approx with an equivalent C++ implementation. The assumption was that the bottleneck was due to approx, since the repeated call to approx was the only function explicitly called in the foreach construct.\nAs a review, the mathematical expression for linear interpolation is given by:\n\\[\nf(x_{0}) + \\frac{f(x_{1}) - f(x_{0})}{x_{1}- x_{0}}(x - x_{0})\n\\]\nThe C++ interpolation routine is straightforward to implement, and was identified as approxc1 to distinguish it from the builtin. I assumed it would serve as a drop-in replacement for approx, but with improved performance. What follows is the declaration for approxc1 found in approxc1.cpp:\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n\n// =============================================================================\n// approxc1.cpp                                                                |\n// =============================================================================\n// This is `approxc1`, C++-implementation of R's native approx function.       |\n// Note that x0, x1, y0 and y1 are scalars.                                    |\n// evalPts is a numeric vector which represents the points of evaluation.      |\n// Returns a vector the same length as evalPts representing the interpolated   |\n// values.                                                                     |\n// =============================================================================\n\n// [[Rcpp::export]]\nNumericVector approxc1(int x0, int x1, double y0, double y1, NumericVector evalPts) {\n    \n    int n = evalPts.size();\n    double quotient = ((y1 - y0)/(x1 - x0));\n    NumericVector vout(n);\n    \n    for(int i=0; i&lt;n; i++) {\n        vout[i] = (quotient * (evalPts[i] - x0) + y0);\n    }\n    return(vout);\n}\napproxc1 interpolates a single row at a time, returning a vector of interpolated values at the points given by evalPts. approxc1 accepts the x* and y* arguments as scalars instead of vectors, but we could have written approxc1 to have the same call signature as approx. Next we test approxc1 on the same dataset to get an idea of the performance improvement. We load approxc1.cpp into to the global environment using sourceCpp:\nlibrary(\"data.table\")\nlibrary(\"foreach\")\nlibrary(\"Rcpp\")\n\n# Build shared library containing approxc1 and load into global environment.\n# Be sure to replace \"approxc1.cpp\" with the full path to the source file. \nRcpp::sourceCpp(\"approxc1.cpp\", rebuild=TRUE)\n\n# Period endpoints.\nxvals = c(0, 12) \n\n# Evaluation points at which to perform interpolation.\nevalp = min(xvals):max(xvals)\n\n# List containing 10000 2-element vectors. \nyvals = mapply(                        \n    function(y0, y1) c(y0, y1), y0=simsDF[[\"t0\"]], y1=simsDF[[\"t12\"]], \n    SIMPLIFY=FALSE\n    )\n\nt_init_1 = proc.time()\n\n# Iterate over yvals list, interpolating at each evalp.\ninterpDF1 = foreach(\n    i=1:length(yvals), .errorhandling=\"stop\", .combine=\"rbind\",\n    .final=function(m) as.data.table(m, keep.rownames=FALSE)\n) %do% {\n    approxc1(\n        x0=xvals[1], x1=xvals[2], y0=yvals[[i]][1], y1=yvals[[i]][2],\n        evalPts=evalp\n        )\n}\n\nsetnames(interpDF1, as.character(evalp))\n\n\nt_total_1 = (proc.time()-t_init_1)[[\"elapsed\"]]\nmessage(\"Runtime for second approach (approxc1): \", t_total_1, \" seconds.\")\n# Runtime for second approach (approxc1): 5.60000000000001. seconds.\nA reduction of &lt; 1 second isn’t exactly what we had in mind. However, this second attempt wasn’t a total loss, since we learned approx wasn’t the bottleneck. In fact, taken in isolation, approx performs just as well as the C++ implementation because it is implemented in C/C++. If you type approx at R’s interactive console without arguments or parens and hit enter, the function body is printed. Near the bottom, we find the following line starting with yout:\nyout &lt;- .Call(C_Approx, x, y, xout, method, yleft, yright, \nIf approx isn’t the bottleneck, the only other construct that could adversely affect total runtime would be the iteration scheme, or in this case the foreach constructor. The next implementation removes the call to foreach, which is subsumed within the revised C++ function. Unlike approxc1, which returned a vector representing a single row of interpolated values at each evaluation point, approxc2 returns a matrix which is coerced to a data.table upon return. Another difference between approxc1 and approxc2 is that in approxc2, y0 and y1 are vectors, not scalars. The x parameter is a 2-element vector, which still represents the end-points for interpolation, but it is passed as a single entity instead of individual scalar values as in approxc1. Here is the declaration for approxc2, which we put in a file named approxc2.cpp:\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n// =============================================================================\n// approxc2.cpp                                                                |\n// =============================================================================\n// This is `approxc2`, a C++ routine that performs tabular interpolation.      |\n// Takes as input 2 vectors to interpolate between (y0, y1), a 2-element vector|\n// containing the x-values (`x`), and the points at which the interpolation    |\n// should take place (`evalPts`).                                              |\n// Returns a matrix of dimension length(y0/y1) rows by length(evalPts) columns.|\n// =============================================================================\n\n// [[Rcpp::export]]\nNumericMatrix approxc2(NumericVector x, NumericVector y0, NumericVector y1, NumericVector evalPts) {\n    // approx2c returns a matrix x.size() rows by evalPts columns\n    // containing interpolated data at each of evalPts between y0 \n    // and y1. `x` is simply an integer vector containing the \n    // bounds for interpolation. \n\n    if(y0.size()!=y1.size()) stop(\"Vectors must have the same dimensionality.\");\n\n    int x0 = x[0];\n    int x1 = x[1];\n    int nrows = y0.size();\n    int ncols = evalPts.size();\n    double quotient = 0.0;\n    NumericMatrix mat(nrows, ncols);\n\n    for(int i=0; i&lt;nrows; i++) {\n\n        quotient = ((y1[i] - y0[i])/(x1 - x0));\n\n        for(int j=0; j&lt;ncols; j++) {\n\n            mat(i, j) = (quotient * (evalPts[j] - x0) + y0[i]);\n        }\n    }\n    return(mat);\n}\nThe implementation of approxc2 is straightforward. The outer-loop iterates by row, the inner-loop by column. We specify a return type of NumericMatrix, which can be specified without having to concern ourselves with any of the details of memory management. Also note that there are more efficient approaches that can be used to populate a matrix via Rcpp (see, for example, the RcppArmadillo library), but for our purposes, approxc2 will suffice. We now test approxc2 on the same dataset used to benchmark the approx and approxc1 implementations:\nlibrary(\"data.table\")\nlibrary(\"Rcpp\")\n\n# Build shared library containing approxc1 and load into global environment.\n# Replace \"approxc2.cpp\" with the full path to the source file. \nRcpp::sourceCpp(\"approxc2.cpp\", rebuild=TRUE)\n\nt_init_2 = proc.time()\n\n# Recall that simsDF contains the values of yvals before it was split into \n# a list of 10000 2-element vectors. With approxc2, we pass the columns directly. \ninterpd2 = as.data.table(\n    approxc2(x=xvals, y0=simsDF[[1]], y1=simsDF[[2]], evalPts=evalp),\n    keep.rownames=FALSE\n    )\n\nsetnames(interpd2, as.character(evalp))\n\nt_total_2 = (proc.time()-t_init_2)[[\"elapsed\"]]\nmessage(\"Runtime for third approach (approxc2): \", t_total_2, \" seconds.\")\n# Runtime for third approach (approxc2): 0.0600000000000023 seconds.\nThat’s closer to expectations. Switching from approxc1 to approxc2 resulted in an implementation 93x faster, and 106x faster than our first implementation.\nWe should verify that interpDF0, interpDF1 and interpDF2 all contain identical values:\n&gt; all(c(all.equal(interpDF0, interpDF1), all.equal(interpDF1, interpDF2)))\n[1] TRUE\nIn conclusion, we learned that at least in some cases, computational bottlenecks in R programs aren’t necessarily due to R builtins themselves, but may instead be attributed to the iteration scheme. By moving the iteration into a compiled code extension, we were able to drastically reduce the runtime required to interpolate a table of values. This is what makes Rcpp so powerful: You get all the benefits of C++ without having to deal with the more challenging aspects of working with compiled, statically-typed programming languages."
  },
  {
    "objectID": "posts/shared-parallel-python/shared-parallel-python.html",
    "href": "posts/shared-parallel-python/shared-parallel-python.html",
    "title": "Shared Data Parallel Processing in Python",
    "section": "",
    "text": "The Python multiprocessing library exposes an interface that simplifies distributing tasks to multiple cores. The multiprocessing.Pool class provides access to a pool of worker processes to which jobs can be submitted. It supports asynchronous results with timeouts and callbacks and has a parallel map implementation. Leveraging multiprocessing.Pool is straightforward. To demonstrate, we will solve Project Euler Problem #14 in a distributed fashion. The problem states:\nThe following iterative sequence is defined for the set of positive integers:\n\nn -&gt;    n/2 (n is even)\nn -&gt; 3n + 1 (n is odd)\n\nUsing the rule above and starting with 13, we generate the following sequence:\n\n13 -&gt; 40 -&gt; 20 -&gt; 10 -&gt; 5 -&gt; 16 -&gt; 8 -&gt; 4 -&gt; 2 -&gt; 1\n\nIt can be seen that this sequence (starting at 13 and finishing at 1) contains \n10 terms. Although it has not been proved yet (Collatz Problem), it is thought \nthat all starting numbers finish at 1.\n\nWhich starting number, under one million, produces the longest chain?\n\nNOTE: Once the chain starts the terms are allowed to go above one million.\nTo start, we define two functions: collatz_test and chain_length. collatz_test contains the logic that either divides the input by 2 (if even) or multiplies it by 3 and adds 1 (if odd). chain_length returns a tuple consisting of the initial integer along with the length of the collatz chain:\ndef collatz_test(n):\n    \"\"\"\n    If n is even, return (n/2), else return (3n+1).\n    \"\"\"\n    return((n / 2) if n%2==0 else (3 * n + 1))\n\n\ndef chain_length(n):\n    \"\"\"\n    Return the length of the collatz chain along\n    with the input value n.\n    \"\"\"\n    if n &lt;= 0: \n        return(None)\n    cntr, tstint = 0, n\n    while tstint != 1:\n        cntr+=1\n        tstint = collatz_test(tstint)\n    return(n, cntr)\nOne thing to keep in mind when using the multiprocessing library is that instances of the Pool and Process classes can only be initialized after the if __name__ == \"__main__\" statement, and as a consequence Pool cannot be called from within an interactive Python session.\nNext we present our declarations from earlier along with the distributed logic, which sets up chain_length parallel dispatch:\n\"\"\"\nParallel solution to Project Euler Problem # 14.\n\"\"\"\nimport multiprocessing\n\n\ndef collatz_test(n):\n    \"\"\"\n    If n is even, return (n/2), else return (3n+1).\n    \"\"\"\n    return((n / 2) if n % 2 == 0 else (3 * n + 1))\n\n\ndef chain_length(n):\n    \"\"\"\n    Return the length of the collatz chain along\n    with the input value `n`.\n    \"\"\"\n    if n &lt;= 0: \n        return(None)\n    cntr, tstint = 0, n\n    while tstint!=1:\n        cntr+=1\n        tstint = collatz_test(tstint)\n    return(n, cntr)\n\n\nif __name__ == \"__main__\":\n\n    # Initialize array of values to test.\n    arr = multiprocessing.Array('L', range(1, 1000000))\n    pool = multiprocessing.Pool()\n    all_lengths = pool.map(chain_length, arr, chunksize=1000)\n    pool.close()\n    pool.join()\n\n    # Search for longest chain.\n    longest_chain = max((i for i in all_lengths), key=lambda x: x[1])\nWe first declare our sequence of test values as multiprocessing.Array, which prevents the same 1,000,000 element sequence from being replicated in each process (only an issue on Windows, where there is no fork system call). Instead, the array will be created once, and all processes will have access to it. The “L” typecode is from the array module in the Python Standard Library, which indicates the datatype of the elements contained in the sequence. We initialize the Pool instance, then call its map method, which works similarly to the builtin map function, only in parallel. Within pool.map, We set chunksize=1000 due to the following commentary in multiprocessing’s documentation:\n\nFor very long iterables using a large value for chunksize can make the job complete much faster than using the default value of 1.\n\nUpon execution, we find that 837,799 produces the longest sequence, and it is of length 524. By distributing the tasks to four cores, the script completes in 25 seconds, whereas the sequential implementation requires approx. 55 seconds. This disparity would only grow as the range of evaluation increases from 1M to 5M or 10M.\nFor more information on the multiprocessing module, be sure to check out the documentation. In addition, the Python Standard Library includes the concurrent.futures module, which exposes an even higher-level interface that facilitates both thread and process-based parallelism via Executor objects."
  },
  {
    "objectID": "posts/spline-smoothing-r/spline-smoothing-r.html",
    "href": "posts/spline-smoothing-r/spline-smoothing-r.html",
    "title": "Smoothing Data with Cubic Splines in R",
    "section": "",
    "text": "In this article, three approaches to smooth data will be demonstrated:\n\nstandard polynomial regression\ncubic B-spline regression\nsmoothing splines\n\nThe data will a set of loss development factors (LDFs) associated with an unidentified line of business. Instead of smoothing LDFs patterns directly, we first compute the cumulative loss development factors (CLDFs), then take the reciprocal to obtain Percent of Ultimate factors. Doing so will generally (not always) result in a monotonically increasing factor as a function of time. The code that follows prepares our data:\nlibrary(\"data.table\")\nlibrary(\"ggplot2\")\noptions(scipen=9999)\n\nldfs = c(\n    2.85637, 1.58402, 1.37531, 1.3001, 1.21469, 1.28128, 1.15415, 1.09783, 1.09302, \n    1.06395, 1.04992, 1.04659, 1.05164, 1.03117, 1.0236, 1.06338, 1.03234, 1.0172, \n    1.01795, 1.01813, 1.01413, 1.00863, 1.01346, 1.00372, 1.00423, 1.00683, 1.04633, \n    1.01796, 1.02279, 1.00629, 1.00205, 1.00316, 1.007, 1.02828, 1.00117, 1.00303, \n    1.00055, 1.02272, 1.00678, 1.00152, 1.00013, 1.01347, 1, 1.00071, 1.00136\n    )\n\n# Compute cumulative development factors.\ncldfs = rev(cumprod(rev(ldfs)))\n\n# Compute percent-of-ultimate factors. \npous = 1 / cldfs\n\nDF = data.table(\n    xinit=1:length(pous), ldf0=ldfs, cldf0=cldfs, y=pous, \n    stringsAsFactors=FALSE\n    )\n\n# Rescale `dev` to fall between 0-1.\nDF[,x:=seq(0, 1, length.out=nrow(DF))]\n\nsetcolorder(\n    DF, c(\"xinit\", \"x\", \"y\", \"ldf0\", \"cldf0\")\n    )\nFields in DF are defined as follows:\n\nxinit: Original development period. 1 &lt;= xinit &lt;= 45.\nx: xinit rescaled to [0,1].\ny: Percent-of-ultimate factors.\nldf0: Original unsmoothed loss development factors.\ncldf0: Original unsmoothed cumulative loss development factors.\n\nInspecting our data yields:\n     xinit          x          y    ldf0     cldf0\n 1:      1 0.00000000 0.03022475 2.85637 33.085464\n 2:      2 0.02272727 0.08633308 1.58402 11.583045\n 3:      3 0.04545455 0.13675333 1.37531  7.312436\n 4:      4 0.06818182 0.18807822 1.30010  5.316937\n 5:      5 0.09090909 0.24452049 1.21469  4.089637\n 6:      6 0.11363636 0.29701659 1.28128  3.366815\n 7:      7 0.13636364 0.38056142 1.15415  2.627697\n 8:      8 0.15909091 0.43922497 1.09783  2.276738\n 9:      9 0.18181818 0.48219434 1.09302  2.073853\n10:     10 0.20454545 0.52704806 1.06395  1.897360\n11:     11 0.22727273 0.56075279 1.04992  1.783317\n12:     12 0.25000000 0.58874556 1.04659  1.698527\n13:     13 0.27272727 0.61617522 1.05164  1.622915\n14:     14 0.29545455 0.64799451 1.03117  1.543223\n15:     15 0.31818182 0.66819250 1.02360  1.496575\n16:     16 0.34090909 0.68396184 1.06338  1.462070\n17:     17 0.36363636 0.72731134 1.03234  1.374927\n18:     18 0.38636364 0.75083259 1.01720  1.331855\n19:     19 0.40909091 0.76374691 1.01795  1.309334\n20:     20 0.43181818 0.77745617 1.01813  1.286246\n21:     21 0.45454545 0.79155145 1.01413  1.263342\n22:     22 0.47727273 0.80273607 1.00863  1.245739\n23:     23 0.50000000 0.80966368 1.01346  1.235081\n24:     24 0.52272727 0.82056176 1.00372  1.218677\n25:     25 0.54545455 0.82361425 1.00423  1.214161\n26:     26 0.56818182 0.82709813 1.00683  1.209046\n27:     27 0.59090909 0.83274721 1.04633  1.200845\n28:     28 0.61363636 0.87132839 1.01796  1.147673\n29:     29 0.63636364 0.88697745 1.02279  1.127424\n30:     30 0.65909091 0.90719167 1.00629  1.102303\n31:     31 0.68181818 0.91289790 1.00205  1.095413\n32:     32 0.70454545 0.91476934 1.00316  1.093172\n33:     33 0.72727273 0.91766001 1.00700  1.089728\n34:     34 0.75000000 0.92408363 1.02828  1.082153\n35:     35 0.77272727 0.95021672 1.00117  1.052392\n36:     36 0.79545455 0.95132847 1.00303  1.051162\n37:     37 0.81818182 0.95421100 1.00055  1.047986\n38:     38 0.84090909 0.95473581 1.02272  1.047410\n39:     39 0.86363636 0.97642741 1.00678  1.024142\n40:     40 0.88636364 0.98304759 1.00152  1.017245\n41:     41 0.90909091 0.98454182 1.00013  1.015701\n42:     42 0.93181818 0.98466981 1.01347  1.015569\n43:     43 0.95454545 0.99793331 1.00000  1.002071\n44:     44 0.97727273 0.99793331 1.00071  1.002071\n45:     45 1.00000000 0.99864185 1.00136  1.001360\n     xinit          x          y    ldf0     cldf0\n\nSmoothing via Polynomial Regression\nPolynomial regression is similar to standard linear regression, except the design matrix contains x raised to the desired power in each column. For example, assuming we have independent value x given by:\nx = c(2, 4, 7, 5, 2)\nInstead of regressing a response y on x alone, polynomial regression fits y using the matrix X:\n     1  2   3\n[1,] 2  4   8\n[2,] 4 16  64\n[3,] 7 49 343\n[4,] 5 25 125\n[5,] 2  4   8\nNotice each column represents x raised to the power in the column header. The first column is \\(x^{1}\\), the second \\(x^{2}\\) and the third \\(x^{3}\\). Creating the design matrix in R can be accomplished using the poly function. Next we create the design matrix X and fit a polynomial regression model of degree 3 to our data:\nX = poly(DF$x, degree=3, raw=TRUE)\ny = DF$y\n\n# Combine design matrix with target response y (pous).\nDF1 = setDT(cbind.data.frame(X, y))\n\n# Call lm function. On RHS of formula, `.` specifies all columns in DF1 are to be used. \nmdl = lm(y ~ ., data=DF1)\n\n# Bind reference to fitted values as yhat1.\nDF[,yhat1:=unname(predict(mdl))]\nA visualization overlaying polynomial regression estimates with original percent-of-ultimate factors is presented below (this code will be reused for all exhibits that follow, with inputs updated as necessary):\nexhibitTitle = paste0(\"Polynomial Regression: Percent-of-Ultimate Modeled vs. Actual\")\n\nggplot(DF) + \n    geom_point(aes(x=xinit, y=y, color=\"Actual\"),  size=2) +\n    geom_line(aes(x=xinit, y=yhat1, color=\"Predicted\"), size=1.0) + \n    guides(color=guide_legend(override.aes=list(shape=c(16, NA), linetype=c(0, 1)))) +\n    scale_color_manual(\"\", values=c(\"Actual\"=\"#758585\", \"Predicted\"=\"#E02C70\")) +\n    scale_x_continuous(breaks=seq(min(DF$xinit), max(DF$xinit), 2)) + \n    scale_y_continuous(breaks=seq(0, 1, .1)) + ggtitle(exhibitTitle) +\n    theme(\n        plot.title=element_text(size=10, color=\"#E02C70\"), \n        axis.title.x=element_blank(), axis.title.y=element_blank(), \n        axis.text.x=element_text(angle=0, vjust=0.5, size=8),\n        axis.text.y=element_text(size=8)\n        )\n\nThere is a generally good fit to actuals, but notice that for later development periods, estimates are increasing upward rather than leveling off asymptotically toward 1.0. This is one of the drawbacks of polynomial regression: The bases are non-local, meaning that the fitted value of \\(y\\) at a given value \\(x=x_{0}\\) depends strongly on data values for \\(x\\) far from \\(x_{0}\\). In modern statistical modeling applications, polynomial basis-functions are used along with new basis functions such as splines, introduced next.\n\n\nSmoothing via B-Spline Regression\nB-spline regression remedies the shortcomings of polynomial regression, namely the issue of non-locality. I’m going to demonstrate the usage of B-splines within the context of R rather than delve into the mathematical details. For an in-depth overview of B-splines, refer to Elements of Statistical Learning, specifically chapter 5.\nTo perform B-spline regression in R, the bs function is used to generate the B-spline basis matrix for a polynomial spline. The number of spline knots to use is specified, along with the degree of polynomial to use (defaults to 3). We then generate the knot locations using the range of our independent value (x) and the number of knots using the seq function. Techniques can be used to minimize a cost function, such as LOOCV to minimize average MSE in order to determine the optimal number of knots, but we omit this step and instead arbitrarily choose 5 knots for the purposes of demonstration. As a rule-of-thumb, more knots leads to higher variance and lower bias.\nlibrary(\"splines\")\n\ny = DF$y\nnbrSplineKnots = 5\n\n# Knot locations using data min/max and nbrSplineKnots. \nknotsSeq = seq(min(DF$x), max(DF$x), length.out=nbrSplineKnots)\n\n# Create basis matrix using splines::bs. degree=3 represents cubic spline.\nBbasis = bs(DF$x, knots=knotsSeq, degree=3)\n\n# Drop columns containing only a single value.\nBbasis = as.matrix(Filter(function(v) uniqueN(v)&gt;1, as.data.table(Bbasis)))\n\n# Combine design matrix with target y (pous).\nDF2 = setDT(cbind.data.frame(Bbasis, y))\n\n# Fit B-spline regression model. \nmdl2 = lm(y ~ ., data=DF2)\n\n# Bind reference to fitted values as yhat2.\nDF[,yhat2:=unname(predict(mdl2))]\nRunning the same ggplot2 code from before, replacing yhat1 with yhat2, we obtain:\n\nPolynomial regression and B-spline estimates are similar, but B-spline estimates exhibit better behavior in the later periods, with estimates far less influenced by erratic observations, while also approaching 1.0 on the right. The B-spline fit exhibits a good trade-off between bias and variance.\n\n\nSmoothing via smooth.spline\n\\(\\{x_{i},Y_{i}:i=1,\\dots ,n\\}\\), which we model by \\(Y_{i} = f(x_{i}) + \\epsilon_{i}\\), where \\(\\epsilon_{i}\\) are zero mean random errors. The cubic smoothing spline estimate \\(\\hat{f}\\) of the function \\(f\\) is defined to be the minimizer of:\n\\[\n\\sum _{i=1}^{n}[Y_{i}-{\\hat {f}}(x_{i})]^{2}+\\lambda \\int {\\hat {f}}''(x)^{2}dx,\n\\]\nwhere \\(\\lambda \\geq 0\\) is a smoothing parameter wehich controls the bias variance trade-off. With respect to the smooth.spline function, \\(\\lambda\\) is identified as spar, where 0 &lt;= spar &lt;=1. As spar approaches 1, the fit resembles linear regression (low variance / high bias). As spar approaches 0, the fit resembles interpolation (high variance / low bias). For the purposes of demonstration, we set spar=.70 and df (degrees of freedom) to the number of records in the data:\nsmoothParam = .70\nyhat3 = smooth.spline(DF$x, DF$y, df=nrow(DF), spar=smoothParam)$y\nDF[,yhat3:=yhat3]\nyhat3 vs. percent-of-ultimates:\n\nIt comes as no surprise that smooth.spline predictions are similar to B-spline estimates. To demonstrate how changing spar modifies the nature of the curve, we present the next code block, which fits the original percent-of-ultimate data using smooth.spline for 6 values of spar:\nlibrary(\"foreach\")\n\ntargetSpar = c(.01, .15, .40 , .60, .85, 1.0)\n\nDF3 = foreach(\n    i=1:length(targetSpar), .inorder=TRUE, .errorhandling=\"stop\", \n    .final=function(ll) rbindlist(ll, fill=TRUE)\n) %do% {\n    currSpar = targetSpar[[i]]\n    currDF = DF[,.(xinit, x, y)]\n    sparID = paste0(\"spar=\", round(currSpar, 2))\n    mdl = smooth.spline(currDF[,x], currDF[,y], df=nrow(currDF), spar=currSpar)\n    currDF[,`:=`(yhat=mdl$y, id=sparID, spar=currSpar)]\n}\n\nggplot(DF3) + \n    geom_point(aes(x=xinit, y=y, color=\"Actual\"),  size=1.5) +\n    geom_line(aes(x=xinit, y=yhat, color=\"Predicted\"), size=.75) + \n    scale_color_manual(\"\", values=c(\"Actual\"=\"#758585\", \"Predicted\"=\"#E02C70\")) +\n    scale_x_continuous(breaks=seq(min(DF3$xinit), max(DF3$xinit), 2)) + \n    scale_y_continuous(breaks=seq(0, 1, .1)) + \n    facet_wrap(facets=vars(id), nrow=2, scales=\"free\", shrink=FALSE) +\n    ggtitle(\"smooth.spline Regression: Percent-of-Ultimate Modeled vs. Actual\") +\n    theme(\n        plot.title=element_text(size=10, color=\"#E02C70\"), \n        axis.title.x=element_blank(), axis.title.y=element_blank(), \n        axis.text.x=element_blank(), axis.text.y=element_blank(), \n        legend.position=\"none\", panel.grid.major=element_blank(), \n        axis.ticks=element_blank()\n        )\nspar=1 is the lower-right facet (lowest variance/highest bias), and spar=.01 the upper-left facet (highest variance/lowest bias):"
  },
  {
    "objectID": "posts/sqlite-r/sqlite-r.html",
    "href": "posts/sqlite-r/sqlite-r.html",
    "title": "Working with SQLite in R",
    "section": "",
    "text": "Although RSQLite is not included with the standard R distribution, the interface is familiar and straightforward to use, especially if you have experience working with other relational database management systems from R: RSQLite is DBI-compatible, and leverages all of the familiar database functionality found in 3rd-party packages such as RMySQL, ROracle, etc.\nIn this tutorial, I’ll demonstrate how to get up and running with RSQLite. We’ll walk through creating a new SQLite database, creating and populating tables in the database, and finally how to query data from the tables we created.\n\nCreating SQLite Databases\nCreating a new SQLite database is straightforward: Specify the type of database to create, along with a filepath to which the database will be saved. One of the major benefits of SQLite is that it is a disk-based database which doesn’t require a separate server process:\n\nlibrary(\"RSQLite\")\n\ndbConn = dbConnect(RSQLite::SQLite(), \"/path/to/database/file.db\")\nThe database file extension can be either \".db\" or \".sqlite\".\nNote that I’ve explicitly referenced the library of origin using the RSQLite:: prefix when specifying SQLite(). This is generally a good practice, and removes any ambiguity regarding the library of origin for objects in the current working environment.\nIf the database is required only temporarily and it is preferable not to save the database to file, it is possible to create an in-memory database. The initialization is the same as for the persisted database, except the filepath is replaced with \":memory:\":\ndbConn = dbConnect(RSQLite::SQLite(), \":memory:\")\nTo disconnect from the database, call:\ndbDisconnect(dbConn)\nWhen opting for the in-memory database, when dbDisconnect is called, the database will be purged from memory.\n\n\nCreating Tables in SQLite\nOne of the advantages of interfacing with relational database management systems in R is that it is not necessary to explicitly create and execute the DDL associated with the table to be written. The structure of an R data.frame is such that all data types and additional table specifications can be inferred from the data.frame, and the DDL in turn is then compiled and executed on the fly. This is especially convenient for workflows that rely on a large number table creation and population routines.\nIn the next example, we load the trees dataset into an SQLite database identified as sample.db into a table named trees. Viewing the first few records of trees yields:\n  Girth Height Volume\n1   8.3     70   10.3\n2   8.6     65   10.3\n3   8.8     63   10.2\n4  10.5     72   16.4\n5  10.7     81   18.8\n6  10.8     83   19.7\nIn addition to Girth, Height and Volume, I’ll include TIMESTAMP to indicate when the table was loaded:\ndbConn = dbConnect(RSQLite::SQLite(), \"sample.db\")\nDF = trees\nDF = cbind.data.frame(DF, Timestamp=c(toString(Sys.time())))\n\n# Change fieldnames to uppercase.\nnames(DF) = toupper(names(DF))\n\n# dbWriteTable arguments: (connection, tablename, dataset)\nsuccessInd = dbWriteTable(dbConn, \"trees\", DF)\nIf the table is loaded successfully, successInd will be TRUE.\nTo list all the tables present in a particular database, run:\n&gt; dbListTables(dbConn)\n[1] \"trees\"\nTo drop/remove a table from the database, run:\ndbRemoveTable(dbConn, \"tablename\")\n\n\nQuerying SQLite Tables\nThere are two common approaches with respect to data retrieval: First, pass a valid SQL statement to dbGetQuery. The SQL statement gets passed along to the SQLite parser and the corresponding dataset is returned as an R data.frame. Second, pass the name of the table to dbReadTable, and the table will be returned in its entirety as a data.frame. The second approach may incur significant overhead for large tables (I’ll demonstrate a work-around in the next section).\nTo demonstrate dbGetQuery, we retrieve records from the trees table with HEIGHT &gt; 80:\ndbConn = dbConnect(RSQLite::SQLite(), \"sample.db\")\nSQLStr = \"SELECT * FROM trees WHERE HEIGHT&gt;80\"\ntreesDF = dbGetQuery(dbConn, SQLStr)\ndbDisconnect(dbConn)\nViewing the first few rows of treesDF yields:\n  GIRTH HEIGHT VOLUME           TIMESTAMP\n1  10.7     81   18.8 2017-09-02 21:59:26\n2  10.8     83   19.7 2017-09-02 21:59:26\n3  12.9     85   33.8 2017-09-02 21:59:26\n4  13.3     86   27.4 2017-09-02 21:59:26\n5  17.3     81   55.4 2017-09-02 21:59:26\n6  17.5     82   55.7 2017-09-02 21:59:26\n7  20.6     87   77.0 2017-09-02 21:59:26\nAlternatively, dbReadTable requires only the database connection and tablename. Assuming we haven’t removed the trees table, it can be retrieved un-filtered as follows:\ndbConn = dbConnect(RSQLite::SQLite(), \"sample.db\")\ntreesDF = dbReadTable(dbConn, \"trees\")\ndbDisconnect(dbConn)\n\n\nVariable Substitution and Dynamic Queries\nRSQLite supports parameterized queries, where a value is provided which fully specifies the SQL statement at runtime. To demonstrate, consider the SQL statement which retrieved the records from the trees table with HEIGHT &gt; 80. Instead of hard-coding 80, we can specify the threshold at runtime. This requires a slight modification to the SQL, as well as the inclusion of an additional argument in the call to dbGetQuery. In the example that follows, we demonstrate the use of two substitution parameters to filter the trees table based on HEIGHT and VOLUME:\ndbConn = dbConnect(RSQLite::SQLite(), \"sample.db\")\n\n# Update thresholds for height and volume. \nheightThresh = 80\nvolumeThresh = 30\n\nSQLStr = \"SELECT * FROM trees WHERE HEIGHT&gt;:heightThresh AND VOLUME&lt;=:volumeThresh\"\ntreesDF = dbGetQuery(dbConn, SQLStr, params=list(heightThresh=heightThresh, volumeThresh=volumeThresh))\ndbDisconnect(dbConn)\ntreesDF contains only three records:\nGIRTH HEIGHT VOLUME           TIMESTAMP\n1  10.7     81   18.8 2020-12-01 15:46:31\n2  10.8     83   19.7 2020-12-01 15:46:31\n3  13.3     86   27.4 2020-12-01 15:46:31\n\n\nIterative Retrieval of Large Datasets\nUsing dbGetTable may result in severe performance degradation when retrieving very large datasets. As an alternative, datasets can be retrieved iteratively, using a combination of dbSendQuery and dbFetch.\nThe call to dbSendQuery is identical to dbGetQuery, except dbSendQuery initializes a cursor associated with the table as opposed to retrieving the table outright (as dbGetQuery does). Think of the variable bound to the result of dbSendQuery as a pointer to the row currently being processed, and as each record is retrieved, the pointer moves to the next row, on and on until the entire result set has been traversed.\ndbFetch takes as arguments a cursor as well as a number which determines how many records to retrieve at each iteration. If n is not specified, it defaults to 500. If n is set to -1, the entire dataset will be retrieved at once, exhibiting behavior akin to dbGetQuery.\nNext we demonstrate iterative retrieval using dbSendQuery and dbFetch to query trees in groups of 5 records. Each data.frame is written to a list, then combined upon completion. Once iteration has ceased, calling dbClearResult(&lt;cursor&gt;) closes the result set:\ndbConn = dbConnect(RSQLite::SQLite(), \"sample.db\")\ndfList = list()\ncursor = dbSendQuery(dbConn, \"SELECT * FROM trees\")\n\nwhile (!dbHasCompleted(cursor)) {\n    DF = dbFetch(cursor, n=5)\n    dfList[[length(dfList)+1]] = DF\n}\n\ndbClearResult(cursor)\n\ntreesDF = do.call(\"rbind\", dfList)"
  },
  {
    "objectID": "posts/svd-image-compression/svd-image-compression.html",
    "href": "posts/svd-image-compression/svd-image-compression.html",
    "title": "Using the Singular Value Decomposition for Image Compression",
    "section": "",
    "text": "The Singular Value Decomposition (SVD) is a mathematical technique used in linear algebra to decompose a matrix into three other matrices. Specifically, for any \\(n \\times m\\) matrix \\(X\\), the SVD is a factorization of the form\n\\[\nX = U \\Sigma V^{*},\n\\]\nwhere:\n\n\\(X\\) is \\(n\\times m\\).\n\\(U\\) is \\(n \\times n\\) (unitary with orthonormal columns; columns = left signular vectors).\n\\(V\\) is \\(m \\times m\\) (unitary with orthonormal columns; columns = right signular vectors).\n\\(\\Sigma\\) is \\(n \\times m\\) with real non-negative entries along the diagonal (singular values). The singular values are the square roots of the eigenvalues of \\(A^{T}A\\) or \\(AA^{T}\\).\nWhen \\(n \\geq m\\), \\(\\Sigma\\) has at most \\(m\\) non-zero elements on the diagonal.\nRank of \\(X\\) = number of non-zero singular values.\n\nIn numpy:\n\nThe rows of \\(V^{T}\\) represent the eigenvectors of \\(X^{T}X\\).\nThe columns of \\(U\\) represent the eigenvectors of \\(XX^{T}\\).\nThe eigenvalues are \\(\\Sigma^{2}\\).\n\n\nThe SVD provides a systematic way to determine a low-dimensional approximation to high-dimensional data in terms of dominant patterns. This technique is data-driven in that patterns are discovered purely from data, without the addition of expert knowledge or intuition.\nIf \\(X\\) is self-adjoint, (\\(X = X^{*}\\)), then the singular values of \\(X\\) are equal to the absolute values of the eigenvalues of \\(X\\). In Numpy, we compute the SVD as follows:\n\n\nimport numpy as np\n\nX = np.random.rand(5, 3)\nU, S, Vt = np.linalg.svd(X, full_matrices=True)\nUhat, Shat, Vhatt = np.linalg.svd(X, full_matrices=False)\n\nprint(\"\\nfull_matrices=True:\")\nprint(f\"U.shape: {U.shape}.\")\nprint(f\"S.shape: {S.shape}.\")\nprint(f\"Vt.shape: {Vt.shape}.\")\n\nprint(\"\\nfull_matrices=False:\")\nprint(f\"Uhat.shape: {Uhat.shape}.\")\nprint(f\"Shat.shape: {Shat.shape}.\")\nprint(f\"Vhatt.shape: {Vhatt.shape}.\")\n\nprint(f\"\\nS:\\n{S}.\\n\")\nprint(f\"Shat:\\n{Shat}.\\n\")\n\n\nfull_matrices=True:\nU.shape: (5, 5).\nS.shape: (3,).\nVt.shape: (3, 3).\n\nfull_matrices=False:\nUhat.shape: (5, 3).\nShat.shape: (3,).\nVhatt.shape: (3, 3).\n\nS:\n[2.13628638 0.91901978 0.39330927].\n\nShat:\n[2.13628638 0.91901978 0.39330927].\n\n\n\n\nMatrix Approximation\nPerhaps the most useful and defining property of the SVD is that it provides an optimal low-rank approximation to a matrix \\(X\\). The Eckhart-Young theorem states that the optimal rank-\\(r\\) approximation to \\(X\\) in a least-squares sense is given by the rank-\\(r\\) SVD truncation \\(\\tilde X\\):\n\\[\n\\underset{\\tilde X, \\hspace{.25em} \\mathrm{s.t.} \\hspace{.25em}\\mathrm{rank}(\\tilde X)=r}{\\mathrm{argmin}} || X - \\tilde X||_{F} = \\tilde U \\tilde \\Sigma \\tilde V^{*},\n\\]\nwhere:\n\n\\(\\tilde U, \\tilde V\\) represent the first \\(r\\) leading columns of \\(U, V\\).\n\\(\\tilde \\Sigma\\) represents the leading \\(r \\times r\\) sub-block of \\(\\Sigma\\).\n\\(||\\cdot||_{F}\\) represents the Frobenius norm.\n\n\nBecause \\(\\Sigma\\) is diagonal, the rank-\\(r\\) SVD approximation is given by the sum of \\(r\\) distinct rank-1 matrices:\n\\[\n\\tilde X = \\sum_{k=1}^{r} \\sigma_{k} \\boldsymbol{u}_{k} \\boldsymbol{v}_{k}^{*} = \\sigma_{1} \\boldsymbol{u}_{1} \\boldsymbol{v}_{1}^{*} + \\sigma_{2} \\boldsymbol{u}_{2} \\boldsymbol{v}_{2}^{*} + \\cdots + \\sigma_{r} \\boldsymbol{u}_{r} \\boldsymbol{v}_{r}^{*}\n\\]\nThe truncated SVD basis \\(\\tilde U\\) provides a coordinate transformation from the high-dimensional original matrix into a lower dimensional representation.\nFor truncation values \\(r\\) that are smaller than the number of non-zero singular values (i.e., the rank of \\(X\\)), the truncated SVD only approximates \\(X\\):\n\\[\n\\tilde X \\approx \\tilde U \\tilde \\Sigma \\tilde V^{*}\n\\]\nIf we choose the truncation value to keep all non-zero singular values, then \\(\\tilde X = \\tilde U \\tilde \\Sigma \\tilde V^{*}\\) is exact.\n\n\n\nExample: Image Compression\nFor the next example, we use an alternate cover photo from the Allman Brothers 1971 release At the Fillmore East, shown in color and grayscale side-by-side. We’ll work with the grayscale image going forward since it limits us to two dimensions:\n\n\nfrom skimage import io\nfrom skimage.color import rgb2gray\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# 3-D RGB image. \nimgrgb = io.imread(\"fillmore.jpg\")\n\n# 2-D grayscale image.\nimg = rgb2gray(imgrgb)\n\n# Make grayscale image symmetric. \nimg = img[:800, :800]\n\nprint(f\"img.shape: {img.shape}\")\n\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 5), tight_layout=True)# figsize=(8, 4))\n\nax[0].imshow(imgrgb)\nax[0].set_title(\"original\", fontsize=9)\nax[0].set_axis_off()\n\nax[1].imshow(img, cmap=plt.cm.gray)\nax[1].set_title(\"grayscale\", fontsize=9)\nax[1].set_axis_off()\n\nplt.show()\n\n\nimg.shape: (800, 800)\n\n\n\n\n\n\n\n\n\n\nNext we generate successive rank-\\(r\\) approximations of the original image, showing the storage requirement of each rank-\\(r\\) approximation.\n\n\n# Grayscale image.\nX = img\n\n# Run SVD on grayscale image X. \nU, S, Vt = np.linalg.svd(X, full_matrices=False)\n\n# Convert signular values array to full matrix. \nS = np.diag(S) \n\n\n# Rank-r approximations to evaluate. \nranks = [1, 20, 100, 200,]# len(S)]\n\n# Matplotlib indices. \nindices = [(0, 0), (0, 1), (1, 0), (1, 1)]\n\n\n# Number of values associated with original image.\ntotal_nbr_vals = np.prod(X.shape)\n\nfig, ax = plt.subplots(2, 2, tight_layout=True, figsize=(8, 8))\n\nfor r, (ii, jj) in zip(ranks, indices):\n    \n    # Compute rank-r approximation of X.\n    Xr = U[:, :r] @ S[:r, :r] @ Vt[:r, :]\n\n    # Compute storage or rank-r approximation vs. full image.\n    rank_r_nbr_vals = np.prod(U[:, :r].shape) + r + np.prod(Vt[:r, :].shape)\n    rank_r_storage = rank_r_nbr_vals / total_nbr_vals\n\n    # Display rank-r approximation.\n    ax[ii, jj].imshow(Xr, cmap=plt.cm.gray)\n    ax[ii, jj].set_title(f\"r={r:,.0f} (storage={rank_r_storage:.2%})\", fontsize=9)\n    ax[ii, jj].set_axis_off()\n\nplt.show()\n\n\n\n\n\n\n\n\n\nA rank-100 approximation provides a decent representation of the original. At rank-200, there is virtually no difference between the original and the approximation. In practice, we could store U[:, :200], S[:200, :200] and Vt[:200, :] separately, then compute the matrix product prior to rendering the image. Doing so reduces the storage requirements by a factor of 2.\nWe can plot the magnitude of the singular values along with the cumulative proportion to assess how much variation in the original image is captured for a given rank-\\(r\\) approximation:\n\n\ns = np.diag(S)\n\nranks = ranks + [400, 800]\n\nfig, ax = plt.subplots(1, 2, tight_layout=True, figsize=(8, 4))\nax[0].semilogy(s, color=\"#000000\", linewidth=1)\nax[0].set_ylabel(r\"Singular value,  $\\sigma_{r}$\")\nax[0].set_xlabel(r\"$r$\")\nax[0].tick_params(axis=\"x\", which=\"major\", direction='in', labelsize=8)\nax[0].tick_params(axis=\"x\", which=\"minor\", direction='in', labelsize=8)\nax[0].tick_params(axis=\"y\", which=\"major\", direction='in', labelsize=8)\nax[0].tick_params(axis=\"y\", which=\"minor\", direction='in', labelsize=8)\nax[0].xaxis.set_ticks_position(\"none\")\nax[0].yaxis.set_ticks_position(\"none\")\nax[0].grid(True)\n\nax[1].plot(np.cumsum(s) / np.sum(s), color=\"#000000\", linewidth=1)\nax[1].set_ylabel(r\"cumulative sum\")\nax[1].set_xlabel(r\"$r$\")\nax[1].tick_params(axis=\"x\", which=\"major\", direction='in', labelsize=8)\nax[1].tick_params(axis=\"x\", which=\"minor\", direction='in', labelsize=8)\nax[1].tick_params(axis=\"y\", which=\"major\", direction='in', labelsize=8)\nax[1].tick_params(axis=\"y\", which=\"minor\", direction='in', labelsize=8)\nax[1].xaxis.set_ticks_position(\"none\")\nax[1].yaxis.set_ticks_position(\"none\")\nax[1].grid(True)\n\nfor r in ranks:\n    y = np.sum(s[:r]) / np.sum(s)\n    ax[1].scatter(r, y, s=50, color=\"red\")\n    ax[1].annotate(\n    r\"$r=$\" + \"{:,.0f}\".format(r), xycoords=\"data\", xy=(r, y), \n    xytext=(10, 0), textcoords=\"offset points\", ha=\"left\", va=\"center\", \n    fontsize=8, rotation=0, weight=\"normal\", color=\"#000000\", \n    )\n\nplt.show()\n\n\n\n\n\n\n\n\n\nThe rank-100 approximation accounts for ~60% of the cumulative sum of singular values. By rank-200, the approximation is closer to 80%. For completeness, we also show that a rank-800 approximation is able to recover the original image fully, since it is using all singular values and vectors (the original grayscale image was 800 x 800). The benefit of using SVD for image compression lies in its ability to prioritize and retain the most significant features of the image data, while excluding less significant features. \nNote that much of this analysis is based on Chapter 1 of Steve Brunton’s Data-Driven Science and Engineering, which is an excellent resource for practicing Data Scientists. Be sure to pickup your own copy, as the second edition was recently released."
  },
  {
    "objectID": "posts/uniroot-r/uniroot-r.html",
    "href": "posts/uniroot-r/uniroot-r.html",
    "title": "Finding Roots of Equations in R with uniroot",
    "section": "",
    "text": "Often times we encounter equations which cannot be solved using direct methods. Such systems of equations are commonly encountered within the context of maximum likelihood estimation, and in such cases, iterative methods can be used to obtain a solution.\nAssume a set of observations representing ground-up property losses in dollars:\n19999  19974  5051  7179 34416  56840  4420  6558\nOur task is to fit a Weibull distribution to the loss data in order to produce a severity curve. The Weibull density is given by:\n\\[\nf(x;\\lambda ,k) = \\frac{k}{\\lambda}\\Big(\\frac{x}{\\lambda}\\Big)^{k - 1}\\mathrm{exp}\\big[-(x/\\lambda)^k\\big],\n\\]\nwhere:\n\n\\(k\\) is the shape parameter, \\(k \\in (0, +\\infty).\\)\n\\(\\lambda\\) is the scale parameter, \\(\\lambda \\in (0, +\\infty)\\).\n\\(x \\in [0, +\\infty]\\).\n\nThe expected value of the Weibull distribution is\n\\[\nE[X] = \\lambda\\Gamma(1 + 1/k),\n\\]\nand the median is given by\n\\[\n\\mathrm{median} = F(X \\leq .50) = \\lambda(\\mathrm{Ln}(2))^{1/k}.\n\\]\nThe variance is given by\n\\[\n\\mathrm{Var}(X) = \\lambda^2\\big[\\Gamma\\big(1 + 2/k\\big) - \\big(\\Gamma(1 + 1/k)\\big)^2\\big].\n\\]\nIn \\(E[X]\\), \\(\\Gamma\\) represents the gamma function, a generalization of the factorial expressed as\n\\[\n\\Gamma{(z)} = \\int_{0}^{\\infty} x^{z-1}e^{-x}dx, \\hspace{.50em} \\mathcal{R}(z) &gt; 0.\n\\]\nThe fitdistrplus library calculates parameter estimates given data and a hypothesized distribution. The fitdist function takes an optional start parameter, which represents initial parameter values associated with the hypothesized distribution. The Weibull distribution has two parameters that require estimation: \\(k\\), the shape parameter and \\(\\lambda\\), the scale parameter. How can we come up with reasonable initial estimates of \\(k\\) and \\(\\lambda\\)?\nFirst, notice that if the mean is divided by the median, \\(\\lambda\\) cancels, leaving a function of \\(k\\) only. By setting what remains to the ratio of the empirical mean to median, the result will be an expression we can use to obtain an initial estimate of \\(k\\):\n\\[\n\\frac{E[X]}{\\mathrm{median}} = 1.421915 = \\frac{\\Gamma(1 + 1/k)}{\\mathrm{Ln}(2)^{1/k}}\n\\]\nAs a consequence of the Gamma function in the right-hand-side numerator, we cannot solve for \\(k\\) using direct methods. In R, we use uniroot to estimate roots of univariate functions numerically. In the code that follows, we implement a closure which returns a function which then can be evaluated and k, it’s sole argument, which uniroot will use to zero-in on a solution:\n# Example solving for Weibull shape parameter using uniroot.\nlossData = c(19999,  19974,  5051,  7179, 34416,  56840,  4420,  6558)\n\n# Calling shapeFunc returns a function, which can then be used by uniroot to find a solution.\nshapeFunc = function(v) {\n    # Compute ratio of empirical mean to median.\n    ratio = mean(v) / median(v)\n    function(k) {\n        return((gamma(1 + (1 / k)) / (log(2)^(1 / k))) - ratio)\n    }\n}\n\n# Evaluate shapeFunc. ff is a function which takes a single argument `k`. \nff = shapeFunc(lossData)\nThe body of shapeFunc is a straightforward implementation of our ratio expression above. The only difference is the expression is set to 0 by subtracting the ratio (1.421915) from both sides. We have our function ff and the interval over which to search for a solution \\(0 \\lt k \\leq \\mathrm{max}(\\mathrm{lossData}))\\). The call to uniroot is made below:\nshape = uniroot(ff, interval=c(.Machine$double.eps, max(lossData)))$root\nSince \\(k\\) is strictly greater than 0, we set the search interval lower bound to .Machine$double.eps, which represents the smallest positive floating-point value \\(x\\) such that \\(1 + x != 1\\). Our initial estimate for the shape parameter given our data is \\(\\hat{k} = 1.018877\\). To determine an initial estimate for the scale parameter, we can use the fact that\n\\[\n\\lambda = \\frac{E[X]}{\\Gamma(1 + 1 / \\hat{k})},\n\\]\nresulting in \\(\\hat{\\lambda} = 19454.27\\).\n\nObtaining Maximum Likelihood Estimates\nWith our hypothesized distribution and initial parameters, obtaining maximum likelihood estimates is straightforward. The initial parameter estimation code is included again for convenience:\n# Computing maximum likelihood estimates using fitdistrplus.\nlibrary(\"fitdistplus\")\n\nlossData = c(19999,  19974,  5051,  7179, 34416,  56840,  4420,  6558)\n\nshapeFunc = function(v) {\n    # Compute ratio of empirical mean to median.\n    ratio = mean(v) / median(v)\n    function(k) {\n        return((gamma(1 + (1 / k)) / (log(2)^(1 / k))) - ratio)\n    }\n}\n\n# Evaluate shapeFunc. ff is a function which takes a single argument `k`. \nff = shapeFunc(lossData)\n\n# Initial shape parameter estimate.\nshape0 = uniroot(ff, interval=c(.Machine$double.eps, max(lossData)))$root\n\n# Initial scale parameter estimate.\nscale0 = mean(lossData) / gamma(1 + (1 / shape0))\n\n# Obtain mle parameter estimates.\nmleFit = fitdistrplus::fitdist(\n    lossData, distr=\"weibull\", method=\"mle\", start=list(shape=shape0, scale=scale0)\n    )\nAccessing mleFit’s estimate attribute, parameter estimates are:\n&gt; mleFit$estimate\n        shape        scale \n    1.177033 20525.761478 \nWhich is close to our initial starting parameter estimates."
  }
]