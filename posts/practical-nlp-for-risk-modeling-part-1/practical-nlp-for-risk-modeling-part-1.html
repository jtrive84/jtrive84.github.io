<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2026-01-26">
<meta name="description" content="Using pre-trained encoder only models to classify tornado severity based on NOAA event narratives.">

<title>Practical NLP for Risk Modeling, Part I - Turning Text into Embeddings with Pretrained Transformers – The Pleasure of Finding Things Out: A blog by James Triveri</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">The Pleasure of Finding Things Out: A blog by James Triveri</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About Me</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/jtrive84/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/jamestriveri/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Practical NLP for Risk Modeling, Part I - Turning Text into Embeddings with Pretrained Transformers</h1>
                  <div>
        <div class="description">
          Using pre-trained encoder only models to classify tornado severity based on NOAA event narratives.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Python</div>
                <div class="quarto-category">Machine Learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 26, 2026</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<blockquote class="blockquote">
<p>The notebook version of this post can be downloaded <a href="https://github.com/jtrive84/jtrive84.github.io/blob/master/posts/practical-nlp-for-risk-modeling-part-1/practical-nlp-for-risk-modeling-part-1.ipynb">here</a>.</p>
</blockquote>
<p>Modern NLP workflows increasingly rely on pretrained transformer models such as BERT and its variants. These models are trained on massive text datasets to learn general purpose language representations that can be reused across a wide range of downstream tasks, including classification, information extraction, and document triage.</p>
<p>In an insurance setting, text classification problems show up frequently. Claim descriptions, underwriting notes, and risk control reports all contain unstructured text that often needs to be categorized, prioritized, or routed. Pretrained language models offer a practical way to extract meaningful signal from text without building task-specific representations from scratch.</p>
<p>When applying a pretrained model to a text classification problem, there are two common approaches: The first uses the pretrained model strictly as a feature extractor. The language model is treated as a frozen encoder that converts raw text into dense, fixed-length vector representations. These vectors are then passed into a traditional machine learning model for classification. The language model itself is never updated; it simply provides high-quality semantic features that can be plugged into an existing ML pipeline.</p>
<p>The second approach is end-to-end fine-tuning. A small classification head is attached directly to the pretrained model, and the entire network is trained jointly on a given task. During training, the model adjusts the classification layer as well as the internal language representations to better align with the target labels. This often yields higher accuracy, but comes at the cost of increased computational requirements and greater sensitivity to hyperparameters.</p>
<p>This post is the first in a four-part series that provides an end-to-end guide to applying modern transformer models for text classification in real-world data science workflows. Our focus will be on predicting the EF Scale for US tornado events based on event narratives captured in the NOAA Severe Weather Events Database, which is a publicly available resource maintained by the National Centers for Environmental Information (NCEI) that documents significant weather events across the United States. Throughout the series, we’ll work through four complementary themes:</p>
<ul>
<li><p><strong>Part I: Using Pretrained Transformers as Embedding Generators</strong> Introduces how to convert raw text into dense numeric embeddings using DistilBERT and use those embeddings as inputs to a classical machine learning model, establishing a simple baseline for text classification.</p></li>
<li><p><strong>Part II: Fine-Tuning Transformers for Domain-Specific Text Classification</strong> Walks through training a transformer end-to-end on labeled text, explaining what changes compared to feature extraction, when fine-tuning is worth the cost, and how to structure training against the NOAA tornado event dataset.</p></li>
<li><p><strong>Part III: Evaluating Compressed vs Full Text</strong> Compares model performance when using full text versus compressed representations to assess trade-offs between accuracy and operational complexity.</p></li>
<li><p><strong>Part IV: Interpretation and Calibration</strong> Focuses on making NLP models usable in production by covering probability calibration, threshold selection, error analysis, and interpretability techniques.</p></li>
</ul>
<p><br></p>
<section id="pre-processing" class="level3">
<h3 class="anchored" data-anchor-id="pre-processing"><strong>1. Pre-Processing</strong></h3>
<p>The text descriptions are sourced from the the <a href="https://www.ncei.noaa.gov/stormevents/ftp.jsp">NOAA Severe Weather Events Database</a>. It contains detailed records on hazards such as tornadoes, hail, floods, hurricanes and winter storms. Each entry provides structured data and narrative descriptions. Each file contains two columns with text descriptions of significant weather events:</p>
<ul>
<li><p><strong>EVENT_NARRATIVE</strong> describes the specific, individual event. For example, if there is a tornado in one county, the EVENT NARRATIVE contains details about that tornado (its path, damage, casualties, and circumstances).</p></li>
<li><p><strong>EPISODE_NARRATIVE</strong> describes the broader weather episode that groups multiple related events. For instance, if a line of severe thunderstorms produced several tornadoes and hail reports across multiple counties, the EPISODE_NARRATIVE provides the larger context of the storm system, while each tornado or hail report has its own EVENT NARRATIVE.</p></li>
</ul>
<p>Think of EPISODE NARRATIVE as the big picture story of the weather system and EVENT NARRATIVE as the details of each individual occurrence. We will focus on EVENT_NARRATIVE for our analysis, but a follow-up analysis will assess whether EPISODE_NARRATIVE combined with EVENT_NARRATIVE offers improvement vs.&nbsp;EVENT_NARRATIVE alone.</p>
<p>The dataset is filtered to retain events from 2008 to present and records with EVENT_TYPE = ‘Tornado’. We also drop records having TOR_F_SCALE “EFU”, “EF0”, “EF1”,“F0”, “F1”.</p>
<p>Since our goal is to train a classifier that can distinguish between low severity (EF2) and high severity (EF3, EF4 and EF5) events based on the event narrative, a <code>CLASS</code> column is created and used as the target for the downstream classifier. <code>CLASS</code> takes on the value 0 for EF2 events and 1 for EF3, EF4 and EF5 events.</p>
<p>To ensure the code provided is this series is reproducible into the future, it is strongly encouraged that a new virtual environment be created with the following dependencies:</p>
<pre><code># requirements.txt
numpy==1.26.4
pandas==2.2.1
scikit-learn==1.4.2
scipy==1.12.0
torch==2.4.1

# Hugging Face stack (Trainer-capable)
transformers==5.1.0
accelerate==1.3.0
datasets==2.18.0
evaluate==0.4.1

# Must match Transformers v5 requirements
tokenizers
huggingface-hub

# Utilities
tqdm==4.66.2
pyarrow==15.0.2
sentencepiece==0.2.0

# Additional
matplotlib==3.8.3
wordcloud==1.9.6
bs4 
requests 
contextily
geopandas</code></pre>
<p><br></p>
<p>These can be installed directly from GitHub by executing the next cell:</p>
<div id="00466ffd" class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>python <span class="op">-</span>m pip install <span class="op">-</span>r https:<span class="op">//</span>gist.githubusercontent.com<span class="op">/</span>jtrive84<span class="op">/</span>e313afbf2def24687e3c3247aa836fe9<span class="op">/</span>raw<span class="op">/</span>f2493005d291920dc1fcfd54274b9dfa42004ebe<span class="op">/</span>requirements.txt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><br></p>
<p>NOAA’s Storm Events Database can be pulled directly over HTTP, so we don’t need use a special API client. The next cell programmatically discovers the latest file for each year and reads each into a single DataFrame. We first check whether <em>“noaa-events-2008-2025.parquet”</em> exists in the current working directory. If not, the files are downloaded and saved locally. Subsequent executions of the next cell will read your local file instead of unnecessarily burdening NOAA’s file server:</p>
<div id="7155fa22" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> bs4 <span class="im">import</span> BeautifulSoup</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>base_url <span class="op">=</span> <span class="st">"https://www.ncei.noaa.gov/pub/data/swdi/stormevents/csvfiles/"</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_latest_details_file(year):</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co">    Return the filename of the latest details file for the given year.</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co">    Sample filename: StormEvents_details-ftp_v1.0_d2022_c20250721.csv.gz</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    html <span class="op">=</span> requests.get(base_url).text</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    soup <span class="op">=</span> BeautifulSoup(html, <span class="st">"html.parser"</span>)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    candidates <span class="op">=</span> [</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        a[<span class="st">"href"</span>]</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> a <span class="kw">in</span> soup.find_all(<span class="st">"a"</span>, href<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="ss">f"StormEvents_details-ftp_v1.0_d</span><span class="sc">{</span>year<span class="sc">}</span><span class="ss">"</span> <span class="kw">in</span> a[<span class="st">"href"</span>]</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">sorted</span>(candidates)[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Check if dataset exists locally prior to hitting NOAA servers.</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">not</span> os.path.exists(<span class="st">"noaa-events-2008-2025.parquet"</span>):</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Fetching latest filenames from NOAA servers..."</span>)</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get latest filenames for years 2008-2025.</span></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>    latest_filenames <span class="op">=</span> [get_latest_details_file(year) <span class="cf">for</span> year <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2008</span>, <span class="dv">2026</span>)]</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Load each file into a DataFrame and concatenate.</span></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>    dfall <span class="op">=</span> pd.concat(</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>        [pd.read_csv(<span class="ss">f"</span><span class="sc">{</span>base_url <span class="op">+</span> fname<span class="sc">}</span><span class="ss">"</span>, compression<span class="op">=</span><span class="st">"gzip"</span>) <span class="cf">for</span> fname <span class="kw">in</span> latest_filenames],</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>        ignore_index<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Loading dataset locally..."</span>)</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>    dfall <span class="op">=</span> pd.read_parquet(<span class="st">"noaa-events-2008-2025.parquet"</span>)</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total records loaded: </span><span class="sc">{</span>dfall<span class="sc">.</span>shape[<span class="dv">0</span>]<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>dfall.head()</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Fetching latest filenames from NOAA servers...
Total records loaded: 1,158,836</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="2">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">BEGIN_YEARMONTH</th>
<th data-quarto-table-cell-role="th">BEGIN_DAY</th>
<th data-quarto-table-cell-role="th">BEGIN_TIME</th>
<th data-quarto-table-cell-role="th">END_YEARMONTH</th>
<th data-quarto-table-cell-role="th">END_DAY</th>
<th data-quarto-table-cell-role="th">END_TIME</th>
<th data-quarto-table-cell-role="th">EPISODE_ID</th>
<th data-quarto-table-cell-role="th">EVENT_ID</th>
<th data-quarto-table-cell-role="th">STATE</th>
<th data-quarto-table-cell-role="th">STATE_FIPS</th>
<th data-quarto-table-cell-role="th">...</th>
<th data-quarto-table-cell-role="th">END_RANGE</th>
<th data-quarto-table-cell-role="th">END_AZIMUTH</th>
<th data-quarto-table-cell-role="th">END_LOCATION</th>
<th data-quarto-table-cell-role="th">BEGIN_LAT</th>
<th data-quarto-table-cell-role="th">BEGIN_LON</th>
<th data-quarto-table-cell-role="th">END_LAT</th>
<th data-quarto-table-cell-role="th">END_LON</th>
<th data-quarto-table-cell-role="th">EPISODE_NARRATIVE</th>
<th data-quarto-table-cell-role="th">EVENT_NARRATIVE</th>
<th data-quarto-table-cell-role="th">DATA_SOURCE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>200802</td>
<td>22</td>
<td>1300</td>
<td>200802</td>
<td>22</td>
<td>2200</td>
<td>14216</td>
<td>79884</td>
<td>NEW HAMPSHIRE</td>
<td>33</td>
<td>...</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>A noreaster moved up the coast southeast of Ca...</td>
<td>NaN</td>
<td>CSV</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>200804</td>
<td>1</td>
<td>352</td>
<td>200804</td>
<td>1</td>
<td>352</td>
<td>15549</td>
<td>88334</td>
<td>NEW HAMPSHIRE</td>
<td>33</td>
<td>...</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>Strong southwest flow behind a warm front allo...</td>
<td>An amateur radio operator recorded a wind gust...</td>
<td>CSV</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>200803</td>
<td>1</td>
<td>0</td>
<td>200803</td>
<td>1</td>
<td>1320</td>
<td>14773</td>
<td>83820</td>
<td>NEW HAMPSHIRE</td>
<td>33</td>
<td>...</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>Low pressure tracked from the Great Lakes acro...</td>
<td>NaN</td>
<td>CSV</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>200801</td>
<td>14</td>
<td>500</td>
<td>200801</td>
<td>14</td>
<td>1700</td>
<td>13559</td>
<td>75727</td>
<td>NEW HAMPSHIRE</td>
<td>33</td>
<td>...</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>Low pressure moved up the Atlantic coast and s...</td>
<td>NaN</td>
<td>CSV</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>200812</td>
<td>19</td>
<td>1353</td>
<td>200812</td>
<td>21</td>
<td>200</td>
<td>25148</td>
<td>146588</td>
<td>NEW HAMPSHIRE</td>
<td>33</td>
<td>...</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>An intensifying coastal low spread heavy snow ...</td>
<td>Six to eight inches of snow fell across easter...</td>
<td>CSV</td>
</tr>
</tbody>
</table>

<p>5 rows × 51 columns</p>
</div>
</div>
</div>
<div id="adb50ae8" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>np.set_printoptions(suppress<span class="op">=</span><span class="va">True</span>, precision<span class="op">=</span><span class="dv">5</span>, linewidth<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>pd.options.mode.chained_assignment <span class="op">=</span> <span class="va">None</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'display.max_columns'</span>, <span class="va">None</span>)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'display.width'</span>, <span class="va">None</span>)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">"display.precision"</span>, <span class="dv">5</span>)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">"ignore"</span>)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Event files by year available @ https://www.ncei.noaa.gov/stormevents/ftp.jsp.</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="co"># events_path = "C:\\Users\\jtriv\\repos\\blog-posts-in-progress\\practical-nlp-for-risk-modeling-part-2\\noaa-events-2008-2025.parquet"</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="co"># dfall = pd.read_parquet(events_path)</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter for tornadoes from 2008 onward with significant damage ratings.</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> (</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    dfall[</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>        (dfall.YEAR <span class="op">&gt;=</span> <span class="dv">2008</span>) <span class="op">&amp;</span> </span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>        (dfall.EVENT_TYPE <span class="op">==</span> <span class="st">"Tornado"</span>) <span class="op">&amp;</span> </span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        (<span class="op">~</span>dfall.TOR_F_SCALE.isin([<span class="st">"EFU"</span>, <span class="st">"EF0"</span>, <span class="st">"EF1"</span> ,<span class="st">"F0"</span>, <span class="st">"F1"</span>]))</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>    .dropna(subset<span class="op">=</span>[<span class="st">"EVENT_NARRATIVE"</span>])</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>    .drop_duplicates(subset<span class="op">=</span>[<span class="st">"EVENT_NARRATIVE"</span>])</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>    .reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Strip whitespace from EVENT_NARRATIVE.</span></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>df[<span class="st">"EVENT_NARRATIVE"</span>] <span class="op">=</span> df[<span class="st">"EVENT_NARRATIVE"</span>].<span class="bu">str</span>.replace(<span class="vs">r"\s+"</span>, <span class="st">" "</span>, regex<span class="op">=</span><span class="va">True</span>).<span class="bu">str</span>.strip()</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Create target class based on TOR_F_SCALE.</span></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>df[<span class="st">"CLASS"</span>] <span class="op">=</span> np.where(df.TOR_F_SCALE.isin([<span class="st">"EF2"</span>]), <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total records after filtering: </span><span class="sc">{</span>df<span class="sc">.</span>shape[<span class="dv">0</span>]<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Total records after filtering: 3,261</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="3">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">BEGIN_YEARMONTH</th>
<th data-quarto-table-cell-role="th">BEGIN_DAY</th>
<th data-quarto-table-cell-role="th">BEGIN_TIME</th>
<th data-quarto-table-cell-role="th">END_YEARMONTH</th>
<th data-quarto-table-cell-role="th">END_DAY</th>
<th data-quarto-table-cell-role="th">END_TIME</th>
<th data-quarto-table-cell-role="th">EPISODE_ID</th>
<th data-quarto-table-cell-role="th">EVENT_ID</th>
<th data-quarto-table-cell-role="th">STATE</th>
<th data-quarto-table-cell-role="th">STATE_FIPS</th>
<th data-quarto-table-cell-role="th">YEAR</th>
<th data-quarto-table-cell-role="th">MONTH_NAME</th>
<th data-quarto-table-cell-role="th">EVENT_TYPE</th>
<th data-quarto-table-cell-role="th">CZ_TYPE</th>
<th data-quarto-table-cell-role="th">CZ_FIPS</th>
<th data-quarto-table-cell-role="th">CZ_NAME</th>
<th data-quarto-table-cell-role="th">WFO</th>
<th data-quarto-table-cell-role="th">BEGIN_DATE_TIME</th>
<th data-quarto-table-cell-role="th">CZ_TIMEZONE</th>
<th data-quarto-table-cell-role="th">END_DATE_TIME</th>
<th data-quarto-table-cell-role="th">INJURIES_DIRECT</th>
<th data-quarto-table-cell-role="th">INJURIES_INDIRECT</th>
<th data-quarto-table-cell-role="th">DEATHS_DIRECT</th>
<th data-quarto-table-cell-role="th">DEATHS_INDIRECT</th>
<th data-quarto-table-cell-role="th">DAMAGE_PROPERTY</th>
<th data-quarto-table-cell-role="th">DAMAGE_CROPS</th>
<th data-quarto-table-cell-role="th">SOURCE</th>
<th data-quarto-table-cell-role="th">MAGNITUDE</th>
<th data-quarto-table-cell-role="th">MAGNITUDE_TYPE</th>
<th data-quarto-table-cell-role="th">FLOOD_CAUSE</th>
<th data-quarto-table-cell-role="th">CATEGORY</th>
<th data-quarto-table-cell-role="th">TOR_F_SCALE</th>
<th data-quarto-table-cell-role="th">TOR_LENGTH</th>
<th data-quarto-table-cell-role="th">TOR_WIDTH</th>
<th data-quarto-table-cell-role="th">TOR_OTHER_WFO</th>
<th data-quarto-table-cell-role="th">TOR_OTHER_CZ_STATE</th>
<th data-quarto-table-cell-role="th">TOR_OTHER_CZ_FIPS</th>
<th data-quarto-table-cell-role="th">TOR_OTHER_CZ_NAME</th>
<th data-quarto-table-cell-role="th">BEGIN_RANGE</th>
<th data-quarto-table-cell-role="th">BEGIN_AZIMUTH</th>
<th data-quarto-table-cell-role="th">BEGIN_LOCATION</th>
<th data-quarto-table-cell-role="th">END_RANGE</th>
<th data-quarto-table-cell-role="th">END_AZIMUTH</th>
<th data-quarto-table-cell-role="th">END_LOCATION</th>
<th data-quarto-table-cell-role="th">BEGIN_LAT</th>
<th data-quarto-table-cell-role="th">BEGIN_LON</th>
<th data-quarto-table-cell-role="th">END_LAT</th>
<th data-quarto-table-cell-role="th">END_LON</th>
<th data-quarto-table-cell-role="th">EPISODE_NARRATIVE</th>
<th data-quarto-table-cell-role="th">EVENT_NARRATIVE</th>
<th data-quarto-table-cell-role="th">DATA_SOURCE</th>
<th data-quarto-table-cell-role="th">CLASS</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>200805</td>
<td>1</td>
<td>1759</td>
<td>200805</td>
<td>1</td>
<td>1816</td>
<td>18091</td>
<td>105611</td>
<td>IOWA</td>
<td>19</td>
<td>2008</td>
<td>May</td>
<td>Tornado</td>
<td>C</td>
<td>167</td>
<td>SIOUX</td>
<td>FSD</td>
<td>01-MAY-08 17:59:00</td>
<td>CST-6</td>
<td>01-MAY-08 18:16:00</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0.50M</td>
<td>0.00K</td>
<td>NWS Storm Survey</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>EF2</td>
<td>8.89</td>
<td>400.0</td>
<td>FSD</td>
<td>IA</td>
<td>119.0</td>
<td>LYON</td>
<td>4.0</td>
<td>S</td>
<td>ROCK VLY</td>
<td>6.0</td>
<td>NW</td>
<td>ROCK VLY</td>
<td>43.1421</td>
<td>-96.3000</td>
<td>43.2573</td>
<td>-96.3787</td>
<td>Thunderstorms produced 5 confirmed tornadoes o...</td>
<td>A tornado damaged numerous trees, including la...</td>
<td>CSV</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>200802</td>
<td>6</td>
<td>21</td>
<td>200802</td>
<td>6</td>
<td>25</td>
<td>13933</td>
<td>80215</td>
<td>KENTUCKY</td>
<td>21</td>
<td>2008</td>
<td>February</td>
<td>Tornado</td>
<td>C</td>
<td>93</td>
<td>HARDIN</td>
<td>LMK</td>
<td>06-FEB-08 00:21:00</td>
<td>EST-5</td>
<td>06-FEB-08 00:25:00</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3.00M</td>
<td>0.00K</td>
<td>NWS Storm Survey</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>EF2</td>
<td>7.10</td>
<td>400.0</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>1.0</td>
<td>SE</td>
<td>FRANKLIN XRDS</td>
<td>1.0</td>
<td>NE</td>
<td>MARTIN BOX</td>
<td>37.6625</td>
<td>-86.0125</td>
<td>37.6807</td>
<td>-85.8848</td>
<td>A cold front along with a strong upper level l...</td>
<td>The tornado destroyed a trailer and an outbuil...</td>
<td>CSV</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>200802</td>
<td>6</td>
<td>27</td>
<td>200802</td>
<td>6</td>
<td>29</td>
<td>13933</td>
<td>80216</td>
<td>KENTUCKY</td>
<td>21</td>
<td>2008</td>
<td>February</td>
<td>Tornado</td>
<td>C</td>
<td>93</td>
<td>HARDIN</td>
<td>LMK</td>
<td>06-FEB-08 00:27:00</td>
<td>EST-5</td>
<td>06-FEB-08 00:29:00</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>150.00K</td>
<td>0.00K</td>
<td>NWS Storm Survey</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>EF2</td>
<td>1.02</td>
<td>300.0</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>4.0</td>
<td>SE</td>
<td>TUNNEL HILL</td>
<td>4.0</td>
<td>ESE</td>
<td>TUNNEL HILL</td>
<td>37.6965</td>
<td>-85.7759</td>
<td>37.7037</td>
<td>-85.7596</td>
<td>A cold front along with a strong upper level l...</td>
<td>The tornado knocked a trailer off its foundati...</td>
<td>CSV</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>200802</td>
<td>6</td>
<td>106</td>
<td>200802</td>
<td>6</td>
<td>108</td>
<td>13933</td>
<td>80218</td>
<td>KENTUCKY</td>
<td>21</td>
<td>2008</td>
<td>February</td>
<td>Tornado</td>
<td>C</td>
<td>229</td>
<td>WASHINGTON</td>
<td>LMK</td>
<td>06-FEB-08 01:06:00</td>
<td>EST-5</td>
<td>06-FEB-08 01:08:00</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>15.00K</td>
<td>0.00K</td>
<td>NWS Storm Survey</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>EF2</td>
<td>0.99</td>
<td>250.0</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>2.0</td>
<td>WSW</td>
<td>DEEP CREEK</td>
<td>1.0</td>
<td>WSW</td>
<td>DEEP CREEK</td>
<td>37.6889</td>
<td>-85.0649</td>
<td>37.6937</td>
<td>-85.0478</td>
<td>A cold front along with a strong upper level l...</td>
<td>This tornado touched down on Russell Lane abou...</td>
<td>CSV</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>200802</td>
<td>6</td>
<td>50</td>
<td>200802</td>
<td>6</td>
<td>52</td>
<td>13933</td>
<td>79065</td>
<td>KENTUCKY</td>
<td>21</td>
<td>2008</td>
<td>February</td>
<td>Tornado</td>
<td>C</td>
<td>179</td>
<td>NELSON</td>
<td>LMK</td>
<td>06-FEB-08 00:50:00</td>
<td>EST-5</td>
<td>06-FEB-08 00:52:00</td>
<td>2</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>250.00K</td>
<td>0.00K</td>
<td>NWS Storm Survey</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>EF2</td>
<td>0.76</td>
<td>300.0</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>1.0</td>
<td>SW</td>
<td>WOODLAWN</td>
<td>0.0</td>
<td>WNW</td>
<td>WOODLAWN</td>
<td>37.8113</td>
<td>-85.3812</td>
<td>37.8215</td>
<td>-85.3763</td>
<td>A cold front along with a strong upper level l...</td>
<td>The tornado destroyed or heavily damaage two s...</td>
<td>CSV</td>
<td>0</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p><br></p>
<p>Let’s get an idea of the distribution of events by <code>TOR_F_SCALE</code> as well as <code>CLASS</code>:</p>
<div id="6e79bc5d" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> display_html</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Distribution over TOR_F_SCALE.</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>tbl1a <span class="op">=</span> df.TOR_F_SCALE.value_counts().reset_index(drop<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>tbl1b <span class="op">=</span> df.TOR_F_SCALE.value_counts(normalize<span class="op">=</span><span class="va">True</span>).reset_index(drop<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>tbl1 <span class="op">=</span> tbl1a.merge(tbl1b, on<span class="op">=</span><span class="st">"TOR_F_SCALE"</span>, how<span class="op">=</span><span class="st">"left"</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Distribution over class.</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>tbl2a <span class="op">=</span> df.CLASS.value_counts().reset_index(drop<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>tbl2b <span class="op">=</span> df.CLASS.value_counts(normalize<span class="op">=</span><span class="va">True</span>).reset_index(drop<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>tbl2 <span class="op">=</span> tbl2a.merge(tbl2b, on<span class="op">=</span><span class="st">"CLASS"</span>, how<span class="op">=</span><span class="st">"left"</span>)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>tbl1_style <span class="op">=</span> tbl1.style.set_table_attributes(<span class="st">"style='display:inline'"</span>).set_caption(<span class="st">'TOR_F_SCALE'</span>).hide(axis<span class="op">=</span><span class="st">'index'</span>)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>tbl2_style <span class="op">=</span> tbl2.style.set_table_attributes(<span class="st">"style='display:inline'"</span>).set_caption(<span class="st">'CLASS'</span>).hide(axis<span class="op">=</span><span class="st">'index'</span>)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>display_html(tbl1_style._repr_html_()<span class="op">+</span>tbl2_style._repr_html_(), raw<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<style type="text/css">
</style>

<div id="T_a1a89" class="quarto-float quarto-figure quarto-figure-center anchored" data-quarto-postprocess="true" style="display:inline">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="T_a1a89-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: TOR_F_SCALE
</figcaption>
<div aria-describedby="T_a1a89-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table id="T_a1a89" class="caption-top table table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th id="T_a1a89_level0_col0" class="col_heading level0 col0" data-quarto-table-cell-role="th">TOR_F_SCALE</th>
<th id="T_a1a89_level0_col1" class="col_heading level0 col1" data-quarto-table-cell-role="th">count</th>
<th id="T_a1a89_level0_col2" class="col_heading level0 col2" data-quarto-table-cell-role="th">proportion</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td id="T_a1a89_row0_col0" class="data row0 col0">EF2</td>
<td id="T_a1a89_row0_col1" class="data row0 col1">2460</td>
<td id="T_a1a89_row0_col2" class="data row0 col2">0.754370</td>
</tr>
<tr class="even">
<td id="T_a1a89_row1_col0" class="data row1 col0">EF3</td>
<td id="T_a1a89_row1_col1" class="data row1 col1">656</td>
<td id="T_a1a89_row1_col2" class="data row1 col2">0.201165</td>
</tr>
<tr class="odd">
<td id="T_a1a89_row2_col0" class="data row2 col0">EF4</td>
<td id="T_a1a89_row2_col1" class="data row2 col1">134</td>
<td id="T_a1a89_row2_col2" class="data row2 col2">0.041092</td>
</tr>
<tr class="even">
<td id="T_a1a89_row3_col0" class="data row3 col0">EF5</td>
<td id="T_a1a89_row3_col1" class="data row3 col1">11</td>
<td id="T_a1a89_row3_col2" class="data row3 col2">0.003373</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>

<style type="text/css">
</style>

<div id="T_889c3" class="quarto-float quarto-figure quarto-figure-center anchored" data-quarto-postprocess="true" style="display:inline">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="T_889c3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: CLASS
</figcaption>
<div aria-describedby="T_889c3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table id="T_889c3" class="caption-top table table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th id="T_889c3_level0_col0" class="col_heading level0 col0" data-quarto-table-cell-role="th">CLASS</th>
<th id="T_889c3_level0_col1" class="col_heading level0 col1" data-quarto-table-cell-role="th">count</th>
<th id="T_889c3_level0_col2" class="col_heading level0 col2" data-quarto-table-cell-role="th">proportion</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td id="T_889c3_row0_col0" class="data row0 col0">0</td>
<td id="T_889c3_row0_col1" class="data row0 col1">2460</td>
<td id="T_889c3_row0_col2" class="data row0 col2">0.754370</td>
</tr>
<tr class="even">
<td id="T_889c3_row1_col0" class="data row1 col0">1</td>
<td id="T_889c3_row1_col1" class="data row1 col1">801</td>
<td id="T_889c3_row1_col2" class="data row1 col2">0.245630</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</div>
</div>
<p><br></p>
<p>~75% of events fall in CLASS = 0, and ~25% in CLASS = 1. The dataset is mildly imbalanced, but a 75/25 split is common in real-world problems and usually workable with standard classifiers. Most algorithms can still learn meaningful decision boundaries without special tricks.</p>
<p><code>DAMAGE_PROPERTY</code> captures the estimated dollar value of property damage associated with each event. This includes losses to homes, buildings, infrastructure, vehicles, and other physical assets. While we will not use <code>DAMAGE_PROPERTY</code> as a feature in our classifier in Part I, it is still informative to understand how average property damage varies across EF classes. Because this field is stored in a non-numeric format within the NOAA database, we first convert it to a numeric type and then compute average damage by <code>TOR_F_SCALE</code>:</p>
<div id="6691b17d" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> convert_to_numeric(damage_str):</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Convert damage string with K, M, B suffix to numeric value.</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> pd.isna(damage_str):</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">0</span>  </span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    damage_str <span class="op">=</span> <span class="bu">str</span>(damage_str).upper()</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">"K"</span> <span class="kw">in</span> damage_str:</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">float</span>(damage_str.replace(<span class="st">"K"</span>, <span class="st">""</span>)) <span class="op">*</span> <span class="dv">1000</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> <span class="st">"M"</span> <span class="kw">in</span> damage_str:</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">float</span>(damage_str.replace(<span class="st">"M"</span>, <span class="st">""</span>)) <span class="op">*</span> <span class="dv">1000000</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> <span class="st">"B"</span> <span class="kw">in</span> damage_str: </span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">float</span>(damage_str.replace(<span class="st">"B"</span>, <span class="st">""</span>)) <span class="op">*</span> <span class="dv">1000000000</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply function to each row of dft. </span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>df[<span class="st">"DAMAGE_PROPERTY_NUMERIC"</span>] <span class="op">=</span> df[<span class="st">"DAMAGE_PROPERTY"</span>].<span class="bu">apply</span>(convert_to_numeric).fillna(<span class="dv">0</span>)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute average property damage by TOR_F_SCALE.</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>avg_damage <span class="op">=</span> df.groupby(<span class="st">"TOR_F_SCALE"</span>)[<span class="st">"DAMAGE_PROPERTY_NUMERIC"</span>].mean()</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Average Property Damage by TOR_F_SCALE:"</span>)</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>avg_damage <span class="op">=</span> avg_damage.<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="ss">f"$</span><span class="sc">{</span>x<span class="sc">:,.2f}</span><span class="ss">"</span>).reset_index(drop<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>avg_damage</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Average Property Damage by TOR_F_SCALE:</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="5">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">TOR_F_SCALE</th>
<th data-quarto-table-cell-role="th">DAMAGE_PROPERTY_NUMERIC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>EF2</td>
<td>$1,990,912.94</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>EF3</td>
<td>$15,629,913.11</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>EF4</td>
<td>$55,035,537.31</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>EF5</td>
<td>$466,145,454.55</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p><br></p>
<p>This aligns with our expectations. EF classes are based on wind speed, and the damage from EF5 events will be far greater on average than the other classes:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 6%">
<col style="width: 18%">
<col style="width: 74%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: right;">EF Scale</th>
<th style="text-align: right;">Wind Speed Range (mph)</th>
<th>Damage Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">EF0</td>
<td style="text-align: right;">65–85</td>
<td>Light damage (tree branches broken, shallow-rooted trees pushed over)</td>
</tr>
<tr class="even">
<td style="text-align: right;">EF1</td>
<td style="text-align: right;">86–110</td>
<td>Moderate damage (roofs stripped, mobile homes overturned)</td>
</tr>
<tr class="odd">
<td style="text-align: right;">EF2</td>
<td style="text-align: right;">111–135</td>
<td>Considerable damage (roofs torn off well-constructed houses, large trees snapped)</td>
</tr>
<tr class="even">
<td style="text-align: right;">EF3</td>
<td style="text-align: right;">136–165</td>
<td>Severe damage (entire stories of houses destroyed, significant structural damage)</td>
</tr>
<tr class="odd">
<td style="text-align: right;">EF4</td>
<td style="text-align: right;">166–200</td>
<td>Devastating damage (well-built homes leveled, cars thrown)</td>
</tr>
<tr class="even">
<td style="text-align: right;">EF5</td>
<td style="text-align: right;">&gt;200</td>
<td>Incredible damage (strong-frame houses swept away, large debris carried long distances)</td>
</tr>
</tbody>
</table>
<p><br></p>
<p>A word cloud can be used to quickly visualize the most frequent or prominent words in a body of text, with more important words appearing larger. Its purpose is to give an at-a-glance sense of dominant themes in the text.</p>
<p>Two separate word clouds are created based on <code>EVENT_NARRATIVE</code> (one for CLASS = 0 the other for CLASS = 1). Output is limited to the top 50 words:</p>
<div id="419be014" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> wordcloud <span class="im">import</span> WordCloud</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Limit to top 50 words for word clouds.</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>num_words <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>text0 <span class="op">=</span> df.loc[df[<span class="st">"CLASS"</span>]<span class="op">==</span><span class="dv">0</span>, <span class="st">"EVENT_NARRATIVE"</span>].<span class="bu">str</span>.cat(sep<span class="op">=</span><span class="st">" "</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>text1 <span class="op">=</span> df.loc[df[<span class="st">"CLASS"</span>]<span class="op">==</span><span class="dv">1</span>, <span class="st">"EVENT_NARRATIVE"</span>].<span class="bu">str</span>.cat(sep<span class="op">=</span><span class="st">" "</span>)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>wc0 <span class="op">=</span> WordCloud(max_words<span class="op">=</span>num_words, background_color<span class="op">=</span><span class="st">"white"</span>).generate(text0)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>wc1 <span class="op">=</span> WordCloud(max_words<span class="op">=</span>num_words, background_color<span class="op">=</span><span class="st">"white"</span>).generate(text1)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">12</span>), dpi<span class="op">=</span><span class="dv">150</span>, tight_layout<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">"CLASS=0 (EF2)"</span>, fontsize<span class="op">=</span><span class="dv">10</span>, weight<span class="op">=</span><span class="st">"bold"</span>, color<span class="op">=</span><span class="st">"red"</span>)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].imshow(wc0)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].axis(<span class="st">"off"</span>)</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">"CLASS=1 (EF3, EF4, EF5)"</span>, fontsize<span class="op">=</span><span class="dv">10</span>, weight<span class="op">=</span><span class="st">"bold"</span>, color<span class="op">=</span><span class="st">"red"</span>)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].imshow(wc1)</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].axis(<span class="st">"off"</span>)</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="practical-nlp-for-risk-modeling-part-1_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p><br></p>
<p>At first glance, the CLASS=0 and CLASS=1 word clouds do not appear meaningfully different.</p>
<p><br></p>
</section>
<section id="using-distilbert-as-a-feature-extractor" class="level3">
<h3 class="anchored" data-anchor-id="using-distilbert-as-a-feature-extractor"><strong>2. Using DistilBERT as a Feature Extractor</strong></h3>
<p>Hugging Face is an open-source AI platform best known for making modern NLP and foundation models easy to use and share. It provides pre-trained models, standardized model APIs, and datasets so you don’t have to build everything from scratch. Training is more convenient because common tasks—tokenization, batching, fine-tuning, and evaluation—are handled by well-designed libraries with sensible defaults.</p>
<p>The Python transformers library is Hugging Face’s core toolkit for working with models like BERT, GPT, T5, and vision transformers. It offers simple APIs to load pre-trained models, tokenize data and fine-tune on your own datasets. In this section, we carry out the following steps:</p>
<ol start="0" type="1">
<li>Create Dataset object from event narratives.</li>
<li>Tokenize event narratives with DistilBERT tokenizer.<br>
</li>
<li>Run frozen DistilBERT forward pass.</li>
<li>Pool token embeddings into one vector per event.</li>
</ol>
<p>Instead of creating a random train-test split, events are split based on time. The training set will include all events from 2008-2022, the test set 2022-present. We do this because language evolves over time and reporting practices change. It can also be viewed as a way to assess how well the model generalizes to future event descriptions.</p>
<p>In the next cell train and validation Dataset objects are created from the original DataFrame. We create the <code>DatasetDict</code> instance manually: Had we opted for a random split, this would have been created automatically when calling <code>.train_test_split</code> on the original DataFrame, i.e., <code>Dataset.from_pandas(df).train_test_split(test_size=0.20)</code>. Dataset objects integrate tightly with the Transformers training APIs. They represent data in a standard format that supports lazy loading and fast preprocessing.</p>
<div id="1bd55875" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> Dataset, DatasetDict</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>keep_columns <span class="op">=</span> [</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"EVENT_ID"</span>,</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"EVENT_NARRATIVE"</span>,</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"TOR_F_SCALE"</span>,</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"CLASS"</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Create train-test splits. </span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>dftrain <span class="op">=</span> df[df[<span class="st">"YEAR"</span>] <span class="op">&lt;=</span> <span class="dv">2022</span>][keep_columns].reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>dfvalid <span class="op">=</span> df[df[<span class="st">"YEAR"</span>] <span class="op">&gt;</span>  <span class="dv">2022</span>][keep_columns].reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Create Dataset objects.</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>ds_train <span class="op">=</span> Dataset.from_pandas(dftrain)</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>ds_valid <span class="op">=</span> Dataset.from_pandas(dfvalid)</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>ds <span class="op">=</span> DatasetDict({<span class="st">"train"</span>: ds_train, <span class="st">"valid"</span>: ds_valid})</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Inspect first training sample.</span></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>ds[<span class="st">"train"</span>][<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>{'EVENT_ID': 105611,
 'EVENT_NARRATIVE': 'A tornado damaged numerous trees, including large trees uprooted, blew windows out of a home, destroyed a metal shed, blew two windows and part of a wall out of a metal building, damaged at least three grain bins, destroyed or damaged numerous outbuildings and small sheds, blew down or snapped off at least 15 power poles, bent a metal light pole, tipped one wagon and blew the top off another, blew down a barb wire fence and pushed fence posts almost to the ground, destroyed a hog barn, and flattened corn stubble, before crossing the county line into Lyon County. Contents inside several damaged or destroyed buildings and sheds were also damaged, especially on one farm where damaged buildings housed a farm and trucking business.',
 'TOR_F_SCALE': 'EF2',
 'CLASS': 0}</code></pre>
</div>
</div>
<p><br></p>
<p>Inspecting the output of the first training sample, we see it is nothing more than a dictionary representation of the first row in <code>dftrain</code> with column names as keys and values set to the original column values.</p>
<p>The next step it to tokenize event narratives with the DistilBERT tokenizer. <a href="https://huggingface.co/docs/transformers/en/model_doc/distilbert">DistilBERT</a> is a smaller, faster, and lighter version of BERT created using knowledge distillation. Essentially a smaller student model learns to mimic a larger teacher BERT model. It retains most of BERT’s language understanding performance while using fewer parameters and requiring less compute.</p>
<p>There are two variants we can choose from:</p>
<ul>
<li><strong>distilbert-base-uncased</strong>: Does not make a difference between english and English.</li>
<li><strong>distilbert-base-cased</strong>: Does make a difference between english and English.</li>
</ul>
<p>For the remainder of the post, the focus will be on distilbert-base-uncased, but it is very easy to swap and assess different models when working with the transformers library, in many cases it is as simple as updating the model name.</p>
<div id="1698dd32" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModel</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Model name.</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"distilbert-base-uncased"</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize tokenizer and model.</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_name)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModel.from_pretrained(model_name)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Run model on GPU if available.</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Send model to device. </span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>model.to(device)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Set model to evaluation mode.</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Print number of parameters in model.</span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>total_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters())</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total parameters in </span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>total_params<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Tokenizer vocab_size: </span><span class="sc">{</span>tokenizer<span class="sc">.</span>vocab_size<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Tokenizer model_max_length (maximum context size): </span><span class="sc">{</span>tokenizer<span class="sc">.</span>model_max_length<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.
Loading weights: 100%|██████████| 100/100 [00:00&lt;00:00, 979.85it/s, Materializing param=transformer.layer.5.sa_layer_norm.weight]    
DistilBertModel LOAD REPORT from: distilbert-base-uncased
Key                     | Status     |  | 
------------------------+------------+--+-
vocab_layer_norm.weight | UNEXPECTED |  | 
vocab_transform.bias    | UNEXPECTED |  | 
vocab_transform.weight  | UNEXPECTED |  | 
vocab_layer_norm.bias   | UNEXPECTED |  | 
vocab_projector.bias    | UNEXPECTED |  | 

Notes:
- UNEXPECTED    :can be ignored when loading from different task/architecture; not ok if you expect identical arch.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Total parameters in distilbert-base-uncased: 66,362,880
Tokenizer vocab_size: 30,522
Tokenizer model_max_length (maximum context size): 512</code></pre>
</div>
</div>
<p><br></p>
<p>Tokenization bridges the gap between free-form narrative text and the numeric tensors required by transformer models. Transformers do not operate on strings. Instead, they consume token IDs which are integers representing sub-word units from a learned vocabulary. DistilBERT uses a subword tokenizer, meaning words can be split into smaller pieces. This allows the model to handle rare words, misspellings, and morphological variations while keeping the vocabulary size manageable.</p>
<p>The tokenizer also produces an attention mask indicating which positions correspond to real tokens versus padding. This is critical when batching variable-length narratives, because the model needs to ignore padded positions when computing internal representations.</p>
<p>distilbert-base-uncased has a vocabulary of size 30,522 and a maximum context size of 512. The maximum context size means there can be at most 512 tokens (subword pieces, not characters or words) in a single input sequence. If an event narrative is longer than this, it must be truncated or split into chunks before being passed to the model.</p>
<p>The next cell provides an example of how we go from raw text to an encoded sequence of embedding IDs.</p>
<div id="07fac5f9" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Stanley Brothers, "The Fields have Turned Brown."</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"The price I have paid, to live and to learn"</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Encoded text. </span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>enc_input <span class="op">=</span> tokenizer(text, return_tensors<span class="op">=</span><span class="st">'pt'</span>)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"enc_input.input_ids.shape : </span><span class="sc">{</span>enc_input<span class="sc">.</span>input_ids<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)  <span class="co"># (batch_size, sequence_length)</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"enc_input:</span><span class="ch">\n</span><span class="sc">{</span>enc_input<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>      </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>enc_input.input_ids.shape : torch.Size([1, 13])
enc_input:
{'input_ids': tensor([[ 101, 1996, 3976, 1045, 2031, 3825, 1010, 2000, 2444, 1998, 2000, 4553,
          102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}</code></pre>
</div>
</div>
<div id="96db7903" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(enc_input[<span class="st">"input_ids"</span>][<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>13</code></pre>
</div>
</div>
<p><code>enc_input</code> is a dict-like object with <code>input_ids</code> representing the embedding ids, and <code>attention_mask</code> a tensor containing 1s for real tokens and 0s for padding tokens added to reach the desired sequence length.</p>
<p>The complete mapping of words/word parts to tokens can be obtained via <code>tokenizer.get_vocab()</code>. For example, we can check which id “learn” maps to:</p>
<div id="fa24b16c" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> tokenizer.get_vocab()</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"id for learn: </span><span class="sc">{</span>vocab[<span class="st">'learn'</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>id for learn: 4553</code></pre>
</div>
</div>
<p>Referring back to <code>enc_input</code>, we see that 4553 is the id at index 11.</p>
<p>To obtain features from the encoded input, <code>enc_input</code> is passed into <code>model</code>:</p>
<div id="de27986a" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtain text features. </span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> model(<span class="op">**</span>enc_input)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"type(output) : </span><span class="sc">{</span><span class="bu">type</span>(output)<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"output.last_hidden_state.shape : </span><span class="sc">{</span>output<span class="sc">.</span>last_hidden_state<span class="sc">.</span>size()<span class="sc">}</span><span class="ss">"</span>)  <span class="co"># (batch_size, sequence_length, hidden_size)</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">output.last_hidden_state:</span><span class="ch">\n\n</span><span class="sc">{</span>output<span class="sc">.</span>last_hidden_state<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>type(output) : &lt;class 'transformers.modeling_outputs.BaseModelOutput'&gt;

output.last_hidden_state.shape : torch.Size([1, 13, 768])

output.last_hidden_state:

tensor([[[-0.0182,  0.0976, -0.0949,  ...,  0.0370,  0.2696,  0.1983],
         [ 0.2047, -0.0677, -0.3011,  ...,  0.0800,  0.4613, -0.0775],
         [ 0.7276, -0.3357, -0.0353,  ..., -0.0306,  0.2783, -0.2500],
         ...,
         [ 0.4852,  0.1291, -0.0054,  ...,  0.0494,  0.4393,  0.2179],
         [ 0.0979,  0.3544, -0.0155,  ..., -0.0864,  0.0971, -0.4329],
         [ 1.0055,  0.4603, -0.2136,  ..., -0.0508, -0.5924, -0.4584]]],
       grad_fn=&lt;NativeLayerNormBackward0&gt;)</code></pre>
</div>
</div>
<p><br></p>
<p>In Hugging Face’s BERT models, <code>last_hidden_state</code> is the sequence of contextualized embeddings for every token in the input after the final transformer layer. They represent features that will be used to train our classifier. For our Stanley Brothers excerpt, this has dimension (1, 13, 768) = <code>(batch_size, sequence_length, hidden_size)</code>:</p>
<ul>
<li><code>batch_size</code>: The number of samples passed into the model.</li>
<li><code>sequence_length</code>: The number of tokens used to represent <code>text</code>.</li>
<li><code>hidden_size</code>: The dimensionality of each token’s final embedding after all transformer layers. Every sub-word token is represented as a 768-dimensional vector.</li>
</ul>
<p>We wrap <code>tokenizer</code> inside a small function rather than calling it inline to ensure that truncation, padding, and maximum length are applied identically across training, validation, and inference. The same function can be reused later when scoring new narratives, which helps to avoid mismatches between training and prediction pipelines:</p>
<div id="94b3794b" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>MAX_LEN <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize(texts):</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokenizer(</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>        texts,</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>        truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>        padding<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>        max_length<span class="op">=</span>MAX_LEN,</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>        return_tensors<span class="op">=</span><span class="st">"pt"</span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>When we pass tokenized text through DistilBERT, the model does not return a single vector for the entire narrative. Instead, as we saw with our excerpt from <em>The Fields Have Turned Brown</em>, a vector is returned for each token in the sequence (the dimension of the output was 1 x 13 x 768, not 1 x 768). Conceptually, every token receives its own 768-dimensional embedding that reflects the token itself and surrounding context.</p>
<p>Classical machine learning models expect a fixed-length feature vector per observation. This creates a mismatch: we have many token-level vectors for a single narrative, but we need exactly one vector representing the entire event.</p>
<p>Mean pooling address this issue by averaging the token embeddings across the sequence, resulting in a single 768-dimensional vector per narrative (the desired 1 x 768 representation). The attention mask is used so that padded tokens do not contribute to the average. The result can be interpreted as a global summary of the narrative’s semantic content.</p>
<p>Pooling is a necessary step whenever we use a transformer as a frozen feature extractor. Without pooling, there is no straightforward way to pass transformer outputs into a traditional classifier.</p>
<p>This is handled in <code>mean_pool</code> below:</p>
<div id="7eea4452" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="at">@torch.no_grad</span>()</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mean_pool(last_hidden_state, attention_mask):</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> attention_mask.unsqueeze(<span class="op">-</span><span class="dv">1</span>).<span class="bu">float</span>()</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>    summed <span class="op">=</span> (last_hidden_state <span class="op">*</span> mask).<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>    counts <span class="op">=</span> mask.<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>).clamp(<span class="bu">min</span><span class="op">=</span><span class="fl">1e-9</span>)</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> summed <span class="op">/</span> counts</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><br></p>
<p>At this stage, we combine everything built so far into a single operation that converts raw narrative text into numeric feature vectors. The <code>embed_texts</code> function in the next cell takes a list of narratives, tokenizes them, passes them through DistilBERT, and applies mean pooling to produce one fixed-length embedding per event.</p>
<p>Transformers operate on batches of tokenized text and return high-dimensional tensors, not directly usable feature matrices. Classical machine learning models expect a two-dimensional array where each row corresponds to an observation and each column corresponds to a feature. The embedding step bridges that gap by turning free-form text into a matrix of shape (number_of_events, 768).</p>
<p>Once this step is complete, the narratives will have been transformed into numerical features that can be handled in exactly the same way as any other tabular predictor. This is what makes the overall approach so practical: after embedding, the rest of the workflow is indistinguishable from a standard classification problem.</p>
<div id="08a8cc31" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="at">@torch.no_grad</span>()</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> embed_texts(texts, batch_size<span class="op">=</span><span class="dv">32</span>):</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    all_vecs <span class="op">=</span> []</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(texts), batch_size):</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>        batch <span class="op">=</span> texts[i:(i <span class="op">+</span> batch_size)]</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>        enc <span class="op">=</span> tokenize(batch)</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>        enc <span class="op">=</span> {k: v.to(device) <span class="cf">for</span> k, v <span class="kw">in</span> enc.items()}</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> model(<span class="op">**</span>enc)</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>        emb <span class="op">=</span> mean_pool(out.last_hidden_state, enc[<span class="st">"attention_mask"</span>])</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>        all_vecs.append(emb.cpu().numpy())</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.vstack(all_vecs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><code>embed_texts</code> is then used to generate features from the training and validation sets. The next cell may take 5-10 minutes to run if using CPU:</p>
<div id="91d959e7" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> embed_texts(ds[<span class="st">"train"</span>][<span class="st">"EVENT_NARRATIVE"</span>], batch_size<span class="op">=</span><span class="dv">32</span>)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>X_valid <span class="op">=</span> embed_texts(ds[<span class="st">"valid"</span>][<span class="st">"EVENT_NARRATIVE"</span>], batch_size<span class="op">=</span><span class="dv">32</span>)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> np.array(ds[<span class="st">"train"</span>][<span class="st">"CLASS"</span>])</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>y_valid <span class="op">=</span> np.array(ds[<span class="st">"valid"</span>][<span class="st">"CLASS"</span>])</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"X_train shape: </span><span class="sc">{</span>X_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">; y_train.shape: </span><span class="sc">{</span>y_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"X_valid shape: </span><span class="sc">{</span>X_valid<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">; y_valid.shape: </span><span class="sc">{</span>y_valid<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>X_train shape: (2620, 768); y_train.shape: (2620,)
X_valid shape: (641, 768); y_valid.shape: (641,)</code></pre>
</div>
</div>
<p><br></p>
</section>
<section id="classification" class="level3">
<h3 class="anchored" data-anchor-id="classification"><strong>3. Classification</strong></h3>
<p>At this point, we can use all the familiar scikit-learn tooling to fit a classifier, because the problem has been fully reduced to standard tabular modeling. Each tornado event is now represented by a fixed-length numeric vector, and the target is a simple binary label indicating EF2 versus EF3+.</p>
<p>When working with transformer embeddings, simple linear models often perform surprisingly well, since much of the complexity has already been absorbed by the language model. Logistic Regression is a natural first choice in this setting. It is fast to train, handles high-dimensional feature spaces well, and provides well-behaved probability estimates. The embeddings produced by DistilBERT have 768 dimensions, which makes the problem inherently high-dimensional. Linear models with regularization are well-suited to this regime, because regularization helps control overfitting by shrinking less informative coefficients toward zero.</p>
<p>Nothing about this step is specific to text anymore. We could swap Logistic Regression for a Random Forest, Gradient Boosting model, or any other scikit-learn classifier without changing the upstream pipeline. This separation between representation learning and downstream modeling is one of the major advantages of using pretrained transformers as feature extractors.</p>
<p>In the next cell, we fit a Logistic Regression model with Elastic Net penalty. A grid search is performed over <code>l1_ratio</code> and <code>C</code> to identify the regularization regime which maximizes f1-score.</p>
<div id="ebf35da0" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> uniform, loguniform</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> RandomizedSearchCV</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>random_state <span class="op">=</span> <span class="dv">516</span></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>verbosity <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>n_iter <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>scoring <span class="op">=</span> <span class="st">"recall"</span></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>cv <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> {</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"l1_ratio"</span>: uniform(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">1</span>), </span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"C"</span>: loguniform(<span class="fl">1e-4</span>, <span class="fl">1e4</span>)</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> RandomizedSearchCV(</span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>    LogisticRegression(solver<span class="op">=</span><span class="st">"saga"</span>, class_weight<span class="op">=</span><span class="st">"balanced"</span>, max_iter<span class="op">=</span><span class="dv">500</span>), </span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>    param_grid, </span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>    scoring<span class="op">=</span>scoring, </span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span>cv, </span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span>verbosity, </span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span>random_state, </span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a>    n_iter<span class="op">=</span>n_iter,</span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a>clf.fit(X_train, y_train)</span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-30"><a href="#cb29-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Optimal parameters: </span><span class="sc">{</span>clf<span class="sc">.</span>best_params_<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Fitting 3 folds for each of 50 candidates, totalling 150 fits

Optimal parameters: {'C': 0.10443596935926203, 'l1_ratio': 0.4360994501308848}</code></pre>
</div>
</div>
<p><br></p>
<p>Assess model performance on holdout data:</p>
<div id="f1a986c5" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_valid, clf.predict(X_valid), digits<span class="op">=</span><span class="dv">5</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              precision    recall  f1-score   support

           0    0.89447   0.71919   0.79731       495
           1    0.42798   0.71233   0.53470       146

    accuracy                        0.71763       641
   macro avg    0.66123   0.71576   0.66601       641
weighted avg    0.78822   0.71763   0.73750       641
</code></pre>
</div>
</div>
<p>This is a very solid result, given that we used a frozen encoder and text-only features. In particular, what matters most for our use case is CLASS=1 recall = 0.71, which means roughly 7 out of every 10 truly severe tornadoes are being correctly identified. Precision = 0.43, so only about half of the tornadoes flagged as EF3+ truly are EF3+. This is the expected tradeoff: recall increased, and we paid for it with more false positives.</p>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion"><strong>Conclusion</strong></h3>
<p>The big-picture interpretation is that the pipeline works and the model has learned meaningful signal from the text on a real-world dataset. It’s worth emphasizing that the goal of this post was not to squeeze out the last basis point of classification performance, but to build intuition around how a pretrained transformer like DistilBERT can be used as a general-purpose feature extractor for downstream modeling. Even with a relatively simple Logistic Regression head, we were able to convert unstructured event narratives into dense numerical representations with only a few lines of code and obtain better than expected results. There are many obvious avenues for improvement:</p>
<ul>
<li>Experiment with alternative pooling strategies.</li>
<li>Trying different pretrained backbones.</li>
<li>Moving beyond linear models to more expressive classifiers.</li>
<li>Incorporating additional features (property damage, spatial variables, etc.)</li>
</ul>
<p>These are left as exercises for the reader.</p>
<p>In Part 2, we’ll take the next step and fine-tune the transformer itself for the same task, allowing the language model to adapt its internal representations directly to the classification objective, and demonstrate why end-to-end fine-tuning often delivers significant performance improvement over fixed embeddings.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/www\.jtrive\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>