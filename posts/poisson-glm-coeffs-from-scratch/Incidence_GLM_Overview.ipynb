{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Count Data with GLMs\n",
    "\n",
    "\n",
    "A walk through fitting a Generalized Linear Model to data with a count response, and demonstrate that a custom implemented solution generates  identical estimates to those by the primary statistical modeling library in Python statsmodels [statsmodels](https://www.statsmodels.org/dev/index.html).       \n",
    "\n",
    "\n",
    "In practice, we do not know the values of the proposed model's parameters, but we do know the data. We use the likelihood function to observe how the function changes for different parameter values while holding the data fixed. \n",
    "We can make use of this to judge which values of the parameters lead to greater relative chances for the sample to occur. Larger values of the likelihood correspond to values of the parameters that are relatively better supported by the data[2].\n",
    "In short, *the underlying goal of a likelihood is to determine which parameters make the given data most likely*. \n",
    "   \n",
    "The joint density of $n$ independently distributed observations $\\mathbf{y} = (y_{1}, \\cdots, y_{n})^{T}$ is given by:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{y}|\\mathbf{\\beta}) = \\prod_{i=1}^{n} f_{i}(y_{i}|\\mathbf{\\beta})\n",
    "$$\n",
    "\n",
    "When this expression is interpreted as a function of unknown $\\beta$ given known data $y$, we obtain the likelihood function:\n",
    "\n",
    "$$\n",
    "L(\\mathbf{\\beta}|\\mathbf{y}) = \\prod_{i=1}^{n} f_{i}(y_{i}|\\mathbf{\\beta})\n",
    "$$\n",
    "\n",
    "Solving the likelihood equation can be difficult. This can be partially alleviated by logging the likelihood expression, arriving at an expression for the log-likelihood:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{\\beta}|\\mathbf{y}) = \\sum_{i=1}^{n} f_{i}(y_{i}|\\mathbf{\\beta})\n",
    "$$\n",
    "             \n",
    "<br>   \n",
    "\n",
    "### Poisson Estimating Equations\n",
    "\n",
    "\n",
    "A linear model can be fit by solving closed form equations. In the case of the Gaussian, we can maximize the log-likelihood and find an analytical solution directly. Unfortunately, that cannot be done with most Generalized Linear Models including Poisson regression. Instead, an iterative approach such as Iterative Reweighted Least Squares is used. The model is fit based on one-term Taylor linearization of the log-likelihood function identified as the working response. Given an initial estimate of the parameters $\\hat{\\beta}$, we calculate the estimated linear predictor $Ln(\\hat{\\mu}_{i}) = \\hat{\\eta}_{i} = x_{i}^{T}\\beta$, which is then used to obtain the fitted values $\\hat{\\mu}_{i} = g^{-1}(\\hat{\\eta}_{i}) = exp(\\hat{\\eta}_{i})$. This process continues until the change in deviance between iterations falls below a predefined threshold. \n",
    "<br>   \n",
    "\n",
    "Recall that the Poisson probability density function with mean $\\mu$ is defined as:\n",
    "\n",
    "$$\n",
    "f(y) = \\frac{\\mu^{y} e^{-\\mu}}{y!}\n",
    "$$\n",
    "<br>      \n",
    "\n",
    "For a dataset with $n$ observations assumed to follow a Poisson distribution, with each observation having mean parameter $\\mu_{i}$, the likelihood is given by\n",
    "\n",
    "$$ \n",
    "L = \\prod_{i=1}^{n} \\frac{\\mu_{i}^{y}e^{-\\mu_{i}}}{y_{i}!},\n",
    "$$\n",
    "\n",
    "and similarly the log-likelihood as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\sum_{i=1}^{n} y_{i} Ln(\\mu_{i}) - \\mu_{i} - Ln(y_{i}!)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Every GLM has an associated [link function](https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function), which specifies the relationship between the linear predictor and the mean of the distribution function. Each member of the exponential family (Gaussian, Binomial, Poisson, Gamma, Inverse Gaussian, Negative Binomial) has an associated canonical link, which, for the Poisson GLM is the *log link* (in GLM literature, the linear predictor is commonly represented by $\\eta_{i}$ for models with canonical link):\n",
    "\n",
    "$$\n",
    "Ln(\\mu_{i}) = \\eta_{i} = x_{i}^{T}\\beta\n",
    "$$\n",
    "\n",
    "<br>\n",
    "  \n",
    "Then the *inverse link* is the transformation required to obtain the mean. For Poisson model, this is the exponential function:   \n",
    "\n",
    "$$\n",
    "\\mu_{i} = e^{\\eta_{i}} = e^{x_{i}^{T}\\beta}\n",
    "$$\n",
    "\n",
    "We can substitute this expression for $\\mu_{i}$ into the Poisson log-likelihood equation, resulting in:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\sum_{i=1}^{n} \\big(y_{i} (x_{i}^{T}\\beta)  - exp(x_{i}^{T}\\beta) - Ln(y_{i}!)\\big)\n",
    "$$\n",
    "\n",
    "\n",
    "The gradient vector, or *score* is the first derivative of the log-likelihood function with respect to $\\beta$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\beta} = \\sum_{i=1}^{n} \\big((y_{i} - exp(x_{i}^{T}\\beta))x_{i}\\big)\n",
    "$$\n",
    "<br>\n",
    "\n",
    "The Hessian matrix is calculated as the second derivative of the log-likelihood function and is negative definite for $\\beta$. It can be expressed as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^{2} \\mathcal{L}}{\\partial \\beta \\partial \\beta^{T}} = -\\sum_{i=1}^{n} \\big(exp(x_{i}^{T}\\beta)\\big)x_{i}x_{j}^{T}\n",
    "$$\n",
    "\n",
    "\n",
    "Estimation of the maximum likelihood covariance matrix is based on the negative inverse of the Hessian, $\\Sigma$: \n",
    "\n",
    "$$\n",
    "\\sum = -H^{-1} = \\Bigg[\\sum_{i=1}^{n} \\big(exp(x_{i}^{T}\\beta)\\big)x_{i}x_{j}^{T} \\Bigg]^{-1}\n",
    "$$\n",
    "<br>\n",
    "\n",
    "This is obtained from the last iteration of the estimation procedure.         \n",
    "The square roots of the diagonal elements of $\\Sigma$ are the values of parameter standard errors. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symbol Map\n",
    "\n",
    "- $n$: Number of dataset records    \n",
    "<br>\n",
    "- $p$: Number of model parameters excluding intercept ($p + 1$ *including* intercept)     \n",
    "<br>    \n",
    "- $X$: Model matrix  \n",
    "<br>\n",
    "- $y$: Dataset response     \n",
    "<br>       \n",
    "- $L = \\prod_{i=1}^{n} \\frac{\\mu_{i}^{y}e^{-\\mu_{i}}}{y_{i}!},$: Poisson likelihood                     \n",
    "<br>                 \n",
    "- $\\mathcal{L} = \\sum_{i=1}^{n} \\big(y_{i} (x_{i}^{T}\\beta)  - exp(x_{i}^{T}\\beta) - Ln(y_{i}!)\\big)$: Poisson log-likelihood         \n",
    "<br>           \n",
    "- $\\beta = (\\beta_{0}, \\cdots, \\beta_{p})^{T}$: Estimated model parameters     \n",
    "<br>          \n",
    "- $g(\\mu_{i})$: Link function. For Poisson GLM, $g(\\mu_{i}) = Ln(\\mu_{i})$.              \n",
    "<br>         \n",
    "- $g'(\\mu_{i}) = 1/\\mu_{i}$          \n",
    "<br>          \n",
    "- $\\eta = \\mathbf{X\\beta}$: Model linear predictor. Same as $g(\\mu_{i})$.    \n",
    "<br>             \n",
    "- $\\mu_{i} = g^{-1}(\\eta_{i}) = g^{-1}(x_{i}^{T}\\beta)$: Inverse link or mean function. For Poisson GLM, $\\mu_{i} = exp(\\eta_{i}) = exp(x_{i}^{T}\\beta)$)     \n",
    "<br>        \n",
    "- $\\frac{\\partial \\mathcal{L}}{\\partial \\beta} = U$: Gradient vector or score        \n",
    "<br>      \n",
    "- $\\frac{\\partial^{2} \\mathcal{L}}{\\partial \\beta \\partial \\beta^{T}}$: Hessian matrix       \n",
    "<br>     \n",
    "- $\\mathcal{I} =  -E\\Big[\\frac{\\partial^{2} \\mathcal{L}}{\\partial \\beta \\partial \\beta^{T}} \\Big] = X^{T}WX$: Expected information matrix. For GLMs with canonical link, observed and expected information are the same.     \n",
    "<br>         \n",
    "- $w_{ii} = \\frac{1}{var(Y_{i})}\\big(\\frac{\\partial \\mu_{i}}{\\partial \\eta_{i}}\\big)^{2}$        \n",
    "<br>        \n",
    "- $W$: $n$-by-$n$ diagonal matrix of weights with $i^{th}$ element equal to $\\mu_{i}$         \n",
    "<br>           \n",
    "- $\\sum = -H^{-1} = \\mathcal{I}^{-1}$: Variance-covariance matrix             \n",
    "<br>   \n",
    "- $\\beta_{r+1} = \\beta_{r} - H^{-1}U$: Newton-Raphson parameter update step     \n",
    "<br>\n",
    "- $\\beta_{r+1} = [X^{T}WX]^{-1}X^{T}Wz$, where $z = X\\beta_{r} + W^{-1}(y - \\mu)$: IRLS update expression      \n",
    "<br>       \n",
    "- $z_{i} = \\hat{\\eta}_{i} + (y_{i} - \\hat{\\mu}_{i})g'(\\mu)$: Working response, a one-term linearization of the log-likelihood function    \n",
    "<br>          \n",
    "- $\\sum_{i=1}^{n}\\big(y_{i} - exp(x_{i}^{T}\\beta)\\big)x_{i}^{T} = 0$: Poisson estimating equation         \n",
    "<br>      \n",
    "- The likelihood of the saturated model is 1.\n",
    "<br>\n",
    "- The log-likelihhod of the saturated model is 0. \n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "Fisher scoring has the advantage that it produces the asymptotic covariance matrix as a by-product.\n",
    "\n",
    "The log mean is the natural parameter for the Poisson distribution, and the log link is the canonical link for a Poisson GLM. \n",
    "A 1-unit increase in $x_{j}$ has a multiplicative impact of $e^{\\beta_{j}}$. The mean at $x_{j} + 1$ equals the mean at $x_{j}$ multiplied by $e^{\\beta_{j}}$.   \n",
    "<br>\n",
    "Linear regression models are found by minimizing the sum of squared residuals. Poisson regression models are found by minimizing the sum of squared residual deviances, which is equivalent to maximizing the loglikelihood of the data given the model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Assessment\n",
    "\n",
    "**Deviance**: Measures how well the model fits the data. Is -2 * loglikelihood of the dataset given the model.       \n",
    "**Deviance Residuals**: Array of -2 * loglikelihood for each data point.  \n",
    "\n",
    "Null Deviance = 2(LL(Saturated Model) - LL(Null Model)) on df = df_Sat - df_Null\n",
    "\n",
    "Residual Deviance = 2(LL(Saturated Model) - LL(Proposed Model)) df = df_Sat - df_Proposed\n",
    "\n",
    "The Saturated Model is a model that assumes each data point has its own parameters (which means there are $n$ parameters to estimate.)\n",
    "\n",
    "The Null Model assumes the exact \"opposite\", in that is assumes one parameter for all of the data points, which means the model only estimates one parameter.\n",
    "\n",
    "The Proposed Model assumes you can explain your data points with $p$ parameters + an intercept term, so you have $p+1$ parameters.\n",
    "\n",
    "If your Null Deviance is really small, it means that the Null Model explains the data pretty well. Likewise with your Residual Deviance. The null deviance shows how well the response is predicted by the model with nothing but an intercept.\n",
    "\n",
    "What does really small mean? If your model is \"good\" then your Deviance is approx Chi^2 with (df_sat - df_model) degrees of freedom.\n",
    "\n",
    "If you want to compare you Null model with your Proposed model, then you can look at\n",
    "\n",
    "(Null Deviance - Residual Deviance) approx Chi^2 with df Proposed - df Null = (n-(p+1))-(n-1)=p\n",
    "\n",
    "\n",
    "- degrees of freedom = no. of observations â€“ no. of predictors    \n",
    "<br>     \n",
    "- Deviance of saturated model is 0, loglikelihood of saturated model will be the maximal attained value.       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "The difference between the null deviance and the model's deviance is distributed as a chi-squared with degrees of freedom equal to the null df minus the model's df. For your model, that would be:\n",
    "\n",
    "1-pchisq(256600 - 237230, df=(671266 - 671263))\n",
    "\n",
    "\n",
    "By default, pchisq() gives the proportion of the distribution to the left of the value. To get the proportion more extreme than your difference, you can specify lower.tail = FALSE or subtract the result from 1 (as you and I have done).\n",
    "\n",
    "\n",
    "Q-What hypothesis exactly are you testing with the statement 1-pchisq(256600 - 237230, df=(671266 - 671263))?\n",
    "\n",
    "\n",
    "you are checking if the deviance changed more than might be expected by chance. Ie, you are testing if the model as a whole is better than the null model. It is analogous to the global F test in a linear model.\n",
    "\n",
    "\n",
    "Q-The null hypothesis is 'the model as a whole is better than the null model', and you have rejected the null hypothesis, which means the model is poor?    \n",
    "\n",
    "no the null hypothesis is: **the model as a whole is no better than the null model**. Since this has been rejected, we conclude that the data are not consistent with the null model. NB, this does not necessarily mean that our model is 'good' or 'correct'\n",
    "\n",
    "Deviance is the likelihood ratio statistic comparing the saturated model to the model under consideration:\n",
    "\n",
    "$$\n",
    "D = 2\\big(\\mathcal{L}(\\beta_{max}) - \\mathcal{L}(\\beta)\\big)\n",
    "$$\n",
    "\n",
    "$D$ is approx chi-square with $m-p$ degrees of freedom, with expected value $m-p$. A value much greater than $m-p$ indicates a poor fitting model. \n",
    "\n",
    "\n",
    "Log-likelihood ratio statistic:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "LRT&=2\\big(\\mathcal{L}(\\beta_{unconstrained}) - \\mathcal{L}(\\beta_{constrained})\\big) \\\\\n",
    "   &=D_{constrained} - D_{unconstrained} \\sim\\chi^{2}_{q}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where $q$ represents the number of parameters in the unconstrained model not present in the constrained model. \n",
    "\n",
    "\n",
    "LRT Null hypothesis: $H_{0}$: Parameters in unconstrained model equal 0. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We can compare the loglikelihoods of the models and select the model with the highest. But it would not be right to compare unadjusted loglikelihoods, because loglikelihood cannot decrease, and usually increases with the addition of variables, no matter how irrelevant they may be. We must charge a penalty for adding variables. \n",
    "\n",
    "**AIC Akaike Information Criterion**\n",
    "\n",
    "$$\n",
    "AIC = -2\\mathcal{L} + 2q\n",
    "$$\n",
    "\n",
    "Where $q$ is the number of model parameters. \n",
    "\n",
    "The lower the AIC, the better the model. \n",
    "\n",
    "\n",
    "**Bayesian Information Criterion**\n",
    "\n",
    "$$\n",
    "BIC = -2\\mathcal{L} + qLn(n)\n",
    "$$\n",
    "\n",
    "Where $n$ represents the number of observations, not the number of groups. \n",
    "\n",
    "In summary, for penalized loglikelihood tests, the improvement in loglikelihood for a model with $q$ additional parameters must be at least $2q$ for AIC or $qLn(n)$ for BIC. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IRLS Algorithm\n",
    "\n",
    "\n",
    "$X$ is the n-by-p design matrix                        \n",
    "$y$ is a n-by-1 vector representing the response            \n",
    "$\\text{offset}$ is a n-by-1 vector representing the exposure              \n",
    "$\\mu$ is a n-by-1 vector representing the mean of each observation      \n",
    "$\\eta$ is a n-by-1 vector representing the linear predictor/component, $\\eta = Ln(\\mu)$        \n",
    "$W$ is a n-by-n diagonal matrix with each value equal to $\\mu$ (weight matrix)     \n",
    "$z$ is a n-by-1 vector representing the working response               \n",
    "$\\epsilon$ represents the change in deviance below which iteration will terminate      \n",
    "<br>     \n",
    "\n",
    "\n",
    "####  Pseudocode\n",
    "\n",
    "\n",
    "- $\\text{deviance} = 0$      \n",
    "- $\\mu = \\text{mean(y)}$    \n",
    "- $\\eta = Ln(\\mu)$    \n",
    "- $epsilon = .0001$   \n",
    "- $\\Delta \\text{deviance}$ = $\\mathrm{Inf}$\n",
    "<br>    \n",
    "\n",
    "\n",
    "\n",
    "**WHILE** $|\\Delta \\text{deviance}| \\gt \\epsilon$:\n",
    "\n",
    "$\\qquad W$ = diag($\\mu$)\n",
    "<br>    \n",
    "$\\qquad z$ = $\\eta + \\frac{y - \\mu}{\\mu} - \\text{offset}$\n",
    "<br>    \n",
    "$\\qquad \\beta = (X^{T}WX)^{-1}X^{T}Wz$   \n",
    "<br>\n",
    "$\\qquad \\eta = X\\beta + \\text{offset}$      \n",
    "<br>\n",
    "$\\qquad \\mu = exp(\\eta)$                      \n",
    "<br>\n",
    "$ \\qquad \\text{deviance}_{0} = \\text{deviance}$\n",
    "<br>  \n",
    "$ \\qquad \\text{deviance} = 2 \\sum \\big(yLn(y/\\mu) - (y-\\mu)\\big)$\n",
    "<br>  \n",
    "$ \\qquad \\Delta \\text{deviance} = |\\text{deviance} - \\text{deviance}_{0}|$\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "IRLS pre-processing and setup. \n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import create_engine\n",
    "from numpy.random import RandomState\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy import special\n",
    "from scipy import stats\n",
    "from numpy.linalg import inv\n",
    "\n",
    "pd.set_option('display.max_columns', 10000)\n",
    "pd.set_option('display.width', 50000)\n",
    "np.set_printoptions(suppress=True, precision=5)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "dforiginal      = pd.read_csv(\"U:/Repos/LTC/Incidence_Model/Datasets/dfgrpd.csv\")\n",
    "cat_fields      = [\"GENDER\", \"AGE_CATEGORY\"]\n",
    "dfinit_         = dforiginal[cat_fields + [\"INCIDENCE_MNTH\", \"EXPOSURE\"]]\n",
    "dfinit          = pd.get_dummies(dfinit_, columns=cat_fields, drop_first=True)\n",
    "dfinit.columns  = [i.upper() for i in dfinit.columns]\n",
    "response_fields = [\"INCIDENCE_MNTH\", \"EXPOSURE\"]\n",
    "design_fields   = list(set(dfinit.columns).difference(set(response_fields)))\n",
    "dfgrpd          = dfinit.groupby(design_fields, as_index=False).sum()\n",
    "offset          = dfgrpd.EXPOSURE.values.reshape(dfgrpd.EXPOSURE.values.size, 1)\n",
    "Xinit           = dfgrpd[design_fields]\n",
    "Xintercept      = np.ones(Xinit.shape[0]).reshape(Xinit.shape[0], 1)\n",
    "X               = np.hstack([Xintercept, Xinit])\n",
    "y               = dfgrpd.INCIDENCE_MNTH.values.reshape(Xinit.shape[0], 1)\n",
    "\n",
    "\n",
    "def deviance(y, mu):\n",
    "    \"\"\"\n",
    "    Compute Poisson deviance.\n",
    "    \"\"\"\n",
    "    y  = y.ravel().tolist()\n",
    "    mu = mu.ravel().tolist()\n",
    "    def f(yi, mui):\n",
    "        return(yi * np.log(yi / mui) - (yi - mui))\n",
    "    v = np.asarray([0 if (i==0 or j==0) else f(i, j)  for i, j in zip(y, mu)])\n",
    "    return(2 * v.sum())\n",
    "\n",
    "\n",
    "def likelihood(y, mu):\n",
    "    \"\"\"\n",
    "    Compute Poisson likelihood.\n",
    "    \"\"\"\n",
    "    v = y * np.log(mu) - mu - np.log(special.factorial(y))\n",
    "    return(np.exp(v).sum())\n",
    "\n",
    "                                     \n",
    "def loglikelihood(y, mu):\n",
    "    \"\"\"\n",
    "    Compute Poisson log-likelihood.\n",
    "    \"\"\"\n",
    "    y  = y.ravel()\n",
    "    mu = mu.ravel()\n",
    "    def f(yi, mui):\n",
    "        return(yi * np.log(mui) - mui - np.log(special.factorial(yi)))\n",
    "    v = np.asarray([0 if i==0 else f(i, j) for i, j in zip(y, mu)])\n",
    "    return(v.sum())\n",
    "\n",
    "\n",
    "\n",
    "coeffs = list()  \n",
    "loglik = list()\n",
    "devlst = list()\n",
    "tol    = .00001\n",
    "ddev   = np.Inf\n",
    "mu0    = (y + y.mean()) / 2\n",
    "eta0   = np.log(mu0)\n",
    "mu0    = np.exp(eta0)  \n",
    "dev0   = deviance(y=y, mu=mu0)\n",
    "llk0   = loglikelihood(y=y, mu=mu0)\n",
    "fsi    = 0           # Fisher scoring iteration counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Original implementation of Iterative Reweighted Least Squares (IRLS).\n",
    "Author: James D. Triveri\n",
    "Date  : 2019-02\n",
    "\"\"\"\n",
    "# Append initial deviance, loglikelihood and parameter estimates.\n",
    "devlst.append(dev0); loglik.append(llk0)\n",
    "\n",
    "\n",
    "\n",
    "while np.abs(ddev) > tol:\n",
    "    \n",
    "    fsi+=1\n",
    "    \n",
    "    # Compute updated W, working response, coeffs, linear component and mean.\n",
    "    W   = np.diag(np.ones(X.shape[0])) * mu0\n",
    "    z   = (eta0 + ((y - mu0) / mu0) - np.log(offset)).reshape(y.size, 1)\n",
    "    B   = inv((X.T @ W @ X)) @ (X.T @ W @ z)\n",
    "    eta = X @ B.reshape(B.size, 1) + np.log(offset)\n",
    "    mu  = np.exp(eta).reshape(eta.size, 1)\n",
    "    \n",
    "    # Compute updated deviance.\n",
    "    dev  = deviance(y=y, mu=mu)\n",
    "    llk  = loglikelihood(y=y, mu=mu)\n",
    "    ddev = dev0 - dev\n",
    "    vcov = inv((X.T @ W @ X))\n",
    "    mu0, eta0, llk0, dev0 = mu, eta, llk, dev\n",
    "\n",
    "    # Append updated parameters, loglikelihood and deviance.\n",
    "    devlst.append(dev0); loglik.append(llk0); coeffs.append(B)\n",
    "\n",
    "    \n",
    "print(\"\\n[=== Summary =====================================================]\\n\")\n",
    "print(f\"Fisher Scoring Iterations: {fsi}\\n\")\n",
    "print(\"Deviance:\\n\")\n",
    "for j in enumerate(devlst): print(j)\n",
    "print(\"\")\n",
    "print(\"Loglikelihood:\\n\")\n",
    "for k in enumerate(loglik): print(k)\n",
    "print(\"\")\n",
    "print(\"Variance-Covariance Matrix:\\n\")\n",
    "print(vcov)\n",
    "print(\"\\n[=================================================================]\\n\")\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Output Description\n",
    "\n",
    "- **`VARIABLE`**: The name of the estimated parameter.          \n",
    "<br>\n",
    "- **`ESTIMATE`**: The parameter estimate/model coefficient resulting from the running of IRLS.              \n",
    "<br>\n",
    "- **`STDERROR`**: The uncertainty in the parameter estimate.     \n",
    "<br>\n",
    "- **`Z-VALUE`**: Represents the ratio of `ESTIMATE / STDERROR`.     \n",
    "<br>\n",
    "- **`P(>|z|)`**: Probability of observing the outcome if the coefficient was not significantly different from 0.   \n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Model Coefficients and Standard Errors ######\n",
    "params = coeffs[-1].ravel()\n",
    "\n",
    "dfparams = pd.DataFrame({\n",
    "    \"VARIABLE\":[\"INTERCEPT\"] + Xinit.columns.tolist(),\n",
    "    \"ESTIMATE\":coeffs[-1].T.ravel(),\n",
    "    \"STDERROR\":np.sqrt(np.diagonal(vcov)),\n",
    "    \"Z-VALUE\" :(coeffs[-1].T.ravel() / np.sqrt(np.diagonal(vcov))),\n",
    "    \"P(>|z|)\" :stats.norm.cdf((coeffs[-1].T.ravel() / np.sqrt(np.diagonal(vcov))))\n",
    "    })\n",
    "\n",
    "dfparams\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Other Summary Statistics #####\n",
    "# Deviance is approx ChiSquare with m - p df\n",
    "# Computation of null deviance (intercept only model).\n",
    "mu_null = np.ones(y.size) * y.ravel().mean()\n",
    "\n",
    "resid_deviance = devlst[-1]\n",
    "null_deviance = deviance(y=y.ravel(), mu=mu_null)\n",
    "deviance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### Plot log-likelihood as a function of Fisher Scoring iteration #####\n",
    "x0, y0 = zip(*enumerate(loglik))\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(x0, y0, marker=\"o\", edgecolor=\"#000000\", s=50, color=\"#FFFFFF\")\n",
    "plt.title(\"Log-likelihood as a Function of Scoring Iteration\", color=\"#000000\", loc=\"left\")\n",
    "plt.plot(x0, y0, color=\"#FF0000\", linewidth=1.5)\n",
    "plt.xlabel(\"Fisher Scoring Iteration\")\n",
    "plt.ylabel(\"Log-likelihood\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Plot Deviance as a function of Fisher Scoring Iteration #####\n",
    "x1, y1 = zip(*enumerate(devlst))\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(x1, y1, marker=\"o\", edgecolor=\"#000000\", s=50, color=\"#FFFFFF\")\n",
    "plt.title(\"Poisson Deviance as a Function of Scoring Iteration\", color=\"#000000\", loc=\"left\")\n",
    "plt.plot(x1, y1, color=\"#FF0000\", linewidth=1.5)\n",
    "plt.xlabel(\"Fisher Scoring Iteration\")\n",
    "plt.ylabel(\"Deviance\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Statsmodels Poisson GLM fit #####\n",
    "\n",
    "keep_fields  = [\"GENDER\", \"AGE_CATEGORY\", \"INCIDENCE_MNTH\", \"EXPOSURE\"]\n",
    "dfmodel      = dforiginal[keep_fields]\n",
    "dfgrpd2      = dfmodel.groupby([\"GENDER\", \"AGE_CATEGORY\"], as_index=False).sum()\n",
    "formula_expr = \"INCIDENCE_MNTH ~ C(GENDER) + C(AGE_CATEGORY)\"\n",
    "\n",
    "poisson_mdl  = smf.glm(\n",
    "    formula=formula_expr, data=dfgrpd2, \n",
    "    family=sm.families.Poisson(link=sm.families.links.log),\n",
    "    exposure=dfgrpd2[\"EXPOSURE\"]\n",
    "    ).fit()\n",
    "\n",
    "\n",
    "# Bind reference to estimated model coefficients.\n",
    "params = poisson_mdl.params\n",
    "summ   = poisson_mdl.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson_mdl.deviance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#poisson_mdl.null_deviance\n",
    "dir(poisson_mdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy     = dfgrpd2.INCIDENCE_MNTH.values / dfgrpd2.EXPOSURE.values\n",
    "munull = yy.mean()\n",
    "\n",
    "\n",
    "loglikelihood(y=yy, mu=munull)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
