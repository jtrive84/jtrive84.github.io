{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Expectation Maximization from Scratch in R\n",
    "date: 2024-02-05\n",
    "description: Expectation maximization from scratch in R      \n",
    "categories: [Machine Learning, R]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Lets create a dataset which appears to be a mixture of two separate distributions. Such a dataset can be created using the \n",
    "following code:\n",
    "\n",
    "```R\n",
    "# Creating simulated bimodal data observations.\n",
    "options(scipen=9999)\n",
    "set.seed(516)\n",
    "\n",
    "x = c(\n",
    "    rnorm(31, mean=75, sd=17.5) + rnorm(31, mean=0, sd=5.5), \n",
    "    rnorm(23, mean=175, sd=25) + rnorm(23, mean=0, sd=10)\n",
    "    )\n",
    "```\n",
    "\n",
    "Running `plot(density(x))` generates a plot of the kernel density estimate:\n",
    "\n",
    "![](em01.png)\n",
    "\n",
    "\n",
    "\n",
    "Our goal is to fit a 2-component Gaussian Mixture Model (GMM) to a dataset consisting of $N$ observations using Expectation Maximization with parameters $\\mu_{k}, \\sigma_{k}, \\pi_{k}$ where $k \\in [1, 2]$. \n",
    "\n",
    " The probability density of a K-component GMM is given by\n",
    "\n",
    "$$\n",
    "f(\\boldsymbol{x}) = \\sum_{k=1}^{K}\\pi_{k} \\cdot \\mathcal{N}(\\boldsymbol{x}| \\mu_{k}, \\sigma^2_{k}),\n",
    "$$\n",
    "\n",
    "where in the 2 component case, $K=2$ (however, expressions will be in terms of $K$ for generality). From the density, an expression for the log-likelihood immediately follows:\n",
    "\n",
    "$$\n",
    "\\mathrm{Ln}\\hspace{.10em}f(X|\\pi, \\mu, \\sigma) = \\sum_{j=1}^{N}\\mathrm{Ln}\\hspace{.10em} \\Big[\\sum_{k=1}^{K} \\pi_{k} \\cdot\\mathcal{N}(\\boldsymbol{x}| \\mu_k, \\sigma^2_k)\\Big].\n",
    "$$\n",
    "\n",
    "In the log-likelihood, $\\mu_{k}$, $\\sigma_{k}$ and $\\pi_{k}$ represent the mean, standard deviation and mixture coefficient respectively for component $k$. The inner summation (indexed by $k$) iterates over mixture components while the outer summation (indexed by $j$) iterates over each observation in the data. Note that for each component $k$, $0 <= \\pi_{k} <= 1$, and for a $K$-component GMM, vectors $\\boldsymbol{\\mu}$, $\\boldsymbol{\\sigma}$ and $\\boldsymbol{\\pi}$ will each have length $K$.\n",
    "\n",
    "Taking the derivative of the log-likelihood w.r.t. $\\boldsymbol{\\mu}$, $\\boldsymbol{\\sigma}$ and $\\boldsymbol{\\pi}$,\n",
    "setting equal to zero and re-arranging yields update expressions for the parameters of interest:\n",
    "\n",
    "For the mean of component $k$, $\\mu^{'}_{k}$:\n",
    "\n",
    "$$\n",
    "\\mu^{'}_{k} = \\sum_{j=1}^{N} \\frac{x_{j} \\cdot \\pi_{k}  \\cdot \\mathcal{N}(x_{j}| \\mu_{k}, \\sigma^2_{k})} {\\pi_{k} \\cdot \\mathcal{N}(x_{j}| \\mu_{k}, \\sigma^2_{k})}\n",
    "$$\n",
    "\n",
    "For the standard deviation of component $k$, $\\sigma^{'}_{k}$:\n",
    "\n",
    "\n",
    "$$\n",
    "\\sigma^{'}_{k} =  \\sqrt{\\sum_{j=1}^{N} \\frac{(x_{j} - \\mu^{'}_{k})^{2} \\cdot \\pi_{k} \\cdot \\mathcal{N}(x_{j}| \\mu_{k}, \\sigma^2_r)} {\\pi_{k} \\cdot \\mathcal{N}(x_{j}| \\mu_k, \\sigma^2_k)}}\n",
    "$$\n",
    "\n",
    "\n",
    "For the mixture probability of component $k$, $\\pi_{k}$, first note that the posterior probability of a single observation $x$ originating from component $k=z$ is given by \n",
    "\n",
    "$$\n",
    "f(z|x) = \\frac{f(k=z) \\cdot f(x|k=z)}{\\sum f(k) \\cdot f(x|k)} = \\frac{\\pi_{z} \\cdot \\mathcal{N}(x| \\mu_{z}, \\sigma^{2}_{z})}{\\sum_{k=1}^{K} \\pi_{k} \\cdot \\mathcal{N}(x| \\mu_{k}, \\sigma^{2}_{k})}.\n",
    "$$\n",
    "\n",
    "$\\pi^{'}_{k}$ is updated by aggregating the probabilities for component $k$ across all observations, then dividing by the total number of observations:\n",
    "\n",
    "$$\n",
    "\\pi^{'}_{k} = \\frac{1}{N}\\sum_{j=1}^{N} f(k|x_{j}) = \\frac{1}{N}\\sum_{j=1}^{N} \\frac{\\pi_{k} \\cdot \\mathcal{N}(x_{j}| \\mu^{'}_k, \\sigma^{2*}_k)}{\\sum_{r=1}^K \\pi_{r} \\cdot \\mathcal{N}(x_{j}| \\mu^{'}_{r}, \\sigma^{2'}_{r})}\n",
    "$$\n",
    "\n",
    "We can summarize Expectation Maximization as follows:\n",
    "\n",
    "1. (E-step): Using current parameter values $(\\mu_{k}, \\sigma_{k}, \\pi_{k})$, estimate the posterior probabilities of each mixture component $\\pi^{'}_{k}$. \n",
    "\n",
    "2. (M-step): Using updated posterior probabilities, re-estimate component means and standard \n",
    "deviations $\\mu^{'}_{k}$, $\\sigma^{'}_{k}$.\n",
    "\n",
    "    \n",
    "### Implementation\n",
    "\n",
    "The code that follows represents an implementation of the Expectation Maximization algorithm for a two-component Gaussian Mixture Model. The initial estimates of $\\mu$, $\\sigma$ and $\\pi$ are obtained using k-means. We create a number of data structures to which parameter estimates and log-likelihood are saved at each iteration. Finally, parameter estimates are saved to the `paramsDF0` data.table for comparison with estimates from the mixtools library:\n",
    "\n",
    "\n",
    "```R\n",
    "# Implementation of the Expectation Maximization algorithm. \n",
    "library(\"data.table\")\n",
    "library(\"foreach\")\n",
    "options(scipen=9999)\n",
    "set.seed(516)\n",
    "\n",
    "# Generate bimodal data to fit via GMM. \n",
    "x = c(\n",
    "    rnorm(31, mean=75, sd=17.5) + rnorm(31, mean=0, sd=5.5),\n",
    "    rnorm(23, mean=175, sd=25) + rnorm(23, mean=0, sd=10)\n",
    "    )\n",
    "\n",
    "n = length(x)  # Number of observations.\n",
    "r = 2          # Number of components in Gaussian Mixture Model.\n",
    "maxIter = 500  # Maximum number of EM iterations.\n",
    "tol = 1e-8     # Log-likelihood exceedance threshold.\n",
    "\n",
    "# Use k-means to obtain initial parameter estimates.\n",
    "km = kmeans(x, r)\n",
    "\n",
    "# Create data.table with original observations and cluster assignments. \n",
    "kmeansDF = setorder(\n",
    "    data.table(x=x, group=km$cluster, stringsAsFactors=FALSE),\n",
    "    \"group\"\n",
    "    )\n",
    "\n",
    "# Bind reference to initial values for rmu, rsd and rpi.\n",
    "rmu = kmeansDF[,.(x=mean(x)), by=\"group\"]$x\n",
    "rsd = kmeansDF[,.(x=sd(x)), by=\"group\"]$x\n",
    "rpi = kmeansDF[,.N, by=\"group\"]$N / n\n",
    "\n",
    "# Collect log-likelihood and updated parameter estimates at each iteration.\n",
    "llEM = rep(0, maxIter)\n",
    "muEM = matrix(rep(0, r * maxIter), ncol=r)\n",
    "sdEM = matrix(rep(0, r * maxIter), ncol=r)\n",
    "piEM = matrix(rep(0, r * maxIter), ncol=r)\n",
    "\n",
    "# Initialize muEM, sdEM and piEM.\n",
    "muEM[1,] = rmu\n",
    "sdEM[1,] = rsd\n",
    "piEM[1,] = rpi\n",
    "\n",
    "# Iteration tracker.\n",
    "jj = 2\n",
    "\n",
    "# Expectation Maximization iteration.\n",
    "# All m-prefixed variables have dimension (n x r).\n",
    "# All r-prefixed variables have length r.\n",
    "while (jj<=maxIter) {\n",
    "\n",
    "    # Expectation step. \n",
    "    mcomp = foreach(ii=1:r, .combine=\"cbind\") %do% {\n",
    "        rpi[ii] * dnorm(x, rmu[ii], rsd[ii])\n",
    "    }\n",
    "    \n",
    "    # Determine likelihood contribution for each observation.\n",
    "    # mcompSum is a vector of length n. \n",
    "    mcompSum = apply(mcomp, MARGIN=1, sum, na.rm=TRUE)\n",
    "    \n",
    "    # Compute mixture probabilities for each observation. Summing across\n",
    "    # columns, each row will equal 1.0.\n",
    "    mpi = foreach(ii=1:ncol(mcomp), .combine=\"cbind\") %do% {\n",
    "        mcomp[,ii] / mcompSum\n",
    "    }\n",
    "        \n",
    "    # Maximization step.\n",
    "    rmu = as.numeric((t(x) %*% mpi) / apply(mpi, MARGIN=2, sum))\n",
    "    \n",
    "    rsd = foreach(ii=1:length(rmu), .combine=\"c\") %do% {\n",
    "        denom = sum(mpi[,ii][is.finite(mpi[,ii])], na.rm=TRUE)\n",
    "        numer = mpi[,ii] * (x - rmu[ii])^2\n",
    "        numer = sum(numer[is.finite(numer)], na.rm=TRUE)\n",
    "        sqrt(numer / denom)\n",
    "    }\n",
    "    \n",
    "    rpi = foreach(ii=1:ncol(mpi), .combine=\"c\") %do% {\n",
    "        sum(mpi[,ii][is.finite(mpi[,ii])], na.rm=TRUE) / n\n",
    "    }\n",
    "    \n",
    "    # Update llEM, muEM, sdEM and piEM.\n",
    "    llEM[jj] = sum(log(mcompSum)); muEM[jj,] = rmu; sdEM[jj,] = rsd; piEM[jj,] = rpi\n",
    "\n",
    "    message(\n",
    "        \"[\", jj, \"] ll=\", llEM[jj], \" (dll=\", abs(llEM[jj] - llEM[jj-1]), \").\"\n",
    "        )\n",
    "    \n",
    "    if (abs(llEM[jj] - llEM[jj-1])<tol) {\n",
    "        break\n",
    "    }\n",
    "    \n",
    "    jj = jj + 1\n",
    "}\n",
    "\n",
    "# Extract last populated row from muEM, sdEM and piEM.\n",
    "paramsDF0 = rbindlist(\n",
    "    list(\n",
    "        data.table(\n",
    "            parameter=\"mean\", w=seq(length(muEM[jj,])), value=muEM[jj,],\n",
    "            stringsAsFactors=FALSE\n",
    "            ),\n",
    "         data.table(\n",
    "            parameter=\"sd\", w=seq(length(sdEM[jj,])), value=sdEM[jj,],\n",
    "            stringsAsFactors=FALSE\n",
    "            ),\n",
    "         data.table(\n",
    "            parameter=\"w\", w=seq(length(piEM[jj,])), value=piEM[jj,],\n",
    "            stringsAsFactors=FALSE\n",
    "            )\n",
    "        ), fill=TRUE\n",
    "    )\n",
    "\n",
    "paramsDF0 = dcast(\n",
    "    paramsDF0, parameter ~ paste0(\"w_\", w), fun.aggregate=sum, \n",
    "    value.var=\"value\", fill=NA_real_\n",
    "    )\n",
    "\n",
    "```\n",
    "\n",
    "The code generates a status message at each iteration indicating the current log-likelihood estimate (`ll`) as well as the change in log-likelihood from the previous estimate (`dll`). Results are given below. \n",
    "\n",
    "\n",
    "```\n",
    "[2] ll=-276.872839784171 (dll=276.872839784171).\n",
    "[3] ll=-276.8390507688 (dll=0.0337890153707576).\n",
    "[4] ll=-276.83590305353 (dll=0.0031477152705861).\n",
    "[5] ll=-276.835432239165 (dll=0.000470814365030492).\n",
    "[6] ll=-276.835356881754 (dll=0.0000753574105374355).\n",
    "[7] ll=-276.835344544138 (dll=0.0000123376157148414).\n",
    "[8] ll=-276.835342506099 (dll=0.00000203803961085214).\n",
    "[9] ll=-276.835342168222 (dll=0.000000337876826961292).\n",
    "[10] ll=-276.835342112125 (dll=0.0000000560970079277467).\n",
    "[11] ll=-276.835342102806 (dll=0.00000000931919430513517).\n",
    "```\n",
    "\n",
    "`paramsDF0` reflects the final estimates of $\\mu_{i}$, $\\sigma_{i}$ and $\\pi_{i}$:\n",
    "\n",
    "```\n",
    "> paramsDF0\n",
    "   parameter         w_1        w_2\n",
    "1:      mean 181.2244438 81.7632674\n",
    "2:        sd  30.6704024 16.0083545\n",
    "3:         w   0.4325322  0.5674678\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "### Comparison with mixtools Library\n",
    "\n",
    "\n",
    "*mixtools* is a third-party R library used to estimate Gaussian Mixture Models. Using out data and initial estimates of $\\mu$, $\\sigma$ and $\\pi$, let's compare the results of our implementation vs. the `normalmixEM` function provided by mixtools:\n",
    "\n",
    "```R\n",
    "library(\"data.table\")\n",
    "library(\"mixtools\")\n",
    "\n",
    "gmm = normalmixEM(\n",
    "    x, k=2, lambda=piEM[1,], mu=muEM[1,],sigma=sdEM[1,]\n",
    "    )\n",
    "\n",
    "# Create data.table containing normalmixEM parameter estimates.\n",
    "paramsDF1 = data.table(\n",
    "    parameter=c(\"mu\", \"sigma\", \"pi\"), stringsAsFactors=FALSE\n",
    "  )\n",
    "for (ii in 1:r) {\n",
    "    paramsDF1[[paste0(\"comp\", ii)]] = c(gmm$mu[ii], gmm$sigma[ii], gmm$lambda[ii])\n",
    "}\n",
    "```\n",
    "\n",
    "Let's compare `paramsDF0` with `paramsDF1`:\n",
    "\n",
    "```\n",
    "> paramsDF0\n",
    "   parameter        w_1         w_2\n",
    "1:        mu 81.7632674 181.2244438\n",
    "2:     sigma 16.0083545  30.6704024\n",
    "3:        pi  0.5674678   0.4325322\n",
    "\n",
    "> paramsDF1\n",
    "   parameter      comp1       comp2\n",
    "1:        mu 81.7632740 181.2244778\n",
    "2:     sigma 16.0083538  30.6704046\n",
    "3:        pi  0.5674678   0.4325322\n",
    "```\n",
    "\n",
    "\n",
    "We find the results to be almost identical.\n",
    "\n",
    "\n",
    "Finally, we can create a density plot to illustrate the adequacy of the GMM fit to the data:\n",
    "\n",
    "\n",
    "```R\n",
    "# Plot comparing empirical data with estimated mixture model.\n",
    "exhibitPath = \"C:/Users/i103455/Repos/Tutorials/Supporting/em2.png\"\n",
    "png(file=exhibitPath)\n",
    "hist(x, prob=TRUE, breaks=23, xlim=c(min(x), max(x), main=\"GMM Estimate via EM\")\n",
    "xx = seq(from=min(x), to=max(x), length.out=500)\n",
    "yy = rpi[1] * dnorm(xx, mean=rmu[1], sd=rsd[1]) + rpi[2] * dnorm(xx, mean=rmu[2], sd=rsd[2])\n",
    "lines(xx, yy, col=\"#E02C70\", lwd=2)\n",
    "dev.off()\n",
    "```\n",
    "\n",
    "Which results in:\n",
    "\n",
    "\n",
    "![](em02.png)\n",
    "\n",
    "We see that the model serves as a good representation of the underlying data. \n",
    "\n",
    "\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "One final note regarding Expectation Maximization estimates: This is taken from Christopher Bishop's *Pattern Recognition and Machine Learning*, Chapter 9:\n",
    "\n",
    "> A K-component mixture model will have a total of $K!$ equivalent solutions corresponding to the $K!$ ways of assigning $K$ sets of parameters to $K$ components. In other words, for any given point in the space of parameter values there will be a further $K! - 1$ additional points all of which give rise to exactly the same distribution. This problem is known as *identifiability*. \n",
    "\n",
    "\n",
    "This means that for certain datasets, the parameters estimates from our implementation may differ from those found via mixtools, but for the purposes of finding a good density model, the difference is irrelevant since any of the equivalent solutions is as good as any other. Just be sure to perform a visual adequacy assessment to ensure the differences in parameter estimates do indeed result in identical or nearly identical probability densities. \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
