{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Gradient Descent for Logistic Regression       \n",
    "date: 2024-02-01\n",
    "description: Implementing gradient descent to estimate logistic regression coefficients       \n",
    "categories: [Machine Learning, Python]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Within the GLM framework, model coefficients are estimated using iterative reweighted \n",
    "least squares (IRLS), sometimes referred to as Fisher Scoring. This works well, \n",
    "but becomes inefficient as the size of the dataset increases: IRLS relies on the \n",
    "matrix of second partial derivatives which is expensive to compute. Instead, we \n",
    "can estimate logistic regression coefficients using gradient descent, which only \n",
    "relies on the first derivative of the cost function. This is much more efficient \n",
    "to compute, and generally provides good estimates once features have been \n",
    "standardized.\n",
    "\n",
    "\n",
    "Expression for linear predictions:\n",
    "\n",
    "$$\n",
    "z = \\boldsymbol{X} \\boldsymbol{\\theta}\n",
    "$$\n",
    "\n",
    "\n",
    "Expression for predicted probabilities:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "\n",
    "Gradient descent:\n",
    "\n",
    "$$\n",
    "\\theta_{t + 1} := \\theta_{t} - \\eta \\nabla L(\\hat{y}, y)\n",
    "$$\n",
    "\n",
    "- $\\eta$ is the learning rate.\n",
    "- $\\nabla L$ represents the gradients of the loss function. \n",
    "\n",
    "\n",
    "Optimal parameters:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\hat{\\theta}} = \\operatorname*{argmin}_\\theta \\frac{1}{m} \\sum_{i=1}^{m} L(f(x^{(i)}; \\theta), y^{(i)})\n",
    "$$\n",
    "\n",
    "\n",
    "The loss function is used to determine how much predictions differs from labels. \n",
    "Here we use binary cross entropy loss:\n",
    "\n",
    "$$\n",
    "L(\\hat{y}, y) = -\\frac{1}{m} \\sum_{i=1}^{m} y \\cdot \\mathrm{log}(\\hat{y}) + (1 - y) \\cdot \\mathrm{log}(1 - \\hat{y}),\n",
    "$$\n",
    "\n",
    "which we obtain from the log-likelihood of a single observation assumed to follow a \n",
    "binomial distribution. We flip the sign (the leading `-`) in order to minimize the \n",
    "loss. Note that $\\hat{y} = \\sigma(z)$.\n",
    "\n",
    "\n",
    "Expression for gradient of loss function:\n",
    "\n",
    "$$\n",
    "\\nabla L(\\hat{y}, y) = \\frac{1}{m}(\\hat{y} - y)x^{T}\n",
    "$$\n",
    "\n",
    "\n",
    "We choose $\\boldsymbol{\\theta}$ that maximizes the log-probability of the true y \n",
    "labels in the training data given the observations $X$. The goal is to find the \n",
    "set of weights which minimize the loss function, averaged over all examples. For \n",
    "each variable $\\theta_{j}$ in $\\boldsymbol{\\theta}$, the gradient will have a \n",
    "component that tells us the slope w.r.t. that variable. \n",
    "\n",
    "\n",
    "The breast cancer dataset is loaded from scikit-learn, the features are standardized\n",
    "and model coefficients estimated using gradient descent. Results are then compared\n",
    "with statsmodels coefficient estimates using the same data to ensure correctness.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load breast cancer dataset from scikit-learn. Standardize features.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data_loader = load_breast_cancer()\n",
    "X_data, y_data = data_loader.data, data_loader.target\n",
    "column_names = data_loader.feature_names\n",
    "df = pd.DataFrame(X_data, columns=column_names)\n",
    "\n",
    "keep_features = [\n",
    "    'mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
    "    'mean smoothness'\n",
    "    ]\n",
    "\n",
    "X = df[keep_features].values\n",
    "y = y_data[:,None]\n",
    "\n",
    "# Standardize features. \n",
    "sclr = StandardScaler()\n",
    "X = sclr.fit_transform(X)\n",
    "\n",
    "# Add bias term.\n",
    "bias = np.ones(X.shape[0])[:, None]\n",
    "X = np.concatenate([bias, X], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`eta` represents the learning rate and is a hyperparameter. `num_epochs` can be \n",
    "set as desired. For this example, we haven't incorporated early stopping logic, \n",
    "but this would be straightforward to implement. We initialize the weight vector \n",
    "`w` to all 0s. `w` has the same length as the number of features + 1 for the \n",
    "intercept term. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [ -0.4592425   22.09479813  -1.56463209 -14.7402888  -14.68918055\n",
      "  -1.66460167]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "eta = .50\n",
    "num_epochs = 50000\n",
    "w = np.zeros((X.shape[1], 1))\n",
    "loss = []\n",
    "\n",
    "for _ in range(num_epochs):\n",
    "    z = X @ w\n",
    "    ypred = 1. / (1. + np.exp(-z))\n",
    "    L = -np.mean(y * np.log(ypred) + (1 - y) * np.log(1 - ypred))\n",
    "    dL = np.mean(((ypred - y) * X), axis=0)[:, None]\n",
    "    loss.append(L)        \n",
    "    w-=eta * dL\n",
    "\n",
    "print(f\"w: {w.ravel()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating the coefficients using statsmodels yields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.148702\n",
      "         Iterations 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Intercept          -0.459246\n",
       "mean_radius        22.094852\n",
       "mean_texture       -1.564632\n",
       "mean_perimeter    -14.740317\n",
       "mean_area         -14.689213\n",
       "mean_smoothness    -1.664601\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "df2 = pd.DataFrame(X[:,1:], columns=keep_features)\n",
    "df2[\"target\"] = y\n",
    "df2.columns = [ii.replace(\" \", \"_\") for ii in df2.columns]\n",
    "features = \" + \".join([ii for ii in df2.columns if ii!=\"target\"])\n",
    "mdl_expr = \"target ~ \" + features\n",
    "mdl = smf.logit(mdl_expr, data=df2).fit()\n",
    "mdl.params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which matches closely with the results produced via gradient descent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
