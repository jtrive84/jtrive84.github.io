{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Up and Running with JAX - JIT Compilation, Vectorizing Transformations and autodiff\n",
    "date: 2025-03-22\n",
    "description: Introduction to the JAX library for high-performance numerical computing in Python.\n",
    "categories: [Machine Learning, Python]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I first learned about JAX a few years back in an article on functorch. Functorch was a library that brought JAX-like composable function transformations to PyTorch. It was initially developed as a separate library but has since been fully integrated into PyTorch's core as of PyTorch 2.0. \n",
    "\n",
    "Recently I've been spending time getting familiar into JAX and it has definitely been worthwhile. The clean functional approach makes my code more maintainable and reproducible, and I'm continuing to find ways to significantly improve performance and efficiency with a surprisingly small number of changes to existing code.  \n",
    "\n",
    "I've recently invested time in learning JAX, which has proven incredibly worthwhile. The clean functional approach makes my code more maintainable and reproducible, while delivering significant performance and efficiency improvements with surprisingly minimal changes to existing codebases.\n",
    "\n",
    "So, what is JAX? It is a high-performance numerical computing library developed by Google Research. It combines the ease of use of Python and Numpy with the speed and efficiency of XLA (Accelerated Linear Algebra), making it particularly well-suited for machine learning research and numerical computing that requires high performance. \n",
    "\n",
    "At its core, JAX extends Numpy's functionality with automatic differentiation capabilities. This is essential for gradient-based optimization in machine learning. JAX also excels at just-in-time compilation, which translates Python functions into optimized machine code at runtime.\n",
    "\n",
    "\n",
    "<p align=\"center\"><img src=\"jax2.jpg\" p>\n",
    "\n",
    "\n",
    "JAX takes a functional programming approach (no side-effects), emphasizing immutability and pure functions. Operations don't modify their inputs but instead return new values. This is particularly valuable for numerical computations since it enables better parallelization and optimization. Rather than changing arrays in place, JAX functions create new arrays with updated values, resulting in code that is more composable and reproducible. As highlighted in [JAX: The Sharp Bits](https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html#pure-functions):\n",
    "\n",
    "> JAX transformation and compilation are designed to work only on Python functions that are functionally pure: all the input data is passed through the function parameters, all the results are output through the function results. A pure function will always return the same result if invoked with the same inputs.\n",
    "\n",
    "\n",
    "Of the JAX features I researched, `vmap` is the most readily applicable to the work I do. `vmap` is a vectorizing transformation that automatically adds a batch dimension to calculations. It stands for \"vectorized map\" and lets you run a function across multiple inputs in a vectorized fashion without explicitly writing code for batch processing. This enables writing simple, single example functions, while simultaneously taking advantage of the performance benefits resulting from vectorized execution.\n",
    "\n",
    "The JAX numpy submodule can often be used as drop-in replacement for Numpy since the API is almost identical. It provides the same API as Numpy, meaning that functions like `jnp.array`, `jnp.sin`, `jnp.dot`, `jnp.mean`, and many others work just as they do in standard Numpy. The key difference is that JAX arrays are immutable and are optimized for GPU execution. \n",
    "\n",
    "This post highlights three features of JAX: Just-in-time compilation, vectorizing transformations and automatic differentiation. In a future post, I'll walk through the forward and backward pass for a fully-connected neural network implemented entirely in JAX. Those future posts will make heavy use of the content covered here. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Just-in-time Compilation\n",
    "\n",
    "JIT compilation in JAX speeds up computations by transforming Python functions into optimized machine code. When you apply `jax.jit` or the `jit` decorator to a function, JAX traces its operations and compiles them into an efficient, reusable representation. This means that instead of executing Python loops and function calls directly, JAX compiles them into a single, optimized computation graph that runs much faster on virtually any hardware. \n",
    "\n",
    "The first time a JIT-compiled function is called, there's a slight overhead as JAX compiles it, but subsequent calls run much faster since the compiled version is reused. JIT works best when inputs have a fixed shape and type, since changing them can trigger a recompilation. It can also be used in conjunction with `grad`, `vmap`, and `pmap` for even greater performance gains. \n",
    "\n",
    "To demonstrate, we'll implement a function that computes the great circle distance between two sets of coordinate pairs. The Haversine formula is defined as\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "a &= \\sin^2\\left(\\frac{\\Delta\\phi}{2}\\right) + \\cos\\phi_1 \\cos\\phi_2 \\sin^2\\left(\\frac{\\Delta\\lambda}{2}\\right)\\\\\n",
    "c &= 2 \\times \\arcsin\\left(\\min (1, \\sqrt{a})\\right)\\\\\n",
    "d &= R \\times c,\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\Delta \\phi = \\phi_{1} - \\phi_2$ (latitude difference in radians)\n",
    "- $\\Delta \\lambda = \\lambda_{1} - \\lambda_{2}$ (longitude difference in radians)\n",
    "- $R = 6,371$ km (globally average value of radius of the Earth in kilometers) \n",
    "- $d =$ great-circle distance\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "A simple implementation of the Haversine formula using trigonometric functions from jax.numpy is provided below. I originally attempted using Python's builtin trigonometric functions, but this caused JIT compilation to fail - the issue seemed to resolved itself when using the JAX-native variants. `get_haversine` accepts an array of `[lon0, lat0, lon1, lat1]` and returns the great circle distance between (lon0, lat0), (lon1, lat1):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "def get_haversine(coords):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points on the earth \n",
    "    (specified in decimal degrees).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    coords: array-like\n",
    "        Array containing the longitude and latitude of two points\n",
    "        arranged as [lon0, lat0, lon1, lat1].\n",
    "   \n",
    "    Returns\n",
    "    -------\n",
    "    Great circle distance in km.\n",
    "    \"\"\"\n",
    "\n",
    "    # Average Earth radius in km.\n",
    "    R = 6371.0\n",
    "\n",
    "    lon0, lat0, lon1, lat1 = coords\n",
    "\n",
    "    # Convert degree latitudes and longitudes to radians.\n",
    "    rlon0 = jnp.radians(lon0)\n",
    "    rlat0 = jnp.radians(lat0)\n",
    "    rlon1 = jnp.radians(lon1)\n",
    "    rlat1 = jnp.radians(lat1)\n",
    "    dlon, dlat = rlon1 - rlon0, rlat1 - rlat0\n",
    "    a = jnp.sin(dlat / 2)**2 + jnp.cos(rlat0) * jnp.cos(rlat1) * jnp.sin(dlon / 2)**2\n",
    "    c = 2 * jnp.asin(jnp.sqrt(a))\n",
    "    return R * c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "To calculate the Haversine distance in kilometers between two points, say Durkin Park on the Southside of Chicago and Nectar's in Burlington, Vermont, simply run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between Durkin Park and Nectar's: 1,215 km\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lon0, lat0 = -87.7295, 41.7390 # Durkin Park, Chicago, IL\n",
    "lon1, lat1 = -73.2117, 44.4762 # Nectar's, Burlington, VT\n",
    "\n",
    "# Put coordinates in JAX array.\n",
    "coords = jnp.array([lon0, lat0, lon1, lat1])\n",
    "\n",
    "d = get_haversine(coords)\n",
    "\n",
    "print(f\"Distance between Durkin Park and Nectar's: {d:,.0f} km\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "We can JIT-compile `get_haversine` and compare the run-time against the original implementation. Notice that we call `get_haversine_jit` once outside of `timeit` to avoid the overhead associated with the initial compilation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265 μs ± 38.2 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "3.46 μs ± 1.03 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from jax import jit\n",
    "\n",
    "# Create jit-compiled version of get_haversine. \n",
    "get_haversine_jit = jit(get_haversine)\n",
    "_ = get_haversine_jit([-80., 40., -85., 45.])  # compiles on first call.\n",
    "\n",
    "# Time the original.\n",
    "%timeit -n100 get_haversine(coords).block_until_ready()\n",
    "\n",
    "# Time the jit-compiled function.\n",
    "%timeit -n100 get_haversine_jit(coords).block_until_ready()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "We realized about a 75x speed up for very little work on our end. For this example, we created a new function `get_haversine_jit` so the jit-compiled version runtime could be compared against the original non-JITed version. It is possible to instead use the `@jit` decorator, allowing for the original function name to be re-used:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Using jit decorator instead.\n",
    "\n",
    "@jit\n",
    "def get_haversine(coords):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points on the earth \n",
    "    (specified in decimal degrees).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    coords: array-like\n",
    "        Array containing the longitude and latitude of two points\n",
    "        arranged as [lon0, lat0, lon1, lat1]\n",
    "   \n",
    "    Returns\n",
    "    -------\n",
    "    Great circle distance in km.\n",
    "    \"\"\"\n",
    "    R = 6371.0\n",
    "\n",
    "    # Convert degree latitudes and longitudes to radians.\n",
    "    lon0, lat0, lon1, lat1 = coords\n",
    "    rlon0 = jnp.radians(lon0)\n",
    "    rlat0 = jnp.radians(lat0)\n",
    "    rlon1 = jnp.radians(lon1)\n",
    "    rlat1 = jnp.radians(lat1)\n",
    "    dlon, dlat = rlon1 - rlon0, rlat1 - rlat0\n",
    "    a = jnp.sin(dlat / 2)**2 + jnp.cos(rlat0) * jnp.cos(rlat1) * jnp.sin(dlon / 2)**2\n",
    "    c = 2 * jnp.asin(jnp.sqrt(a))\n",
    "    return R * c\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "There are some limitations to JIT-compilation in JAX. In particular, loops, if statements, and other control flow mechanisms may not work as expected. Refer to [JAX: The Sharp Bits](https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html#jax-the-sharp-bits) for additional gotchas. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "### Vectorizing Transformations\n",
    "\n",
    "`vmap` automatically vectorizes operations, enabling the application of a function across multiple inputs without writing explicit loops. It enables batch processing while taking advantage of JAX's optimized execution. Instead of using loops, `vmap` efficiently maps a function over an array along a pre-specified axis.\n",
    "\n",
    "To demonstrate, I'll apply `vmap` to `get_haversine`, allowing it to accept coordinate arrays of shape n x 4 as opposed to 1 x 4. We will generate a random coordinate array of 10,000 x 4 using JAX's random generator utilities.\n",
    "\n",
    "In JAX, random number generation is handled a bit differently than in Numpy to ensure functional purity. JAX uses explicit PRNG keys to generate random numbers instead of relying on global state. A \"key\" is a special array that acts as a seed, and every time you use it, JAX produces the same random numbers for the same key.\n",
    "\n",
    "Since JAX enforces immutability, you can't reuse a key for multiple random calls without getting the same result. Instead, you split a key using `jax.random.split`, which deterministically generates new, unique keys from the original one. Each split key is independent, allowing for the generation of different random numbers while maintaining reproducibility. This approach makes JAX's random functions fully compatible with its JIT compilation and parallelization features.\n",
    "\n",
    "In the next cell, we create a 10,000 x 4 array of random coordinate pairs. We are interested in computing the Haversine distance for each pair of coordinates, but don't want to rewrite `get_haversine` to process more than a single pair of points at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coords.shape: (10000, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import jax.random as random\n",
    "\n",
    "# Create a 1,000,000 x 4 array of random longitudes and latitudes.\n",
    "# Longitudes are in the range -175 to 175.\n",
    "# Latitudes are in the range -85 to 85.\n",
    "n = 10_000\n",
    "\n",
    "# Seed for reproducibility. Split key for different random sequences.\n",
    "key = random.PRNGKey(516)  \n",
    "keys = random.split(key, 4)\n",
    "\n",
    "lon0 = random.uniform(keys[0], shape=(n,), minval=-175., maxval=175.)\n",
    "lat0 = random.uniform(keys[1], shape=(n,), minval=-85., maxval=85.)\n",
    "lon1 = random.uniform(keys[2], shape=(n,), minval=-175., maxval=175.)\n",
    "lat1 = random.uniform(keys[3], shape=(n,), minval=-85., maxval=85.)\n",
    "coords = jnp.stack([lon0, lat0, lon1, lat1], axis=1)  # Shape (n, 4)\n",
    "\n",
    "print(f\"coords.shape: {coords.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "Then applying vectorization to `get_haversine` is as simple as wrapping the original function with `vmap`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 8697.932 ,  6449.9453,  8237.629 ,  7416.7593,  9463.392 ,\n",
       "        3435.566 ,  8059.4575, 10055.319 , 16480.527 ,  6943.8413],      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from jax import vmap\n",
    "\n",
    "# Vectorize get_haversine.\n",
    "get_haversine_vmap = vmap(get_haversine)\n",
    "\n",
    "# Calculate distances between 10k coordinate pairs.\n",
    "d = get_haversine_vmap(coords)\n",
    "\n",
    "d[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Not surprisingly, vectorization provides a massive speedup vs. native looping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.42 s ± 20.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "5.45 ms ± 1.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%timeit -n1 for c in coords: get_haversine(c)\n",
    "\n",
    "%timeit -n1 get_haversine_vmap(coords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "It is also possible to combine just-in-time compilation along with vectorized transformations for additional performance gains.\n",
    "  \n",
    "Refer to the [JAX documentation on automatic vectorization](https://docs.jax.dev/en/latest/automatic-vectorization.html) for more advanced use cases of `vmap`, specifically how to apply a vectorized transformation along a specific axis of a multi-dimensional array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "### Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic differentiation (autodiff) in JAX is a method for computing derivatives efficiently and accurately using computational differentiation. Unlike numerical differentiation which relies on finite differences and can be prone to errors, or symbolic differentiation, which can become computationally expensive, autodiff in JAX works by systematically applying the chain rule at a computational level. \n",
    "\n",
    "JAX provides `grad` for computing gradients of scalar-valued functions, `jacfwd` and `jacrev` for Jacobians, and `hessian` for second-order derivatives. It uses forward-mode autodiff for computing derivatives of functions with a small number of inputs, while reverse-mode autodiff is well-suited for functions with many inputs but a single output, which is ideal for deep learning applications. \n",
    "\n",
    "\n",
    "As a simple example of using `grad` for a scalar-valued function, given a continuous random variable $X$ with CDF $F(x)$, the PDF $f(x)$ is obtained by differentiating $F(x)$:\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{d}{dx}F(x).\n",
    "$$\n",
    "\n",
    "For the exponential distribution, the CDF and PDF are given by\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "F(x) &= 1 - e^{-\\lambda x}\\\\\n",
    "f(x) &= F'(x) = \\lambda e^{-\\lambda x}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The exact value of the exponential PDF can be compared with the result returned by `grad` applied to the CDF to verify they are the same. The result will also be compared against the PDF at a given value of $x$ returned by `scipy.stats.expon`. For the purposes of demonstration, we set $\\lambda = 1/ 10$, which is hard-coded within `expon_cdf` and `expon_pdf`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import expon\n",
    "\n",
    "\n",
    "def expon_cdf(x):\n",
    "    \"\"\"\n",
    "    Exponential distribution CDF.\n",
    "    \"\"\"\n",
    "    return 1 - jnp.exp(-.10 * x)\n",
    "\n",
    "\n",
    "def expon_pdf(x):\n",
    "    \"\"\"\n",
    "    Exponential distribution PDF.\n",
    "    \"\"\"\n",
    "\n",
    "    return .10 * jnp.exp(-.10 * x)\n",
    "\n",
    "# Exponential distribution with mean 10. \n",
    "r = expon(scale=1/10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "In order to obtain the derivative of `expon_cdf` using JAX, pass `expon_cdf` into `grad`. The result is a callable that can accept any scalar value on $[0, \\infty)$ and will return the exponential PDF at that point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from jax import grad\n",
    "\n",
    "# Compute derivative of exponential CDF.\n",
    "jax_expon_pdf = grad(expon_cdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "Comparing the analytical PDF, `jax_expon_pdf` and the Scipy-generated PDF evaluated at 4.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact PDF : 0.063763\n",
      "JAX PDF   : 0.063763\n",
      "Scipy PDF : 0.063763\n"
     ]
    }
   ],
   "source": [
    "\n",
    "v0 = expon_pdf(4.5)\n",
    "v1 = jax_expon_pdf(4.5)\n",
    "v2 = r.pdf(4.5)\n",
    "\n",
    "print(f\"Exact PDF : {v0:.8f}\")\n",
    "print(f\"JAX PDF   : {v1:.8f}\")\n",
    "print(f\"Scipy PDF : {v2:.8f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "A particularly useful feature of `grad` is that we can pass `jax_expon_pdf` into `grad` and obtain the second derivative of the exponential CDF. Again we compare the JAX result against the exact analytical solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact second derivative : -0.00637628\n",
      "JAX second derivative   : -0.00637628\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Analytical solution for comparison.\n",
    "def expon_cdf_second_deriv(x):\n",
    "    \"\"\"\n",
    "    Second derivative of exponential CDF.\n",
    "    \"\"\"\n",
    "    return -(.10**2) * jnp.exp(-.10 * x)\n",
    "\n",
    "\n",
    "# Compute second derivative of exponential CDF using grad.\n",
    "jax_expon_cdf_second_deriv = grad(jax_expon_pdf)\n",
    "\n",
    "v0 = expon_cdf_second_deriv(4.5)\n",
    "v1 = jax_expon_cdf_second_deriv(4.5)\n",
    "\n",
    "print(f\"Exact second derivative : {v0:.8f}\")\n",
    "print(f\"JAX second derivative   : {v1:.8f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "It is not an exaggeration to say `vmap`, `jit`, and `grad` have transformed my machine learning workflows. Vectorization without loops, lightning-fast compilation and flexible gradient computation let me build cleaner, faster models with less code. I'm finding new applications all the time and will continue to explore additional ways to leverage JAX and related libraries like flax. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
