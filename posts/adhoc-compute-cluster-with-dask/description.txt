
Recently I attended a conference where a researcher presented on the challenges 
of standing up and maintaining "organic compute clusters". I was unfamiliar 
with the concept of an organic compute cluster, but learned that it is a 
euphemism for a network of mismatched, discarded or otherwise under utilized 
computing resources brought together to give the appearance of a single powerful 
system to process data at scale.

I've been evaluating Dask as a lighter weight alternative to PySpark, and have 
been very happy with the results so far. As part of this evaluation, 
I setup my own "organic compute cluster", using my current laptop, 
a laptop I purchased in 2012 running Ubuntu 22.04 and a Raspberry Pi 5. This 
article walks through my experience setting up a Dask compute cluster, and how 
to go about distributing work and monitoring tasks using the dask JupyterLab 
extension.


Connect to 