{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Smoothing Data with Cubic Splines in R\n",
    "date: 2024-02-10\n",
    "description: Smoothing data with cubic splines in R    \n",
    "categories: [Statistical Modeling, R]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "In this article, three approaches to smooth data will be demonstrated:\n",
    "\n",
    "1) standard polynomial regression\n",
    "2) cubic B-spline regression\n",
    "3) smoothing splines \n",
    "\n",
    "The data will  a set of loss development factors (LDFs) associated with an unidentified line of business. \n",
    "Instead of smoothing LDFs patterns directly, we first compute the cumulative loss development factors (CLDFs), \n",
    "then take the reciprocal to obtain Percent of Ultimate factors. Doing so will generally (not always) result \n",
    "in a monotonically increasing factor as a function of time. The code that follows prepares our data:\n",
    "\n",
    "\n",
    "```r\n",
    "library(\"data.table\")\n",
    "library(\"ggplot2\")\n",
    "options(scipen=9999)\n",
    "\n",
    "ldfs = c(\n",
    "    2.85637, 1.58402, 1.37531, 1.3001, 1.21469, 1.28128, 1.15415, 1.09783, 1.09302, \n",
    "    1.06395, 1.04992, 1.04659, 1.05164, 1.03117, 1.0236, 1.06338, 1.03234, 1.0172, \n",
    "    1.01795, 1.01813, 1.01413, 1.00863, 1.01346, 1.00372, 1.00423, 1.00683, 1.04633, \n",
    "    1.01796, 1.02279, 1.00629, 1.00205, 1.00316, 1.007, 1.02828, 1.00117, 1.00303, \n",
    "    1.00055, 1.02272, 1.00678, 1.00152, 1.00013, 1.01347, 1, 1.00071, 1.00136\n",
    "    )\n",
    "\n",
    "# Compute cumulative development factors.\n",
    "cldfs = rev(cumprod(rev(ldfs)))\n",
    "\n",
    "# Compute percent-of-ultimate factors. \n",
    "pous = 1 / cldfs\n",
    "\n",
    "DF = data.table(\n",
    "    xinit=1:length(pous), ldf0=ldfs, cldf0=cldfs, y=pous, \n",
    "    stringsAsFactors=FALSE\n",
    "    )\n",
    "\n",
    "# Rescale `dev` to fall between 0-1.\n",
    "DF[,x:=seq(0, 1, length.out=nrow(DF))]\n",
    "\n",
    "setcolorder(\n",
    "    DF, c(\"xinit\", \"x\", \"y\", \"ldf0\", \"cldf0\")\n",
    "    )\n",
    "```\n",
    "\n",
    "Fields in `DF` are defined as follows:\n",
    "\n",
    "* `xinit`: Original development period. 1 <= `xinit` <= 45.\n",
    "* `x`: `xinit` rescaled to [0,1].\n",
    "* `y`: Percent-of-ultimate factors.\n",
    "* `ldf0`: Original unsmoothed loss development factors.\n",
    "* `cldf0`: Original unsmoothed cumulative loss development factors. \n",
    "\n",
    "Inspecting our data yields:\n",
    "\n",
    "\n",
    "```R\n",
    "     xinit          x          y    ldf0     cldf0\n",
    " 1:      1 0.00000000 0.03022475 2.85637 33.085464\n",
    " 2:      2 0.02272727 0.08633308 1.58402 11.583045\n",
    " 3:      3 0.04545455 0.13675333 1.37531  7.312436\n",
    " 4:      4 0.06818182 0.18807822 1.30010  5.316937\n",
    " 5:      5 0.09090909 0.24452049 1.21469  4.089637\n",
    " 6:      6 0.11363636 0.29701659 1.28128  3.366815\n",
    " 7:      7 0.13636364 0.38056142 1.15415  2.627697\n",
    " 8:      8 0.15909091 0.43922497 1.09783  2.276738\n",
    " 9:      9 0.18181818 0.48219434 1.09302  2.073853\n",
    "10:     10 0.20454545 0.52704806 1.06395  1.897360\n",
    "11:     11 0.22727273 0.56075279 1.04992  1.783317\n",
    "12:     12 0.25000000 0.58874556 1.04659  1.698527\n",
    "13:     13 0.27272727 0.61617522 1.05164  1.622915\n",
    "14:     14 0.29545455 0.64799451 1.03117  1.543223\n",
    "15:     15 0.31818182 0.66819250 1.02360  1.496575\n",
    "16:     16 0.34090909 0.68396184 1.06338  1.462070\n",
    "17:     17 0.36363636 0.72731134 1.03234  1.374927\n",
    "18:     18 0.38636364 0.75083259 1.01720  1.331855\n",
    "19:     19 0.40909091 0.76374691 1.01795  1.309334\n",
    "20:     20 0.43181818 0.77745617 1.01813  1.286246\n",
    "21:     21 0.45454545 0.79155145 1.01413  1.263342\n",
    "22:     22 0.47727273 0.80273607 1.00863  1.245739\n",
    "23:     23 0.50000000 0.80966368 1.01346  1.235081\n",
    "24:     24 0.52272727 0.82056176 1.00372  1.218677\n",
    "25:     25 0.54545455 0.82361425 1.00423  1.214161\n",
    "26:     26 0.56818182 0.82709813 1.00683  1.209046\n",
    "27:     27 0.59090909 0.83274721 1.04633  1.200845\n",
    "28:     28 0.61363636 0.87132839 1.01796  1.147673\n",
    "29:     29 0.63636364 0.88697745 1.02279  1.127424\n",
    "30:     30 0.65909091 0.90719167 1.00629  1.102303\n",
    "31:     31 0.68181818 0.91289790 1.00205  1.095413\n",
    "32:     32 0.70454545 0.91476934 1.00316  1.093172\n",
    "33:     33 0.72727273 0.91766001 1.00700  1.089728\n",
    "34:     34 0.75000000 0.92408363 1.02828  1.082153\n",
    "35:     35 0.77272727 0.95021672 1.00117  1.052392\n",
    "36:     36 0.79545455 0.95132847 1.00303  1.051162\n",
    "37:     37 0.81818182 0.95421100 1.00055  1.047986\n",
    "38:     38 0.84090909 0.95473581 1.02272  1.047410\n",
    "39:     39 0.86363636 0.97642741 1.00678  1.024142\n",
    "40:     40 0.88636364 0.98304759 1.00152  1.017245\n",
    "41:     41 0.90909091 0.98454182 1.00013  1.015701\n",
    "42:     42 0.93181818 0.98466981 1.01347  1.015569\n",
    "43:     43 0.95454545 0.99793331 1.00000  1.002071\n",
    "44:     44 0.97727273 0.99793331 1.00071  1.002071\n",
    "45:     45 1.00000000 0.99864185 1.00136  1.001360\n",
    "     xinit          x          y    ldf0     cldf0\n",
    "```\n",
    "\n",
    "\n",
    "### Smoothing via Polynomial Regression \n",
    "\n",
    "Polynomial regression is similar to standard linear regression, except the design matrix contains `x` raised to the \n",
    "desired power in each column. For example, assuming we have independent value `x` given by:\n",
    "\n",
    "```r\n",
    "x = c(2, 4, 7, 5, 2)\n",
    "```\n",
    "\n",
    "Instead of regressing a response `y` on `x` alone, polynomial regression fits `y` using the matrix `X`:\n",
    "\n",
    "\n",
    "```\n",
    "     1  2   3\n",
    "[1,] 2  4   8\n",
    "[2,] 4 16  64\n",
    "[3,] 7 49 343\n",
    "[4,] 5 25 125\n",
    "[5,] 2  4   8\n",
    "```\n",
    "\n",
    "Notice each column represents `x` raised to the power in the column header. The first column is $x^{1}$, the second \n",
    "$x^{2}$ and the third $x^{3}$. Creating the design matrix in R can be accomplished using the `poly` function. Next we \n",
    "create the design matrix `X` and fit a polynomial regression model of degree 3 to our data:\n",
    "\n",
    "\n",
    "```R\n",
    "X = poly(DF$x, degree=3, raw=TRUE)\n",
    "y = DF$y\n",
    "\n",
    "# Combine design matrix with target response y (pous).\n",
    "DF1 = setDT(cbind.data.frame(X, y))\n",
    "\n",
    "# Call lm function. On RHS of formula, `.` specifies all columns in DF1 are to be used. \n",
    "mdl = lm(y ~ ., data=DF1)\n",
    "\n",
    "# Bind reference to fitted values as yhat1.\n",
    "DF[,yhat1:=unname(predict(mdl))]\n",
    "```\n",
    "\n",
    "A visualization overlaying polynomial regression estimates with original percent-of-ultimate factors is presented \n",
    "below (this code will be reused for all exhibits that follow, with inputs updated as necessary):\n",
    "\n",
    "```R\n",
    "exhibitTitle = paste0(\"Polynomial Regression: Percent-of-Ultimate Modeled vs. Actual\")\n",
    "\n",
    "ggplot(DF) + \n",
    "    geom_point(aes(x=xinit, y=y, color=\"Actual\"),  size=2) +\n",
    "    geom_line(aes(x=xinit, y=yhat1, color=\"Predicted\"), size=1.0) + \n",
    "    guides(color=guide_legend(override.aes=list(shape=c(16, NA), linetype=c(0, 1)))) +\n",
    "    scale_color_manual(\"\", values=c(\"Actual\"=\"#758585\", \"Predicted\"=\"#E02C70\")) +\n",
    "    scale_x_continuous(breaks=seq(min(DF$xinit), max(DF$xinit), 2)) + \n",
    "    scale_y_continuous(breaks=seq(0, 1, .1)) + ggtitle(exhibitTitle) +\n",
    "    theme(\n",
    "        plot.title=element_text(size=10, color=\"#E02C70\"), \n",
    "        axis.title.x=element_blank(), axis.title.y=element_blank(), \n",
    "        axis.text.x=element_text(angle=0, vjust=0.5, size=8),\n",
    "        axis.text.y=element_text(size=8)\n",
    "        )\n",
    "```\n",
    "\n",
    "![](Rsmooth1.png)\n",
    "\n",
    "\n",
    "There is a generally good fit to actuals, but notice that for later development periods, estimates are increasing upward \n",
    "rather than leveling off asymptotically toward 1.0. This is one of the drawbacks of polynomial regression: The bases are \n",
    "non-local, meaning that the fitted value of $y$ at a given value $x=x_{0}$ depends strongly on data values for $x$ far \n",
    "from $x_{0}$. In modern statistical modeling applications, polynomial basis-functions are used along with new basis \n",
    "functions such as splines, introduced next.\n",
    "\n",
    "\n",
    "### Smoothing via B-Spline Regression\n",
    "\n",
    "B-spline regression remedies the shortcomings of polynomial regression, namely the issue of non-locality. I'm going to \n",
    "demonstrate the usage of B-splines within the context of R rather than delve into the mathematical details. \n",
    "For an in-depth overview of B-splines, refer to \n",
    "[*Elements of Statistical Learning*](https://web.stanford.edu/~hastie/Papers/ESLII.pdf), specifically chapter 5. \n",
    "\n",
    "To perform B-spline regression in R, the `bs` function is used to generate the B-spline basis matrix for a polynomial \n",
    "spline. The number of spline knots to use is specified, along with the degree of polynomial to use (defaults to 3). We \n",
    "then generate the knot locations using the range of our independent value (`x`) and the number of knots using the `seq` \n",
    "function. Techniques can be used to minimize a cost function, such as LOOCV to minimize average MSE in order to determine \n",
    "the optimal number of knots, but we omit this step and instead arbitrarily choose 5 knots for the purposes of \n",
    "demonstration. As a rule-of-thumb, more knots leads to higher variance and lower bias.\n",
    "\n",
    "\n",
    "```R\n",
    "library(\"splines\")\n",
    "\n",
    "y = DF$y\n",
    "nbrSplineKnots = 5\n",
    "\n",
    "# Knot locations using data min/max and nbrSplineKnots. \n",
    "knotsSeq = seq(min(DF$x), max(DF$x), length.out=nbrSplineKnots)\n",
    "\n",
    "# Create basis matrix using splines::bs. degree=3 represents cubic spline.\n",
    "Bbasis = bs(DF$x, knots=knotsSeq, degree=3)\n",
    "\n",
    "# Drop columns containing only a single value.\n",
    "Bbasis = as.matrix(Filter(function(v) uniqueN(v)>1, as.data.table(Bbasis)))\n",
    "\n",
    "# Combine design matrix with target y (pous).\n",
    "DF2 = setDT(cbind.data.frame(Bbasis, y))\n",
    "\n",
    "# Fit B-spline regression model. \n",
    "mdl2 = lm(y ~ ., data=DF2)\n",
    "\n",
    "# Bind reference to fitted values as yhat2.\n",
    "DF[,yhat2:=unname(predict(mdl2))]\n",
    "```\n",
    "\n",
    "Running the same ggplot2 code from before, replacing `yhat1` with `yhat2`, we obtain:\n",
    "\n",
    "![](Rmooth2.png)\n",
    "\n",
    "\n",
    "Polynomial regression and B-spline estimates are similar, but B-spline estimates exhibit better behavior in the later \n",
    "periods, with estimates far less influenced by erratic observations, while also approaching 1.0 on the right. The B-spline \n",
    "fit exhibits a good trade-off between bias and variance. \n",
    "\n",
    "\n",
    "### Smoothing via `smooth.spline`\n",
    "\n",
    "$\\{x_{i},Y_{i}:i=1,\\dots ,n\\}$, which we model by $Y_{i} = f(x_{i}) + \\epsilon_{i}$, where $\\epsilon_{i}$ are zero mean \n",
    "random errors. The cubic smoothing spline estimate $\\hat{f}$ of the function $f$ is defined to be the minimizer of:\n",
    "\n",
    "$$\n",
    "\\sum _{i=1}^{n}[Y_{i}-{\\hat {f}}(x_{i})]^{2}+\\lambda \\int {\\hat {f}}''(x)^{2}dx,\n",
    "$$\n",
    "\n",
    "where $\\lambda \\geq 0$ is a smoothing parameter wehich controls the bias variance trade-off. With respect to the \n",
    "`smooth.spline` function, $\\lambda$ is identified as `spar`, where `0 <= spar <=1`. As `spar` approaches 1, the fit \n",
    "resembles linear regression (low variance / high bias). As `spar` approaches 0, the fit resembles interpolation \n",
    "(high variance / low bias). For the purposes of demonstration, we set `spar=.70` and `df` (degrees of freedom) to the \n",
    "number of records in the data:\n",
    "\n",
    "\n",
    "```R\n",
    "smoothParam = .70\n",
    "yhat3 = smooth.spline(DF$x, DF$y, df=nrow(DF), spar=smoothParam)$y\n",
    "DF[,yhat3:=yhat3]\n",
    "```\n",
    "\n",
    "`yhat3` vs. percent-of-ultimates:\n",
    "\n",
    "![](Rsmooth3.png)\n",
    "\n",
    "\n",
    "It comes as no surprise that `smooth.spline` predictions are similar to B-spline estimates. To demonstrate how changing \n",
    "`spar` modifies the nature of the curve, we present the next code block, which fits the original percent-of-ultimate data \n",
    "using `smooth.spline` for 6 values of `spar`:\n",
    "\n",
    "```R\n",
    "library(\"foreach\")\n",
    "\n",
    "targetSpar = c(.01, .15, .40 , .60, .85, 1.0)\n",
    "\n",
    "DF3 = foreach(\n",
    "    i=1:length(targetSpar), .inorder=TRUE, .errorhandling=\"stop\", \n",
    "    .final=function(ll) rbindlist(ll, fill=TRUE)\n",
    ") %do% {\n",
    "    currSpar = targetSpar[[i]]\n",
    "    currDF = DF[,.(xinit, x, y)]\n",
    "    sparID = paste0(\"spar=\", round(currSpar, 2))\n",
    "    mdl = smooth.spline(currDF[,x], currDF[,y], df=nrow(currDF), spar=currSpar)\n",
    "    currDF[,`:=`(yhat=mdl$y, id=sparID, spar=currSpar)]\n",
    "}\n",
    "\n",
    "ggplot(DF3) + \n",
    "    geom_point(aes(x=xinit, y=y, color=\"Actual\"),  size=1.5) +\n",
    "    geom_line(aes(x=xinit, y=yhat, color=\"Predicted\"), size=.75) + \n",
    "    scale_color_manual(\"\", values=c(\"Actual\"=\"#758585\", \"Predicted\"=\"#E02C70\")) +\n",
    "    scale_x_continuous(breaks=seq(min(DF3$xinit), max(DF3$xinit), 2)) + \n",
    "    scale_y_continuous(breaks=seq(0, 1, .1)) + \n",
    "    facet_wrap(facets=vars(id), nrow=2, scales=\"free\", shrink=FALSE) +\n",
    "    ggtitle(\"smooth.spline Regression: Percent-of-Ultimate Modeled vs. Actual\") +\n",
    "    theme(\n",
    "        plot.title=element_text(size=10, color=\"#E02C70\"), \n",
    "        axis.title.x=element_blank(), axis.title.y=element_blank(), \n",
    "        axis.text.x=element_blank(), axis.text.y=element_blank(), \n",
    "        legend.position=\"none\", panel.grid.major=element_blank(), \n",
    "        axis.ticks=element_blank()\n",
    "        )\n",
    "```\n",
    "\n",
    "`spar=1` is the lower-right facet (lowest variance/highest bias), and `spar=.01` the upper-left facet \n",
    "(highest variance/lowest bias):\n",
    "\n",
    "![](Rsmooth4.png)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
